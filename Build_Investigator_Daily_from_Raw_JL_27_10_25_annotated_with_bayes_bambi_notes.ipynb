{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Investigations Backlog â€“ Annotated Build Notebook\n",
    "**Date:** 2025-10-27\n",
    "\n",
    "This version of the notebook is automatically annotated with:\n",
    "- Line-by-line comments in code cells to explain what each statement is doing.\n",
    "- Brief summaries before each code cell describing the main purpose.\n",
    "- Pointers to comprehensive documentation: see `README_Investigations_Backlog_Documentation.md` in the same folder for the full end-to-end description (data engineering, predictive modelling, and Bayesian analysis approach).\n",
    "\n",
    "> Original source notebook: `Build_Investigator_Daily_from_Raw_JL.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "> **Original note:**\n",
    ">\n",
    "> # Build Investigator Daily Panel (from OPG raw extract)\n",
    "> This notebook mirrors the script flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### general processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!python -m venv .venv && . .venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Jake note regarding linking investigation data to LPA and staff data\n",
    "Iâ€™ve re-added the investigator names as requested. Iâ€™ve also added the LPA/Deputyship ID too. (so just the donors name and DOB is removed). The password remains as â€œbacklogâ€. \n",
    "\n",
    "On the analytical platform, the LPA number is stored as â€˜UIDâ€™ in the cases table in the opg_sirius_prod database. The investigations database has hyphens for these idâ€™s, but if you remove the hyphens you can then join the database with the data on the AP. Effectively the donor names can be re-accessed there, and other key variables such as the LPA registration dates can be retrieved (as these are not stored on the database but these are significant for inbounds).\n",
    "\n",
    "I have also added the FTE of the EO/AO investigators to this sheet, what youâ€™ll notice is that there are some members of staff who were previously EOâ€™s (and are now HEOâ€™s) so they are not on the staff list. The staff list is in a constant state of flux with the incoming cohorts/natural attrition, so Iâ€™d heavily recommend if any projections relating to resource levels are made I send a definitive list on a specific date so thereâ€™s a clear point of reference. In the temporary backlog model, I am manually reviewing the list each month with placeholders for the incoming cohorts, but for the more sophisticated model you may come up with a better solution. Itâ€™s something to discuss in next weekâ€™s meeting im sure.\n",
    "\n",
    "## Peter interpretation of cases left the allocation\n",
    "One question that I do have is, whilst maintaining anonymity can the records of closed cases be linked to individual investigators ? As you know the key problem that we are trying to investigate is how will changes in staff volumes impact OPGâ€™s ability to reduce the backlog, so we really need to understand the variation in workloads assigned to individuals.\n",
    "\n",
    "the cases closed or sent to court for legal review from the anlaytical point of view can be the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### imports and environment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries/modules for use below\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import re, hashlib\n",
    "\n",
    "# Configure paths\n",
    "# Path to the raw investigation data\n",
    "RAW_PATH = Path('data/raw/raw.csv')\n",
    "# Path to the output/processed investigation data\n",
    "OUT_DIR = Path('data/out'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Print if the path exists\n",
    "print(RAW_PATH.exists(), OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### imports and environment setup, date parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ðŸ§¹ DATA PRE-PROCESSING SECTION\n",
    "# -----------------------------\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "# Define a set of string patterns that represent missing or null values.\n",
    "# These strings will be treated as equivalent to NaN during cleaning.\n",
    "NULL_STRINGS = {\n",
    "    '', 'na', 'n/a', 'none', 'null', '-', '--', 'unknown',\n",
    "    'not completed', 'not complete', 'tbc', 'n\\\\a'\n",
    "}\n",
    "\n",
    "\n",
    "def normalise_col(c: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a column name for consistency.\n",
    "\n",
    "    This function cleans up and standardizes column names by:\n",
    "    - Converting to lowercase\n",
    "    - Removing leading/trailing whitespace\n",
    "    - Replacing multiple spaces with a single space\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c : str\n",
    "        The original column name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A cleaned and standardized version of the column name.\n",
    "    \"\"\"\n",
    "    # Convert to string, remove extra spaces, and make lowercase.\n",
    "    return re.sub(r'\\s+', ' ', str(c).strip().lower())\n",
    "\n",
    "\n",
    "def parse_date_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse and clean a pandas Series of date strings.\n",
    "\n",
    "    This function:\n",
    "    - Handles various date formats\n",
    "    - Converts known null strings to NaT\n",
    "    - Removes ordinal suffixes (e.g., '1st', '2nd', '3rd')\n",
    "    - Fixes known typos\n",
    "    - Uses robust pandas date parsing with fallback strategies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : pd.Series\n",
    "        A pandas Series containing raw date values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series of datetime64[ns] values with cleaned and parsed dates.\n",
    "    \"\"\"\n",
    "\n",
    "    def _p(x):\n",
    "        \"\"\"Internal helper to parse a single date entry.\"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        # Return NaT if missing\n",
    "        if pd.isna(x):\n",
    "            return pd.NaT\n",
    "\n",
    "        # Convert to lowercase string\n",
    "        xs = str(x).strip().lower()\n",
    "\n",
    "        # Return NaT if in known null string set\n",
    "        if xs in NULL_STRINGS:\n",
    "            return pd.NaT\n",
    "\n",
    "        # Clean up common errors and ordinal suffixes\n",
    "        xs = re.sub(r'(\\d{1,2})(st|nd|rd|th)', r'\\1', xs).replace('legel', 'legal')\n",
    "\n",
    "        # Try strict parsing, then flexible fallback\n",
    "        try:\n",
    "            return pd.to_datetime(xs, dayfirst=True, errors='raise')\n",
    "        except Exception:\n",
    "            return pd.to_datetime(xs, infer_datetime_format=True, dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Apply the parser to each element of the Series\n",
    "    return s.apply(_p)\n",
    "\n",
    "\n",
    "def hash_id(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a short, anonymized hash-based identifier.\n",
    "\n",
    "    Creates a pseudonymized ID for text entries using SHA1 hashing.\n",
    "    Empty or missing values return an empty string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : str\n",
    "        The input text value (e.g., name, case number).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        An anonymized hash string prefixed with 'S', e.g., 'S1a2b3c4d'.\n",
    "    \"\"\"\n",
    "    # Return empty string for null or blank input\n",
    "    if pd.isna(t) or str(t).strip() == '':\n",
    "        return ''\n",
    "\n",
    "    # Create SHA1 hash and take first 8 characters for compact ID\n",
    "    return 'S' + hashlib.sha1(str(t).encode('utf-8')).hexdigest()[:8]\n",
    "\n",
    "\n",
    "def month_to_season(m: int) -> str:\n",
    "    \"\"\"\n",
    "    Convert a numeric month into a season name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Month number (1â€“12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The season corresponding to the month ('winter', 'spring', 'summer', or 'autumn').\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> month_to_season(4)\n",
    "    'spring'\n",
    "    >>> month_to_season(10)\n",
    "    'autumn'\n",
    "    \"\"\"\n",
    "    # Map month numbers to their respective seasons\n",
    "    return {\n",
    "        12: 'winter', 1: 'winter', 2: 'winter',\n",
    "        3: 'spring', 4: 'spring', 5: 'spring',\n",
    "        6: 'summer', 7: 'summer', 8: 'summer',\n",
    "        9: 'autumn', 10: 'autumn', 11: 'autumn'\n",
    "    }[int(m)]\n",
    "\n",
    "\n",
    "def is_term_month(m: int) -> int:\n",
    "    \"\"\"\n",
    "    Identify whether a month is a 'termination month'.\n",
    "\n",
    "    In the current logic, August (month 8) is excluded and returns 0.\n",
    "    All other months return 1, representing active/valid months.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Month number (1â€“12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if the month is August, else 1.\n",
    "    \"\"\"\n",
    "    # Return binary flag based on month value\n",
    "    return 0 if int(m) == 8 else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### imports and environment setup, data loading, joining/merging datasets, aggregation/grouping, pivot/reshape, data cleaning, sorting, feature engineering, exporting outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# ðŸ§© DATA LOADING AND FEATURE ENGINEERING\n",
    "# -------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: load_raw()\n",
    "# -------------------------------------------------------------\n",
    "def load_raw(p: Path, force_encoding: str | None = None):\n",
    "    \"\"\"\n",
    "    Load a CSV or Excel file into a pandas DataFrame with robust encoding handling.\n",
    "\n",
    "    This function attempts to open and read raw data files safely, even when\n",
    "    character encodings vary or are unknown. It tries multiple encodings in order\n",
    "    until one succeeds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Path\n",
    "        Path to the input file.\n",
    "    force_encoding : str, optional\n",
    "        If provided, forces the use of a specific encoding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (df, colmap)\n",
    "        df : pd.DataFrame\n",
    "            Cleaned dataframe containing the raw data.\n",
    "        colmap : dict\n",
    "            Mapping of normalized column names (lowercased, trimmed) to original column headers.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the file path does not exist.\n",
    "    RuntimeError\n",
    "        If all encoding attempts fail.\n",
    "    \"\"\"\n",
    "    import io\n",
    "\n",
    "    # Check file existence\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "    # Excel files typically do not have encoding issues\n",
    "    if p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(p, dtype=str)\n",
    "    else:\n",
    "        tried = []\n",
    "        # Build list of encodings to try\n",
    "        encodings_to_try = (\n",
    "            [force_encoding] if force_encoding else\n",
    "            [\"utf-8-sig\", \"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-16\", \"utf-16le\", \"utf-16be\"]\n",
    "        )\n",
    "\n",
    "        df = None\n",
    "        last_err = None\n",
    "\n",
    "        # Try to read using multiple encodings\n",
    "        for enc in encodings_to_try:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p, dtype=str, sep=None, engine=\"python\",\n",
    "                    encoding=enc, encoding_errors=\"strict\"\n",
    "                )\n",
    "                break\n",
    "            except UnicodeDecodeError as e:\n",
    "                tried.append(enc)\n",
    "                last_err = e\n",
    "            except Exception as e:\n",
    "                # Continue trying other encodings\n",
    "                tried.append(enc)\n",
    "                last_err = e\n",
    "\n",
    "        # Fallback: attempt to decode with cp1252 and replace bad bytes\n",
    "        if df is None:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p, dtype=str, sep=None, engine=\"python\",\n",
    "                    encoding=\"cp1252\", encoding_errors=\"replace\"\n",
    "                )\n",
    "                print(f\"[load_raw] WARNING: used cp1252 with replacement after failed encodings: {tried}\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to read CSV. Tried encodings {tried}. Last error: {last_err}\"\n",
    "                ) from e\n",
    "\n",
    "    # Strip whitespace from all string values\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Create mapping of normalized column names â†’ original names\n",
    "    colmap = {re.sub(r\"\\s+\", \" \", str(c).strip().lower()): c for c in df.columns}\n",
    "\n",
    "    return df, colmap\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: col()\n",
    "# -------------------------------------------------------------\n",
    "def col(df: pd.DataFrame, colmap: dict, name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Retrieve a column from a DataFrame by fuzzy name matching.\n",
    "\n",
    "    This function normalises the requested column name and searches the column map\n",
    "    for an exact or partial match. Returns a Series of NaNs if not found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The source DataFrame.\n",
    "    colmap : dict\n",
    "        Mapping of normalised column names to original names.\n",
    "    name : str\n",
    "        Column name to look up.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The column data if found, otherwise a Series of NaN values.\n",
    "    \"\"\"\n",
    "    k = normalise_col(name)\n",
    "\n",
    "    # Exact match first\n",
    "    if k in colmap:\n",
    "        return df[colmap[k]]\n",
    "\n",
    "    # Partial match fallback\n",
    "    for kk, v in colmap.items():\n",
    "        if k in kk or kk in k:\n",
    "            return df[v]\n",
    "\n",
    "    # Default: return empty column of NaNs\n",
    "    return pd.Series([np.nan] * len(df))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: engineer()\n",
    "# -------------------------------------------------------------\n",
    "def engineer(df: pd.DataFrame, colmap: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer standardised and typed columns from raw investigation data.\n",
    "\n",
    "    This function extracts and converts the key variables such as case IDs, investigators,\n",
    "    FTEs, and multiple date columns from the raw file using reusable helper functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe from load_raw().\n",
    "    colmap : dict\n",
    "        Column name mapping from load_raw().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned and feature-engineered dataframe ready for downstream modeling.\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame({\n",
    "        'case_id': col(df, colmap, 'ID'),\n",
    "        'investigator': col(df, colmap, 'Investigator'),\n",
    "        'team': col(df, colmap, 'Team'),\n",
    "        'fte': pd.to_numeric(col(df, colmap, 'Investigator FTE'), errors='coerce')\n",
    "    })\n",
    "\n",
    "    # Parse and standardize all relevant date columns\n",
    "    out['dt_received_inv'] = parse_date_series(col(df, colmap, 'Date Received in Investigations'))\n",
    "    out['dt_alloc_invest'] = parse_date_series(col(df, colmap, 'Date allocated to current investigator'))\n",
    "    out['dt_alloc_team'] = parse_date_series(col(df, colmap, 'Date allocated to team'))\n",
    "    out['dt_pg_signoff'] = parse_date_series(col(df, colmap, 'PG Sign off date'))\n",
    "    out['dt_close'] = parse_date_series(col(df, colmap, 'Closure Date'))\n",
    "    out['dt_legal_req_1'] = parse_date_series(col(df, colmap, 'Date of Legal Review Request 1'))\n",
    "    out['dt_legal_rej_1'] = parse_date_series(col(df, colmap, 'Date Legal Rejects 1'))\n",
    "    out['dt_legal_req_2'] = parse_date_series(col(df, colmap, 'Date of Legal Review Request 2'))\n",
    "    out['dt_legal_rej_2'] = parse_date_series(col(df, colmap, 'Date Legal Rejects 2'))\n",
    "    out['dt_legal_req_3'] = parse_date_series(col(df, colmap, 'Date of Legel Review Request 3'))\n",
    "    out['dt_legal_approval'] = parse_date_series(col(df, colmap, 'Legal Approval Date'))\n",
    "    out['dt_date_of_order'] = parse_date_series(col(df, colmap, 'Date Of Order'))\n",
    "    out['dt_flagged'] = parse_date_series(col(df, colmap, 'Flagged Date'))\n",
    "\n",
    "    # Fill missing FTEs with 1.0, hash investigator names for anonymization, and add placeholders\n",
    "    out['fte'] = out['fte'].fillna(1.0)\n",
    "    out['staff_id'] = out['investigator'].apply(hash_id)\n",
    "    out['role'] = ''\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: date_horizon()\n",
    "# -------------------------------------------------------------\n",
    "def date_horizon(typed: pd.DataFrame, pad_days: int = 14):\n",
    "    \"\"\"\n",
    "    Determine the overall start and end date horizon of the dataset.\n",
    "\n",
    "    Combines several date columns to find the earliest and latest dates,\n",
    "    applying a configurable padding period at the end.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Feature-engineered dataset with standardized date columns.\n",
    "    pad_days : int, default=14\n",
    "        Number of days to extend the end horizon.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of pd.Timestamp\n",
    "        (start, end) normalized date range.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from datetime import datetime\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05'), pd.NaT],\n",
    "    ...     'dt_alloc_invest': [pd.NaT, pd.Timestamp('2025-01-10')],\n",
    "    ...     'dt_alloc_team': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.NaT, pd.Timestamp('2025-02-01')],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> s, e = date_horizon(df, pad_days=7)\n",
    "    >>> isinstance(s, pd.Timestamp) and isinstance(e, pd.Timestamp)\n",
    "    True\n",
    "    >>> (e - s).days >= (pd.Timestamp('2025-02-01') - pd.Timestamp('2025-01-05')).days\n",
    "    True\n",
    "    \"\"\"\n",
    "    start = pd.concat([typed['dt_received_inv'], typed['dt_alloc_invest'], typed['dt_alloc_team']]).min()\n",
    "    end = pd.concat([typed['dt_close'], typed['dt_pg_signoff'], typed['dt_date_of_order']]).max()\n",
    "\n",
    "    if pd.isna(start):\n",
    "        start = pd.Timestamp.today().normalize() - pd.Timedelta(days=30)\n",
    "    if pd.isna(end):\n",
    "        end = pd.Timestamp.today().normalize()\n",
    "\n",
    "    end = end + pd.Timedelta(days=pad_days)\n",
    "    return start.normalize(), end.normalize()\n",
    "\n",
    "\n",
    "def build_event_log(typed: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct an event log from feature-engineered investigation data.\n",
    "\n",
    "    For each case, this function creates dated event records (e.g., new case pickup,\n",
    "    legal requests/approvals, court orders) at the staff-day level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Must include:\n",
    "        ['staff_id','team','fte','case_id',\n",
    "         'dt_alloc_invest','dt_legal_req_1','dt_legal_req_2','dt_legal_req_3',\n",
    "         'dt_legal_approval','dt_date_of_order'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        ['date','staff_id','team','fte','case_id','event','meta'].\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id':['S1'], 'team':['A'], 'fte':[1.0], 'case_id':['C1'],\n",
    "    ...     'dt_alloc_invest':[pd.Timestamp('2025-01-10')],\n",
    "    ...     'dt_legal_req_1':[pd.NaT], 'dt_legal_req_2':[pd.NaT], 'dt_legal_req_3':[pd.NaT],\n",
    "    ...     'dt_legal_approval':[pd.Timestamp('2025-01-20')],\n",
    "    ...     'dt_date_of_order':[pd.NaT],\n",
    "    ... })\n",
    "    >>> ev = build_event_log(typed)\n",
    "    >>> sorted(ev['event'].unique().tolist())\n",
    "    ['legal_approval', 'newcase']\n",
    "    >>> set(ev.columns) >= {'date','staff_id','team','fte','case_id','event','meta'}\n",
    "    True\n",
    "    \"\"\"\n",
    "    rec = []\n",
    "    for _, r in typed.iterrows():\n",
    "        sid, team, fte, cid = r['staff_id'], r['team'], r['fte'], r['case_id']\n",
    "\n",
    "        def add(dt, etype):\n",
    "            if pd.isna(dt):\n",
    "                return\n",
    "            rec.append({\n",
    "                'date': dt.normalize(),\n",
    "                'staff_id': sid,\n",
    "                'team': team,\n",
    "                'fte': fte,\n",
    "                'case_id': cid,\n",
    "                'event': etype,\n",
    "                'meta': ''\n",
    "            })\n",
    "\n",
    "        add(r['dt_alloc_invest'], 'newcase')\n",
    "        add(r['dt_legal_req_1'], 'legal_request')\n",
    "        add(r['dt_legal_req_2'], 'legal_request')\n",
    "        add(r['dt_legal_req_3'], 'legal_request')\n",
    "        add(r['dt_legal_approval'], 'legal_approval')\n",
    "        add(r['dt_date_of_order'], 'court_order')\n",
    "\n",
    "    ev = pd.DataFrame.from_records(rec)\n",
    "    return ev if not ev.empty else pd.DataFrame(\n",
    "        columns=['date', 'staff_id', 'team', 'fte', 'case_id', 'event', 'meta']\n",
    "    )\n",
    "\n",
    "\n",
    "def build_wip_series(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a Work-In-Progress (WIP) daily series per staff member.\n",
    "\n",
    "    A case is in WIP from allocation to earliest of (closure, PG sign-off, end).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        ['staff_id','team','dt_alloc_invest','dt_close','dt_pg_signoff'].\n",
    "    start : pd.Timestamp\n",
    "    end : pd.Timestamp\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        ['date','staff_id','team','wip'].\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id':['S1','S1'], 'team':['A','A'],\n",
    "    ...     'dt_alloc_invest':[pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-05')],\n",
    "    ...     'dt_close':[pd.Timestamp('2025-01-03'), pd.NaT],\n",
    "    ...     'dt_pg_signoff':[pd.NaT, pd.Timestamp('2025-01-07')],\n",
    "    ... })\n",
    "    >>> wip = build_wip_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-10'))\n",
    "    >>> set(wip.columns) == {'date','staff_id','team','wip'}\n",
    "    True\n",
    "    >>> wip['wip'].ge(0).all()\n",
    "    True\n",
    "    \"\"\"\n",
    "    end_dt = typed['dt_close'].fillna(typed['dt_pg_signoff']).fillna(end)\n",
    "    intervals = pd.DataFrame({\n",
    "        'staff_id': typed['staff_id'],\n",
    "        'team': typed['team'],\n",
    "        'start': typed['dt_alloc_invest'],\n",
    "        'end': end_dt\n",
    "    }).dropna()\n",
    "\n",
    "    deltas = []\n",
    "    for _, r in intervals.iterrows():\n",
    "        s = r['start'].normalize()\n",
    "        e = r['end'].normalize()\n",
    "        if s > end or e < start:\n",
    "            continue\n",
    "        s = max(s, start)\n",
    "        e = min(e, end)\n",
    "        deltas.append((r['staff_id'], r['team'], s, 1))\n",
    "        deltas.append((r['staff_id'], r['team'], e + pd.Timedelta(days=1), -1))\n",
    "\n",
    "    if not deltas:\n",
    "        return pd.DataFrame(columns=['date', 'staff_id', 'team', 'wip'])\n",
    "\n",
    "    deltas = pd.DataFrame(deltas, columns=['staff_id', 'team', 'date', 'delta'])\n",
    "    all_dates = pd.DataFrame({'date': pd.date_range(start, end, freq='D')})\n",
    "\n",
    "    rows = []\n",
    "    for (sid, team), g in deltas.groupby(['staff_id', 'team']):\n",
    "        gg = g.groupby('date', as_index=False)['delta'].sum()\n",
    "        grid = all_dates.merge(gg, on='date', how='left').fillna({'delta': 0})\n",
    "        grid['wip'] = grid['delta'].cumsum()\n",
    "        grid['staff_id'] = sid\n",
    "        grid['team'] = team\n",
    "        rows.append(grid[['date', 'staff_id', 'team', 'wip']])\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "        columns=['date', 'staff_id', 'team', 'wip']\n",
    "    )\n",
    "\n",
    "\n",
    "def build_backlog_series(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a daily backlog series (accepted minus allocated cumulative totals).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        ['dt_received_inv','dt_alloc_invest'].\n",
    "    start : pd.Timestamp\n",
    "    end : pd.Timestamp\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        ['date','backlog_available'].\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'dt_received_inv':[pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest':[pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ... })\n",
    "    >>> start, end = pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05')\n",
    "    >>> backlog = build_backlog_series(typed, start, end)\n",
    "    >>> list(backlog.columns)\n",
    "    ['date', 'backlog_available']\n",
    "    >>> backlog.iloc[-1]['backlog_available']  # 2 received, 1 allocated -> 1\n",
    "    1\n",
    "    \"\"\"\n",
    "    accepted = (\n",
    "        typed[['dt_received_inv']]\n",
    "        .dropna()\n",
    "        .assign(date=lambda d: d['dt_received_inv'].dt.normalize())['date']\n",
    "        .value_counts()\n",
    "        .sort_index()\n",
    "    )\n",
    "    allocated = (\n",
    "        typed[['dt_alloc_invest']]\n",
    "        .dropna()\n",
    "        .assign(date=lambda d: d['dt_alloc_invest'].dt.normalize())['date']\n",
    "        .value_counts()\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    idx = pd.date_range(start, end, freq='D')\n",
    "    acc = accepted.reindex(idx, fill_value=0).cumsum()\n",
    "    allo = allocated.reindex(idx, fill_value=0).cumsum()\n",
    "    backlog = (acc - allo).rename('backlog_available').to_frame()\n",
    "    backlog.index.name = 'date'\n",
    "    return backlog.reset_index()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- fabricate a tiny typed dataset ---\n",
    "typed = pd.DataFrame({\n",
    "    'case_id': ['C1','C2'],\n",
    "    'investigator': ['Alice','Bob'],\n",
    "    'team': ['T1','T1'],\n",
    "    'role': ['',''],\n",
    "    'fte': [1.0, 0.8],\n",
    "    'staff_id': ['S1','S2'],\n",
    "\n",
    "    # key dates\n",
    "    'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-02')],\n",
    "    'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-03')],\n",
    "    'dt_alloc_team': [pd.NaT, pd.NaT],\n",
    "    'dt_pg_signoff': [pd.NaT, pd.Timestamp('2025-01-08')],\n",
    "    'dt_close': [pd.Timestamp('2025-01-06'), pd.NaT],\n",
    "\n",
    "    # events\n",
    "    'dt_legal_req_1': [pd.NaT, pd.Timestamp('2025-01-04')],\n",
    "    'dt_legal_req_2': [pd.NaT, pd.NaT],\n",
    "    'dt_legal_req_3': [pd.NaT, pd.NaT],\n",
    "    'dt_legal_approval': [pd.NaT, pd.NaT],\n",
    "    'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    'dt_flagged': [pd.NaT, pd.NaT],\n",
    "})\n",
    "\n",
    "# --- run horizon, events, wip, and panel ---\n",
    "start, end = date_horizon(typed, pad_days=3)\n",
    "daily, backlog, events = build_daily_panel(typed, start, end)\n",
    "\n",
    "print(\"Start/End:\", start.date(), end.date())\n",
    "print(\"Daily shape:\", daily.shape)\n",
    "print(\"Backlog shape:\", backlog.shape)\n",
    "print(\"Events shape:\", events.shape)\n",
    "\n",
    "print(\"\\nDaily head:\\n\", daily.head())\n",
    "print(\"\\nBacklog tail:\\n\", backlog.tail())\n",
    "print(\"\\nEvents:\\n\", events.sort_values(['date','staff_id','event']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a reusable function to load raw data\n",
    "def load_raw(p: Path, force_encoding: str | None = None):\n",
    "    \"\"\"\n",
    "    Load CSV/XLSX with robust encoding handling.\n",
    "    - If force_encoding is given, use it.\n",
    "    - Otherwise try common encodings in order and fall back to a safe decode.\n",
    "    Returns: (df, colmap)\n",
    "    \"\"\"\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd, numpy as np, io, re\n",
    "\n",
    "# Conditional branch\n",
    "    if not p.exists():\n",
    "\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "# Excel files are not affected by CSV encoding issues\n",
    "# Conditional branch\n",
    "    if p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "# Load an Excel sheet into a DataFrame\n",
    "        df = pd.read_excel(p, dtype=str)\n",
    "# Fallback branch\n",
    "    else:\n",
    "        tried = []\n",
    "        encodings_to_try = (\n",
    "            [force_encoding] if force_encoding else\n",
    "            [\"utf-8-sig\", \"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-16\", \"utf-16le\", \"utf-16be\"]\n",
    "        )\n",
    "\n",
    "        df = None\n",
    "        last_err = None\n",
    "# Loop over a sequence\n",
    "        for enc in encodings_to_try:\n",
    "# Try a block of code that may raise errors\n",
    "            try:\n",
    "# Load a CSV file into a DataFrame\n",
    "                df = pd.read_csv(p, dtype=str, sep=None, engine=\"python\",\n",
    "                                 encoding=enc, encoding_errors=\"strict\")\n",
    "                break\n",
    "# Handle errors from the try block\n",
    "            except UnicodeDecodeError as e:\n",
    "                tried.append(enc); last_err = e\n",
    "            except Exception as e:\n",
    "# Other parse errors (separator/quotes) â€“ keep trying other encodings\n",
    "                tried.append(enc); last_err = e\n",
    "\n",
    "# Last-resort: decode with cp1252 but *replace* bad bytes\n",
    "# Conditional branch\n",
    "        if df is None:\n",
    "# Try a block of code that may raise errors\n",
    "            try:\n",
    "# Load a CSV file into a DataFrame\n",
    "                df = pd.read_csv(p, dtype=str, sep=None, engine=\"python\",\n",
    "                                 encoding=\"cp1252\", encoding_errors=\"replace\")\n",
    "# Print a message or value\n",
    "                print(f\"[load_raw] WARNING: used cp1252 with replacement after failed encodings: {tried}\")\n",
    "# Handle errors from the try block\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to read CSV. Tried encodings {tried}. Last error: {last_err}\"\n",
    "                ) from e\n",
    "                \n",
    "# Trim whitespace across all string columns\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    colmap = {re.sub(r\"\\s+\", \" \", str(c).strip().lower()): c for c in df.columns}\n",
    "# Return a value from a function\n",
    "    return df, colmap\n",
    "\n",
    "\n",
    "# Define a reusable function\n",
    "def col(df, colmap, name):\n",
    "# Import libraries/modules for use below\n",
    "    import numpy as np\n",
    "    k=normalise_col(name)\n",
    "# Return a value from a function\n",
    "    if k in colmap: return df[colmap[k]]\n",
    "# Loop over a sequence\n",
    "    for kk,v in colmap.items():\n",
    "# Return a value from a function\n",
    "        if k in kk or kk in k: return df[v]\n",
    "# Use NumPy for numeric operations\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "# Define a reusable function\n",
    "def engineer(df, colmap):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Use pandas functionality to rename the most important column to the corresponding variables\n",
    "    out=pd.DataFrame({'case_id':col(df,colmap,'ID'),\n",
    "                      'investigator':col(df,colmap,'Investigator'),'team':col(df,colmap,'Team'),\n",
    "                      'fte':pd.to_numeric(col(df,colmap,'Investigator FTE'), errors='coerce')})\n",
    "    out['dt_received_inv']=parse_date_series(col(df,colmap,'Date Received in Investigations'))\n",
    "    out['dt_alloc_invest']=parse_date_series(col(df,colmap,'Date allocated to current investigator'))\n",
    "    out['dt_alloc_team']=parse_date_series(col(df,colmap,'Date allocated to team'))\n",
    "    out['dt_pg_signoff']=parse_date_series(col(df,colmap,'PG Sign off date'))\n",
    "    out['dt_close']=parse_date_series(col(df,colmap,'Closure Date'))\n",
    "    out['dt_legal_req_1']=parse_date_series(col(df,colmap,'Date of Legal Review Request 1'))\n",
    "    out['dt_legal_rej_1']=parse_date_series(col(df,colmap,'Date Legal Rejects 1'))\n",
    "    out['dt_legal_req_2']=parse_date_series(col(df,colmap,'Date of Legal Review Request 2'))\n",
    "    out['dt_legal_rej_2']=parse_date_series(col(df,colmap,'Date Legal Rejects 2'))\n",
    "    out['dt_legal_req_3']=parse_date_series(col(df,colmap,'Date of Legel Review Request 3'))\n",
    "    out['dt_legal_approval']=parse_date_series(col(df,colmap,'Legal Approval Date'))\n",
    "    out['dt_date_of_order']=parse_date_series(col(df,colmap,'Date Of Order'))\n",
    "    out['dt_flagged']=parse_date_series(col(df,colmap,'Flagged Date'))\n",
    "    out['fte']=out['fte'].fillna(1.0); out['staff_id']=out['investigator'].apply(hash_id); out['role']=''; return out\n",
    "\n",
    "    \n",
    "# Define a reusable function to define the horizon\n",
    "def date_horizon(typed, pad_days:int=14):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Use pandas functionality to contatinate three columns and find min as the satrt date\n",
    "    start=pd.concat([typed['dt_received_inv'],typed['dt_alloc_invest'],typed['dt_alloc_team']]).min()\n",
    "# Use pandas functionality to contatinate three columns and find max as the end date\n",
    "    end=pd.concat([typed['dt_close'],typed['dt_pg_signoff'],typed['dt_date_of_order']]).max()\n",
    "    if pd.isna(start): start=pd.Timestamp.today().normalize()-pd.Timedelta(days=30)\n",
    "    if pd.isna(end): end=pd.Timestamp.today().normalize()\n",
    "    end=end+pd.Timedelta(days=pad_days); return start.normalize(), end.normalize()\n",
    "\n",
    "    \n",
    "# Define a reusable function to build event log\n",
    "def build_event_log(typed):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "    rec=[]\n",
    "# Loop over a sequence\n",
    "    for _,r in typed.iterrows():\n",
    "        sid,team,fte,cid=r['staff_id'],r['team'],r['fte'],r['case_id']\n",
    "# Define a reusable function\n",
    "        def add(dt,etype):\n",
    "            if pd.isna(dt): return\n",
    "            rec.append({'date':dt.normalize(),'staff_id':sid,'team':team,'fte':fte,'case_id':cid,'event':etype,'meta':''})\n",
    "        add(r['dt_alloc_invest'],'newcase'); add(r['dt_legal_req_1'],'legal_request'); add(r['dt_legal_req_2'],'legal_request'); add(r['dt_legal_req_3'],'legal_request'); add(r['dt_legal_approval'],'legal_approval'); add(r['dt_date_of_order'],'court_order')\n",
    "# Use pandas functionality\n",
    "    ev=pd.DataFrame.from_records(rec)\n",
    "# Use pandas functionality\n",
    "    return ev if not ev.empty else pd.DataFrame(columns=['date','staff_id','team','fte','case_id','event','meta'])\n",
    "# Define a reusable function to build wip\n",
    "def build_wip_series(typed,start,end):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Fill missing values with a default\n",
    "    end_dt=typed['dt_close'].fillna(typed['dt_pg_signoff']).fillna(end)\n",
    "# Drop rows with missing values\n",
    "    intervals=pd.DataFrame({'staff_id':typed['staff_id'],'team':typed['team'],'start':typed['dt_alloc_invest'],'end':end_dt}).dropna()\n",
    "\n",
    "    deltas=[]\n",
    "# Loop over a sequence\n",
    "    for _,r in intervals.iterrows():\n",
    "        s=r['start'].normalize(); e=r['end'].normalize()\n",
    "        if s>end or e<start: continue\n",
    "        s=max(s,start); e=min(e,end)\n",
    "        deltas.append((r['staff_id'],r['team'],s,1)); deltas.append((r['staff_id'],r['team'],e+pd.Timedelta(days=1),-1))\n",
    "    if not deltas: return pd.DataFrame(columns=['date','staff_id','team','wip'])\n",
    "    deltas=pd.DataFrame(deltas, columns=['staff_id','team','date','delta'])\n",
    "    all_dates=pd.DataFrame({'date':pd.date_range(start,end,freq='D')})\n",
    "\n",
    "    rows=[]\n",
    "# Group rows and compute aggregations\n",
    "    for (sid,team),g in deltas.groupby(['staff_id','team']):\n",
    "# Group rows and compute aggregations\n",
    "        gg=g.groupby('date', as_index=False)['delta'].sum()\n",
    "# Fill missing values with a default\n",
    "        grid=all_dates.merge(gg,on='date', how='left').fillna({'delta':0})\n",
    "        grid['wip']=grid['delta'].cumsum(); grid['staff_id']=sid; grid['team']=team; rows.append(grid[['date','staff_id','team','wip']])\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['date','staff_id','team','wip'])\n",
    "\n",
    "    \n",
    "# Define a reusable function to build backlog series\n",
    "def build_backlog_series(typed,start,end):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Drop rows with missing values\n",
    "    accepted=typed[['dt_received_inv']].dropna().assign(date=lambda d:d['dt_received_inv'].dt.normalize())['date'].value_counts().sort_index()\n",
    "# Drop rows with missing values\n",
    "    allocated=typed[['dt_alloc_invest']].dropna().assign(date=lambda d:d['dt_alloc_invest'].dt.normalize())['date'].value_counts().sort_index()\n",
    "    idx=pd.date_range(start,end,freq='D'); acc=accepted.reindex(idx, fill_value=0).cumsum(); allo=allocated.reindex(idx, fill_value=0).cumsum()\n",
    "# Rename columns for clarity/consistency\n",
    "    backlog=(acc-allo).rename('backlog_available').to_frame(); backlog.index.name='date'; return backlog.reset_index()\n",
    "\n",
    "\n",
    "# Define a reusable function to build daily panel\n",
    "def build_daily_panel(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp):\n",
    "    \"\"\"Combine WIP, event log, and calendar features. Returns: (daily, backlog, events).\"\"\"\n",
    "    ev = build_event_log(typed)\n",
    "    wip = build_wip_series(typed, start, end)\n",
    "    backlog = build_backlog_series(typed, start, end)\n",
    "\n",
    "# Base grid: all staff x all dates\n",
    "    staff = typed[[\"staff_id\",\"team\",\"role\",\"fte\"]].drop_duplicates()\n",
    "    dates = pd.DataFrame({\"date\": pd.date_range(start, end, freq=\"D\")})\n",
    "# Combine tables by key columns\n",
    "    grid = dates.assign(key=1).merge(staff.assign(key=1), on=\"key\").drop(columns=\"key\")\n",
    "\n",
    "# Merge WIP\n",
    "# Fill missing values with a default\n",
    "    grid = grid.merge(wip, on=[\"date\",\"staff_id\",\"team\"], how=\"left\").fillna({\"wip\":0})\n",
    "\n",
    "# Event flags\n",
    "# Conditional branch\n",
    "    if not ev.empty:\n",
    "        ev_flags = (\n",
    "# Create or transform columns\n",
    "            ev.assign(flag=1)\n",
    "              .pivot_table(index=[\"date\",\"staff_id\"], columns=\"event\", values=\"flag\", aggfunc=\"max\")\n",
    "# Reset index to turn group keys into columns\n",
    "              .reset_index().rename_axis(None, axis=1)\n",
    "        )\n",
    "# Combine tables by key columns\n",
    "        grid = grid.merge(ev_flags, on=[\"date\",\"staff_id\"], how=\"left\")\n",
    "# Loop over a sequence\n",
    "    for c in [\"newcase\",\"legal_request\",\"legal_approval\",\"court_order\"]:\n",
    "# Conditional branch\n",
    "        if c not in grid:\n",
    "            grid[c] = 0\n",
    "# Fallback branch\n",
    "        else:\n",
    "# Cast column(s) to a specific dtype\n",
    "            grid[c] = grid[c].fillna(0).astype(int)\n",
    "\n",
    "# --- SAFE time_since_last_pickup (no index mismatch) ---\n",
    "# Sort rows by specified columns\n",
    "    grid = grid.sort_values([\"staff_id\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "    grp = grid.groupby(\"staff_id\", sort=False)\n",
    "    runs = grp[\"newcase\"].transform(lambda s: (s == 1).cumsum())\n",
    "# Group rows and compute aggregations\n",
    "    grid[\"time_since_last_pickup\"] = grid.groupby([grid[\"staff_id\"], runs]).cumcount()\n",
    "    mask_no_pickups = grp[\"newcase\"].transform(\"sum\") == 0\n",
    "# Select/assign rows/columns by label/position\n",
    "    grid.loc[mask_no_pickups, \"time_since_last_pickup\"] = 99\n",
    "\n",
    "# Calendar\n",
    "    grid[\"dow\"] = grid[\"date\"].dt.day_name().str[:3]\n",
    "    grid[\"season\"] = grid[\"date\"].dt.month.map(month_to_season)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"term_flag\"] = grid[\"date\"].dt.month.map(is_term_month).astype(int)\n",
    "    grid[\"bank_holiday\"] = 0\n",
    "\n",
    "# New starters\n",
    "    first_alloc = (\n",
    "# Drop rows with missing values\n",
    "        typed.dropna(subset=[\"dt_alloc_invest\"])\n",
    "# Group rows and compute aggregations\n",
    "             .groupby(\"staff_id\")[\"dt_alloc_invest\"].min()\n",
    "# Rename columns for clarity/consistency\n",
    "             .rename(\"first_alloc\")\n",
    "    )\n",
    "    \n",
    "# Combine tables by key columns\n",
    "    grid = grid.merge(first_alloc, on=\"staff_id\", how=\"left\")\n",
    "    grid[\"weeks_since_start\"] = (\n",
    "        (grid[\"date\"] - grid[\"first_alloc\"]).dt.days // 7\n",
    "# Cast column(s) to a specific dtype\n",
    "    ).fillna(0).clip(lower=0).astype(int)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"is_new_starter\"] = (grid[\"weeks_since_start\"] < 4).astype(int)\n",
    "\n",
    "# Default flags\n",
    "    grid[\"mentoring_flag\"] = 0\n",
    "    grid[\"trainee_flag\"] = 0\n",
    "\n",
    "# Backlog (same for all staff/day)\n",
    "# Fill missing values with a default\n",
    "    grid = grid.merge(backlog, on=\"date\", how=\"left\").fillna({\"backlog_available\":0})\n",
    "\n",
    "# Final columns\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"event_newcase\"] = grid[\"newcase\"].astype(int)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"event_legal\"]   = ((grid[\"legal_request\"] + grid[\"legal_approval\"]) > 0).astype(int)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"event_court\"]   = grid[\"court_order\"].astype(int)\n",
    "    grid = grid.drop(columns=[\"newcase\",\"legal_request\",\"legal_approval\",\"court_order\",\"first_alloc\"])\n",
    "\n",
    "    cols = [\"date\",\"staff_id\",\"team\",\"role\",\"fte\",\n",
    "\n",
    "            \"is_new_starter\",\"weeks_since_start\",\n",
    "\n",
    "            \"wip\",\"time_since_last_pickup\",\n",
    "\n",
    "            \"mentoring_flag\",\"trainee_flag\",\n",
    "\n",
    "            \"backlog_available\",\"term_flag\",\"season\",\"dow\",\"bank_holiday\",\n",
    "\n",
    "            \"event_newcase\",\"event_legal\",\"event_court\"]\n",
    "# Reset index to turn group keys into columns\n",
    "    daily = grid[cols].sort_values([\"staff_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# <-- IMPORTANT: return the frames\n",
    "# Return a value from a function\n",
    "    return daily, backlog, ev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### data loading, exporting outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file into a DataFrame\n",
    "df_test = pd.read_csv(RAW_PATH, dtype=str, sep=None, engine=\"python\", encoding=\"cp1252\")\n",
    "\n",
    "df_test.head()\n",
    "\n",
    "df_raw, colmap = load_raw(RAW_PATH)\n",
    "# print(f\"df_raw: \", df_raw)\n",
    "# print(f\"colmap: \", df_raw)\n",
    "\n",
    "typed = engineer(df_raw, colmap)\n",
    "# Print a message or value\n",
    "print(f\"typed: \", typed)\n",
    "\n",
    "start, end = date_horizon(typed, 14)\n",
    "# Print a message or value\n",
    "print(f\"start: \", start)\n",
    "# Print a message or value\n",
    "print(f\"end: \", end)\n",
    "\n",
    "daily, backlog, events = build_daily_panel(typed, start, end)\n",
    "\n",
    "# (optional) save to disk\n",
    "# Save a DataFrame to CSV\n",
    "daily.to_csv(OUT_DIR / \"investigator_daily.csv\", index=False)\n",
    "# Save a DataFrame to CSV\n",
    "backlog.to_csv(OUT_DIR / \"backlog_series.csv\", index=False)\n",
    "# Save a DataFrame to CSV\n",
    "events.to_csv(OUT_DIR / \"event_log.csv\", index=False)\n",
    "\n",
    "# # Print a message or value\n",
    "# print(f\"{len(daily):,} daily rows\")\n",
    "# # Print a message or value\n",
    "# print(\"Date range:\", daily[\"date\"].min().date(), \"â†’\", daily[\"date\"].max().date())\n",
    "# # Print a message or value\n",
    "# print(\"Investigators:\", daily[\"staff_id\"].nunique())\n",
    "# # Print a message or value\n",
    "# print(\"Total new case events:\", int(daily[\"event_newcase\"].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## > # === Stage 2 extension: historical \"investigated so far\" + 90-day daily predictions\n",
    "> # Assumes Stage-2 output exists at data/out/investigator_daily.csv\n",
    "> # \"Investigated\" here = daily pickups (event_newcase). Swap to a different event if needed.\n",
    "> \n",
    "> - This model builds the historical time series of how many cases were investigated (interpreted as new case pickups = event_newcase) per investigator, role, and team, including the cumulative (â€œso farâ€) curves;\n",
    "> \n",
    "> - fits simple Gammaâ€“Poisson posteriors and produces 90-day daily predictions (mean and 5â€“95% credible interval) for each investigator/role/team.\n",
    "> \n",
    "> - It saves six CSVs into data/out/ so we can join/plot later.\n",
    "> \n",
    "> **For â€œinvestigatedâ€ = completed rather than picked up, just change the column used from event_newcase to the right completion flag (e.g., if you track completions per day, swap it in where noted).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### imports and environment setup, data loading, joining/merging datasets, aggregation/grouping, data cleaning, sorting, feature engineering, exporting outputs, prediction/forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Stage 2 extension: historical \"investigated so far\" + 90-day daily predictions\n",
    "\n",
    "# Assumes Stage-2 output exists at data/out/investigator_daily.csv\n",
    "\n",
    "# \"Investigated\" here = daily pickups (event_newcase). Swap to a different event if needed.\n",
    "\n",
    "\n",
    "from scipy.stats import nbinom  # Negative Binomial for Gammaâ€“Poisson posterior predictive\n",
    "\n",
    "\n",
    "OUT = Path(\"data/out\")\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_path = OUT / \"investigator_daily.csv\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "# Load a CSV file into a DataFrame\n",
    "daily = pd.read_csv(daily_path, parse_dates=[\"date\"])\n",
    "\n",
    "# If you want \"investigated\" to mean something else, swap this column:\n",
    "\n",
    "target_col = \"event_newcase\"   # <--- change if needed (e.g., 'event_court' or a completion flag)\n",
    "\n",
    "# Cast column(s) to a specific dtype\n",
    "daily[target_col] = pd.to_numeric(daily[target_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "# Fill missing values with a default\n",
    "daily[\"team\"] = daily.get(\"team\", pd.Series(index=daily.index)).fillna(\"Unknown\")\n",
    "# Fill missing values with a default\n",
    "daily[\"role\"] = daily.get(\"role\", pd.Series(index=daily.index)).fillna(\"Unknown\")\n",
    "\n",
    "last_date = daily[\"date\"].max()\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "# 1) HISTORICAL: daily counts + cumulative (\"so far\") per entity\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "# Investigator\n",
    "# Group rows and compute aggregations\n",
    "hist_inv = (daily.groupby([\"date\",\"staff_id\",\"team\",\"role\"], as_index=False)[target_col]\n",
    "\n",
    "                 .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                 .rename(columns={target_col:\"daily_pickups\"}))\n",
    "# Sort rows by specified columns\n",
    "hist_inv = hist_inv.sort_values([\"staff_id\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "hist_inv[\"cum_pickups\"] = hist_inv.groupby(\"staff_id\")[\"daily_pickups\"].cumsum()\n",
    "# Save a DataFrame to CSV\n",
    "hist_inv.to_csv(OUT / \"hist_pickups_investigator.csv\", index=False)\n",
    "\n",
    "\n",
    "# Role\n",
    "# Group rows and compute aggregations\n",
    "hist_role = (daily.groupby([\"date\",\"role\"], as_index=False)[target_col]\n",
    "\n",
    "                  .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                  .rename(columns={target_col:\"daily_pickups\"}))\n",
    "# Sort rows by specified columns\n",
    "hist_role = hist_role.sort_values([\"role\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "hist_role[\"cum_pickups\"] = hist_role.groupby(\"role\")[\"daily_pickups\"].cumsum()\n",
    "# Save a DataFrame to CSV\n",
    "hist_role.to_csv(OUT / \"hist_pickups_role.csv\", index=False)\n",
    "\n",
    "\n",
    "# Team\n",
    "# Group rows and compute aggregations\n",
    "hist_team = (daily.groupby([\"date\",\"team\"], as_index=False)[target_col]\n",
    "\n",
    "                  .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                  .rename(columns={target_col:\"daily_pickups\"}))\n",
    "# Sort rows by specified columns\n",
    "hist_team = hist_team.sort_values([\"team\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "hist_team[\"cum_pickups\"] = hist_team.groupby(\"team\")[\"daily_pickups\"].cumsum()\n",
    "# Save a DataFrame to CSV\n",
    "hist_team.to_csv(OUT / \"hist_pickups_team.csv\", index=False)\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "# 2) PREDICTIONS: 90-day daily counts per entity (Gammaâ€“Poisson)\n",
    "#    Posterior (rate-param Gamma prior Î±0=1, Î²0=1):\n",
    "#      For a single day ahead, y ~ NegBinom(r=Î±_post, p=Î²_post/(Î²_post+1)),\n",
    "#      E[y] = Î±_post / Î²_post, with 5â€“95% credible interval from NB quantiles.\n",
    "# =====================================================================\n",
    "\n",
    "# Define a reusable function\n",
    "def posterior_by_key(daily_df: pd.DataFrame, key_cols: list[str]) -> pd.DataFrame:\n",
    "    # Aggregate to per-day counts for the entity\n",
    "# Group rows and compute aggregations\n",
    "    g_daily = (daily_df.groupby(key_cols + [\"date\"], as_index=False)[target_col]\n",
    "\n",
    "                      .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                      .rename(columns={target_col:\"y\"}))\n",
    "    # Total counts and exposure days (T = # unique dates observed for that entity)\n",
    "# Group rows and compute aggregations\n",
    "    g_total = (g_daily.groupby(key_cols, as_index=False)\n",
    "# Apply aggregation(s) to grouped data\n",
    "                      .agg(y_total=(\"y\",\"sum\"),\n",
    "\n",
    "                           T=(\"date\",\"nunique\")))\n",
    "\n",
    "    # Weak prior\n",
    "    alpha0, beta0 = 1.0, 1.0\n",
    "\n",
    "    g_total[\"alpha_post\"] = alpha0 + g_total[\"y_total\"]\n",
    "\n",
    "    g_total[\"beta_post\"]  = beta0 + g_total[\"T\"]\n",
    "\n",
    "    # Negative Binomial params for 1-day-ahead predictive:\n",
    "    # In scipy: nbinom(n=r, p) has mean = r*(1-p)/p. Choose p = Î²/(Î²+1) â†’ mean = Î±/Î²\n",
    "\n",
    "    g_total[\"p_nb\"] = g_total[\"beta_post\"] / (g_total[\"beta_post\"] + 1.0)\n",
    "\n",
    "    g_total[\"r_nb\"] = g_total[\"alpha_post\"]\n",
    "\n",
    "    # Daily expected value and 90% credible interval\n",
    "\n",
    "    g_total[\"mean\"] = g_total[\"r_nb\"] * (1 - g_total[\"p_nb\"]) / g_total[\"p_nb\"]\n",
    "\n",
    "    g_total[\"p05\"]  = nbinom.ppf(0.05, n=g_total[\"r_nb\"], p=g_total[\"p_nb\"])\n",
    "\n",
    "    g_total[\"p95\"]  = nbinom.ppf(0.95, n=g_total[\"r_nb\"], p=g_total[\"p_nb\"])\n",
    "# Return a value from a function\n",
    "    return g_total[key_cols + [\"mean\",\"p05\",\"p95\"]]\n",
    "\n",
    "\n",
    "H = 90\n",
    "# Use pandas functionality\n",
    "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=H, freq=\"D\")\n",
    "\n",
    "# Investigator predictions\n",
    "post_inv = posterior_by_key(daily, [\"staff_id\"])\n",
    "\n",
    "# Join convenient labels (first observed team/role for each staff)\n",
    "\n",
    "first_map = daily[[\"staff_id\",\"team\",\"role\"]].drop_duplicates(\"staff_id\")\n",
    "# Combine tables by key columns\n",
    "post_inv = post_inv.merge(first_map, on=\"staff_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Create or transform columns\n",
    "f_inv = (post_inv.assign(key=1)\n",
    "# Combine tables by key columns\n",
    "                .merge(pd.DataFrame({\"date\":future_dates, \"key\":1}), on=\"key\")\n",
    "\n",
    "                .drop(columns=\"key\"))[[\"date\",\"staff_id\",\"team\",\"role\",\"mean\",\"p05\",\"p95\"]]\n",
    "# Save a DataFrame to CSV\n",
    "f_inv.to_csv(OUT / \"forecast_pickups_investigator.csv\", index=False)\n",
    "\n",
    "\n",
    "# Role predictions\n",
    "post_role = posterior_by_key(daily, [\"role\"])\n",
    "# Create or transform columns\n",
    "f_role = (post_role.assign(key=1)\n",
    "# Combine tables by key columns\n",
    "          .merge(pd.DataFrame({\"date\":future_dates, \"key\":1}), on=\"key\")\n",
    "\n",
    "          .drop(columns=\"key\"))[[\"date\",\"role\",\"mean\",\"p05\",\"p95\"]]\n",
    "# Save a DataFrame to CSV\n",
    "f_role.to_csv(OUT / \"forecast_pickups_role.csv\", index=False)\n",
    "\n",
    "\n",
    "# Team predictions\n",
    "post_team = posterior_by_key(daily, [\"team\"])\n",
    "# Create or transform columns\n",
    "f_team = (post_team.assign(key=1)\n",
    "# Combine tables by key columns\n",
    "          .merge(pd.DataFrame({\"date\":future_dates, \"key\":1}), on=\"key\")\n",
    "\n",
    "          .drop(columns=\"key\"))[[\"date\",\"team\",\"mean\",\"p05\",\"p95\"]]\n",
    "# Save a DataFrame to CSV\n",
    "f_team.to_csv(OUT / \"forecast_pickups_team.csv\", index=False)\n",
    "\n",
    "# Print\n",
    "print(\"Saved:\\n -\", OUT / \"hist_pickups_investigator.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"hist_pickups_role.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"hist_pickups_team.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"forecast_pickups_investigator.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"forecast_pickups_role.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"forecast_pickups_team.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "\n",
    "> # Bayesian Stage-3 implementation \n",
    "> works directly off the outputs already generated (data/out/investigator_daily.csv and data/out/backlog_series.csv). \n",
    "> It does two things:\n",
    "> 1. Backlog forecasting (next 90 days) via a conjugate Bayesian linear model on daily backlog deltas with an AR(1) feature + weekday + annual seasonality â€” returns full predictive uncertainty.\n",
    "> \n",
    "> 2. Per-investigator pickup rates via Gammaâ€“Poisson posteriors (hierarchical-by-investigator baseline), including 7-day and 28-day expected pickups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "> **Original note:**\n",
    ">\n",
    "> # === Stage 3: Bayesian predictive models (Backlog + Investigator pickups) ===\n",
    "> \n",
    "> The backlog model This model is a conjugate Bayesian regression on daily change, with:\n",
    ">     1. AR(1) feature via yesterdayâ€™s delta,\n",
    ">     2. Weekday effects,\n",
    ">     3. Annual seasonality (sin/cos).\n",
    "> \n",
    "> It produces proper predictive uncertainty and naturally handles short histories (weak priors).\n",
    "> \n",
    "> If you want team-level or role-level pickup posteriors, copy the per-investigator block and replace groupby(\"staff_id\") with groupby(\"team\") or any other cohort, with the same Gammaâ€“Poisson math.\n",
    "> \n",
    "> If you have bank holiday flags in your daily panel, you can add them as another regressor column in X (just remember to include it consistently when building make_x_row for the forecast).\n",
    "> \n",
    "> ## This model generate the following outputs\n",
    "> 1. Backlog forecaster â€” predicts how many cases will be waiting on each of the next 90 days.\n",
    "> 2. Pickup-rate estimator â€” estimates how often each investigator typically picks up a new case, and how many theyâ€™re likely to pick up over the next 1â€“4 weeks.\n",
    "> \n",
    "> ## Inputs\n",
    "> 1. The daily backlog totals from Stage-2 (backlog_series.csv).\n",
    "> 2. The per-investigator daily table from Stage-2 (investigator_daily.csv) which includes the daily â€œnew case picked upâ€ flag.\n",
    "> \n",
    "> \n",
    "> ## 1) Backlog forecaster (daily totals)\n",
    "> Instead of predicting the raw backlog level directly, the code predicts the daily change in backlog (todayâ€™s backlog minus yesterdayâ€™s).\n",
    "> That change tends to be:\n",
    "> - a bit like yesterdayâ€™s change (momentum),\n",
    "> - slightly different on different weekdays (e.g., Mondays vs Fridays),\n",
    "> - and nudged by time-of-year patterns (seasonality).\n",
    "> \n",
    "> ### Bayesian\n",
    "> - We start with very weak, generic expectations (â€œpriorsâ€), look at the data, and update our beliefs to a â€œposterior.â€ \n",
    "> - Then we simulate many possible futures consistent with what we learned. This gives not just a single forecast, but a spread (best guess + uncertainty bands).\n",
    "> \n",
    "> ### What the code actually does\n",
    "> - Builds a simple recipe for daily change: **todayâ€™s change â‰ˆ intercept + (yesterdayâ€™s change) + weekday effect + seasonal wiggle + random noise**\n",
    "> \n",
    "> - Fits that recipe with a Bayes method thatâ€™s efficient (a conjugate prior). This gives us a clean way to learn from the data and quantify uncertainty.\n",
    "> \n",
    "> - Simulates thousands of future paths day-by-day: each new dayâ€™s change depends on the previous simulated dayâ€™s change (so it keeps momentum).\n",
    "> \n",
    "> - Converts those simulated changes back into backlog levels, and clips at zero (no negative backlog).\n",
    "> \n",
    "> - Summarises the simulations for each future date as:\n",
    ">     - mean/median (central forecasts),\n",
    ">     - p05/p95 (a 90% â€œcredible intervalâ€),\n",
    ">     - p20/p80 (a tighter middle band).\n",
    "> Example: **If p05 = 120 and p95 = 180 on a date, the model is saying â€œgiven history and patterns, thereâ€™s about a 90% chance backlog will be between 120 and 180 that day.â€**\n",
    "> \n",
    "> \n",
    "> ## 2) Investigator pickup rates (how often investigator take new cases)\n",
    "> Each investigatorâ€™s daily pickup count is treated as a â€œcounting processâ€ (like number of arrivals per day). \n",
    "> Some people pick up more, some less, and some have sparse histories. We want fair estimates that stabilise when data is thin.\n",
    "> \n",
    "> ### Gammaâ€“Poisson\n",
    "> - We assume daily pickups follow a Poisson process (a common, simple model for counts).\n",
    "> - We put a Gamma prior on each personâ€™s true underlying daily rate (how often they pick up).\n",
    "> - Combining those gives a neat closed-form update (no heavy computation): you get a posterior for each personâ€™s rate that blends their data with a little stabilising prior.\n",
    "> \n",
    "> ### What the code outputs\n",
    "> For each investigator:\n",
    "> 1. A posterior mean daily pickup rate (our best estimate),\n",
    "> 2. A credible range (p05â€“p95) to show uncertainty,\n",
    "> 3. Expected pickups over the next 7 and 28 days (rate Ã— days).\n",
    "> \n",
    "> Example of useability: \n",
    "> **Rank investigators by posterior mean (or lower-bound like p05 for conservative planning) to understand expected intake capacity in the short term.**\n",
    "> \n",
    "> Outputs:\n",
    "> 1. backlog_forecast_bayes.csv: one row per future day with mean, median, p05, p20, p80, p95.\n",
    "> 2. investigator_pickup_posterior.csv: one row per investigator with posterior rate and 7-/28-day expectations.\n",
    "> \n",
    "> ## Assumptions (and what to tweak)\n",
    "> 1. Momentum matters: tomorrowâ€™s change tends to resemble yesterdayâ€™s.\n",
    "> 2. Weekdays differ: e.g., fewer allocations on weekends.\n",
    "> 3. Seasonality: simple annual pattern (sine/cosine); can add school terms or fiscal periods.\n",
    "> 4. Counts are Poisson: good first pass; if pickups bunch up or are capped, consider a richer model later.\n",
    "> 5. Data quality: the forecast inherits any biases or gaps; adding bank holidays (already optional in your notebook) helps.\n",
    "> \n",
    "> ## Future developement\n",
    "> 1. Add bank holiday and term time flags as extra predictors in the backlog model.\n",
    "> 2. Estimate pickup rates by team/role (swap the group-by key).\n",
    "> 3. Move to a hierarchical pickup model (shares strength across investigators/teams) if data per person is very sparse.\n",
    "> \n",
    "> ### Why this is useful\n",
    "> 1. We can get actionable ranges, not just a single numberâ€”great for planning under uncertainty.\n",
    "> 2. The pickup posteriors turn noisy daily events into a stable, comparable measure of capacity.\n",
    "> 3. Itâ€™s all fast and transparent, so you can iterate quickly as new data arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### imports and environment setup, data loading, aggregation/grouping, data cleaning, sorting, exporting outputs, prediction/forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries/modules for use below\n",
    "from pathlib import Path\n",
    "# Import libraries/modules for use below\n",
    "from scipy.stats import invgamma, gamma as gamma_dist\n",
    "\n",
    "# ---- Locations ----\n",
    "BASE = Path(\"data\")\n",
    "\n",
    "OUT  = BASE / \"out\"\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_path   = OUT / \"investigator_daily.csv\"\n",
    "\n",
    "backlog_path = OUT / \"backlog_series.csv\"\n",
    "\n",
    "# ---- Load outputs from Stage-2 ----\n",
    "# Load a CSV file into a DataFrame\n",
    "daily   = pd.read_csv(daily_path, parse_dates=[\"date\"])\n",
    "# Load a CSV file into a DataFrame\n",
    "backlog = pd.read_csv(backlog_path, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---- Build daily delta series for backlog ----\n",
    "\n",
    "backlog[\"delta\"] = backlog[\"backlog_available\"].diff()\n",
    "# Drop rows with missing values\n",
    "backlog = backlog.dropna(subset=[\"delta\"]).reset_index(drop=True)\n",
    "\n",
    "# Design matrix for a conjugate Bayesian linear model:\n",
    "\n",
    "# y_t = delta_t ~ N(X_t beta, sigma^2), with X_t = [1, lag_delta, sin, cos, DOW dummies]\n",
    "\n",
    "df = backlog.copy()\n",
    "\n",
    "df[\"lag_delta\"] = df[\"delta\"].shift(1)\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=[\"lag_delta\"]).reset_index(drop=True)\n",
    "\n",
    "# Weekday effects (Mon=0..Sun=6), drop_first to avoid dummy trap\n",
    "\n",
    "df[\"dow\"] = df[\"date\"].dt.dayofweek\n",
    "# Use pandas functionality\n",
    "dow_dummies = pd.get_dummies(df[\"dow\"], prefix=\"dow\", drop_first=True)\n",
    "\n",
    "# Annual seasonality with sin/cos (period ~ 365.25)\n",
    "# Cast column(s) to a specific dtype\n",
    "day_of_year = df[\"date\"].dt.dayofyear.astype(float)\n",
    "# Use NumPy for numeric operations\n",
    "df[\"sin_annual\"] = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "# Use NumPy for numeric operations\n",
    "df[\"cos_annual\"] = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "\n",
    "# Use pandas functionality\n",
    "X = pd.concat([\n",
    "    pd.Series(1.0, index=df.index, name=\"intercept\"),\n",
    "\n",
    "    df[[\"lag_delta\", \"sin_annual\", \"cos_annual\"]],\n",
    "\n",
    "    dow_dummies\n",
    "\n",
    "], axis=1)\n",
    "\n",
    "y = df[\"delta\"].to_numpy(float)\n",
    "\n",
    "X_mat = X.to_numpy(float)\n",
    "\n",
    "# ---- Conjugate Normalâ€“Inverse-Gamma posterior ----\n",
    "\n",
    "# Prior: beta|sigma^2 ~ N(m0, sigma^2 V0),  sigma^2 ~ InvGamma(a0, b0)\n",
    "n, p = X_mat.shape\n",
    "# Use NumPy for numeric operations\n",
    "m0   = np.zeros(p)\n",
    "V0   = np.eye(p) * 1e6         # weakly-informative\n",
    "\n",
    "a0   = 2.0\n",
    "# Use NumPy for numeric operations\n",
    "yvar = float(np.var(y)) if np.isfinite(np.var(y)) and np.var(y) > 0 else 1.0\n",
    "b0   = yvar * (a0 - 1)\n",
    "\n",
    "\n",
    "XtX    = X_mat.T @ X_mat\n",
    "V0inv  = np.linalg.inv(V0)\n",
    "Vn     = np.linalg.inv(XtX + V0inv)\n",
    "mn     = Vn @ (V0inv @ m0 + X_mat.T @ y)\n",
    "an     = a0 + n/2.0\n",
    "bn     = b0 + 0.5*(y @ y + m0 @ V0inv @ m0 - mn @ np.linalg.inv(Vn) @ mn)\n",
    "\n",
    "# ---- Posterior predictive: forward simulate next H days with AR(1) lag ----\n",
    "\n",
    "H = 90         # forecast horizon (days)\n",
    "S = 4000       # posterior draws\n",
    "\n",
    "# Select/assign rows/columns by label/position\n",
    "last_delta   = float(df.iloc[-1][\"delta\"])\n",
    "# Select/assign rows/columns by label/position\n",
    "last_backlog = float(backlog.iloc[-1][\"backlog_available\"])\n",
    "# Select/assign rows/columns by label/position\n",
    "last_date    = df.iloc[-1][\"date\"]\n",
    "\n",
    "# Use pandas functionality\n",
    "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=H, freq=\"D\")\n",
    "future_dow   = future_dates.dayofweek\n",
    "# Use NumPy for numeric operations\n",
    "future_sin   = np.sin(2 * np.pi * future_dates.dayofyear / 365.25)\n",
    "# Use NumPy for numeric operations\n",
    "future_cos   = np.cos(2 * np.pi * future_dates.dayofyear / 365.25)\n",
    "dow_cols     = [c for c in X.columns if c.startswith(\"dow_\")]\n",
    "\n",
    "# Define a reusable function\n",
    "def make_x_row(lag_delta_val, idx):\n",
    "\n",
    "    # Build X* in the same column order as training X\n",
    "    dow = int(future_dow[idx])\n",
    "# Use NumPy for numeric operations\n",
    "    dd = np.zeros(len(dow_cols))\n",
    "# Loop over a sequence\n",
    "    for j, c in enumerate(dow_cols):\n",
    "# Try a block of code that may raise errors\n",
    "        try:\n",
    "            target = int(c.split(\"_\")[1])  # 'dow_3' -> 3\n",
    "# Handle errors from the try block\n",
    "        except Exception:\n",
    "            target = None\n",
    "\n",
    "        dd[j] = 1.0 if (target is not None and dow == target) else 0.0\n",
    "# Use NumPy for numeric operations\n",
    "    return np.concatenate(([1.0, lag_delta_val, future_sin[idx], future_cos[idx]], dd))\n",
    "\n",
    "# Use NumPy for numeric operations\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "# Robust Cholesky (add tiny jitter if near-singular)\n",
    "# Use NumPy for numeric operations\n",
    "evals = np.linalg.eigvals(Vn)\n",
    "# Conditional branch\n",
    "if np.min(np.real(evals)) < 1e-12:\n",
    "# Use NumPy for numeric operations\n",
    "    Vn = Vn + np.eye(p) * 1e-10\n",
    "# Use NumPy for numeric operations\n",
    "L = np.linalg.cholesky(Vn)\n",
    "\n",
    "# Sample (sigma^2, beta) from posterior\n",
    "\n",
    "sigma2 = invgamma.rvs(a=an, scale=bn, size=S, random_state=rng)\n",
    "\n",
    "z      = rng.standard_normal((S, p))\n",
    "# Use NumPy for numeric operations\n",
    "beta   = mn + np.sqrt(sigma2)[:, None] * (z @ L.T)\n",
    "\n",
    "# Simulate daily deltas forward with AR lag in X\n",
    "# Use NumPy for numeric operations\n",
    "delta_draws = np.zeros((S, H))\n",
    "# Loop over a sequence\n",
    "for s in range(S):\n",
    "\n",
    "    lag = last_delta\n",
    "# Use NumPy for numeric operations\n",
    "    bs  = beta[s]; sig = np.sqrt(sigma2[s])\n",
    "# Loop over a sequence\n",
    "    for h in range(H):\n",
    "\n",
    "        xh = make_x_row(lag, h)\n",
    "\n",
    "        mean_h = float(xh @ bs)\n",
    "\n",
    "        delta_h = mean_h + rng.normal(0.0, sig)\n",
    "\n",
    "        delta_draws[s, h] = delta_h\n",
    "\n",
    "        lag = delta_h\n",
    "\n",
    "# Transform to backlog levels; clip at zero\n",
    "# Use NumPy for numeric operations\n",
    "backlog_paths = last_backlog + np.cumsum(delta_draws, axis=1)\n",
    "# Use NumPy for numeric operations\n",
    "backlog_paths = np.clip(backlog_paths, 0, None)\n",
    "\n",
    "# Summaries\n",
    "\n",
    "q = [0.05, 0.2, 0.5, 0.8, 0.95]\n",
    "# Use NumPy for numeric operations\n",
    "Q = np.quantile(backlog_paths, q, axis=0).T\n",
    "# Use pandas functionality\n",
    "forecast_df = pd.DataFrame({\n",
    "\n",
    "    \"date\":  future_dates,\n",
    "\n",
    "    \"mean\":  backlog_paths.mean(axis=0),\n",
    "\n",
    "    \"median\":Q[:, 2],\n",
    "\n",
    "    \"p05\":   Q[:, 0],\n",
    "\n",
    "    \"p20\":   Q[:, 1],\n",
    "\n",
    "    \"p80\":   Q[:, 3],\n",
    "\n",
    "    \"p95\":   Q[:, 4],\n",
    "\n",
    "})\n",
    "# Save a DataFrame to CSV\n",
    "forecast_df.to_csv(OUT / \"backlog_forecast_bayes.csv\", index=False)\n",
    "\n",
    "# ---- Per-investigator pickup rates: Gammaâ€“Poisson posteriors ----\n",
    "\n",
    "# For each staff_id, y_i ~ Poisson(theta_i * T_i) with daily exposure T_i (days).\n",
    "\n",
    "# Prior theta_i ~ Gamma(alpha0, beta0) (rate parameterization) => posterior Gamma(alpha0 + y, beta0 + T).\n",
    "\n",
    "di = daily.copy()\n",
    "# Cast column(s) to a specific dtype\n",
    "di[\"event_newcase\"] = pd.to_numeric(di[\"event_newcase\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Group rows and compute aggregations\n",
    "per_staff = (di.groupby(\"staff_id\", as_index=False)\n",
    "# Apply aggregation(s) to grouped data\n",
    "               .agg(y_total=(\"event_newcase\",\"sum\"),\n",
    "\n",
    "                    days=(\"date\",\"nunique\")))\n",
    "\n",
    "\n",
    "alpha0, beta0 = 1.0, 1.0\n",
    "\n",
    "per_staff[\"alpha_post\"] = alpha0 + per_staff[\"y_total\"]\n",
    "\n",
    "per_staff[\"beta_post\"]  = beta0 + per_staff[\"days\"]\n",
    "\n",
    "# Posterior summaries for daily rate theta_i\n",
    "\n",
    "per_staff[\"rate_mean\"]   = per_staff[\"alpha_post\"] / per_staff[\"beta_post\"]\n",
    "\n",
    "per_staff[\"rate_median\"] = gamma_dist.ppf(0.5, a=per_staff[\"alpha_post\"], scale=1.0/per_staff[\"beta_post\"])\n",
    "\n",
    "per_staff[\"rate_p05\"]    = gamma_dist.ppf(0.05, a=per_staff[\"alpha_post\"], scale=1.0/per_staff[\"beta_post\"])\n",
    "\n",
    "per_staff[\"rate_p95\"]    = gamma_dist.ppf(0.95, a=per_staff[\"alpha_post\"], scale=1.0/per_staff[\"beta_post\"])\n",
    "\n",
    "# Expected pickups in next horizons\n",
    "\n",
    "per_staff[\"exp_7d_mean\"]  = per_staff[\"rate_mean\"] * 7.0\n",
    "\n",
    "per_staff[\"exp_28d_mean\"] = per_staff[\"rate_mean\"] * 28.0\n",
    "\n",
    "# Save a DataFrame to CSV\n",
    "per_staff.to_csv(OUT / \"investigator_pickup_posterior.csv\", index=False)\n",
    "\n",
    "# Print a message or value\n",
    "print(\"Done.\")\n",
    "# Print a message or value\n",
    "print(\"Saved:\", OUT / \"backlog_forecast_bayes.csv\")\n",
    "# Print a message or value\n",
    "print(\"Saved:\", OUT / \"investigator_pickup_posterior.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Code cell purpose: general processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Stage 3 â€” Bayesian Forecasting (Ready-to-Run)\n",
    "**Added:** 2025-10-27 10:45:02Z (UTC)\n",
    "\n",
    "This section fits a **hierarchical Bayesian model** (PyMC) to predict **daily investigated cases** for the next **90 days** for each **investigator, role, and team**.\n",
    "It is designed to work with the daily dataset built earlier in this notebook. If the dataset is not found in memory, you can point the loader to a CSV.\n",
    "\n",
    "**What you'll get:**\n",
    "- Posterior predictive draws and summary statistics per investigator Ã— team Ã— role Ã— day (next 90 days).\n",
    "- Aggregations to team-level, role-level, and org-level totals.\n",
    "- A quick plot of the org-level total forecast.\n",
    "\n",
    "> Tip: First run the data build section above so the in-memory DataFrame is available for immediate modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if needed (uncomment if missing).\n",
    "# %pip install pymc bambi arviz holidays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Configuration & Dataset Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Config ----\n",
    "INPUT_CSV = '/mnt/data/investigator_daily.csv'  # Set to a file if you prefer to load from disk\n",
    "COUNT_COL_CANDIDATES = ['cases_investigated','investigated','num_investigated','completed_cases','cases_completed']\n",
    "DATE_COL_CANDIDATES = ['date','activity_date','day']\n",
    "INVESTIGATOR_COL_KEYS = ['investigator','assignee','user']\n",
    "TEAM_COL_KEYS = ['team','squad']\n",
    "ROLE_COL_KEYS = ['role','grade']\n",
    "MAX_TRAIN_ROWS = None  # e.g., 250_000 to subsample for faster initial runs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_df_in_globals():\n",
    "    \"\"\"Heuristically find a pandas DataFrame with the expected columns in the current namespace.\"\"\"\n",
    "    candidates = []\n",
    "    for name, obj in globals().items():\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            cols_lower = [c.lower() for c in obj.columns]\n",
    "            has_date = any(c in cols_lower for c in DATE_COL_CANDIDATES)\n",
    "            has_count = any(c in cols_lower for c in COUNT_COL_CANDIDATES)\n",
    "            has_inv = any(any(k in c for k in INVESTIGATOR_COL_KEYS) for c in cols_lower)\n",
    "            has_team = any(any(k in c for k in TEAM_COL_KEYS) for c in cols_lower)\n",
    "            has_role = any(any(k in c for k in ROLE_COL_KEYS) for c in cols_lower)\n",
    "            if has_date and has_count and has_inv and has_team and has_role:\n",
    "                candidates.append((name, obj))\n",
    "    return candidates[0][1] if candidates else None\n",
    "\n",
    "def _standardise_columns(df):\n",
    "    df = df.copy()\n",
    "    lower_map = {c: c.lower() for c in df.columns}\n",
    "    df.rename(columns=lower_map, inplace=True)\n",
    "    # Date column\n",
    "    date_col = next((c for c in DATE_COL_CANDIDATES if c in df.columns), None)\n",
    "    assert date_col is not None, 'No date column found.'\n",
    "    df['date'] = pd.to_datetime(df[date_col]).dt.tz_localize(None)\n",
    "    # Count column\n",
    "    count_col = next((c for c in COUNT_COL_CANDIDATES if c in df.columns), None)\n",
    "    assert count_col is not None, 'No count column found.'\n",
    "    df['y'] = pd.to_numeric(df[count_col], errors='coerce').fillna(0).astype(int)\n",
    "    # Investigator/Team/Role\n",
    "    def _first_col_containing(keys):\n",
    "        for c in df.columns:\n",
    "            for k in keys:\n",
    "                if k in c:\n",
    "                    return c\n",
    "        return None\n",
    "    inv_col = _first_col_containing(INVESTIGATOR_COL_KEYS)\n",
    "    team_col = _first_col_containing(TEAM_COL_KEYS)\n",
    "    role_col = _first_col_containing(ROLE_COL_KEYS)\n",
    "    assert inv_col and team_col and role_col, 'Missing investigator/team/role columns.'\n",
    "    df['investigator'] = df[inv_col].astype(str)\n",
    "    df['team'] = df[team_col].astype(str)\n",
    "    df['role'] = df[role_col].astype(str)\n",
    "    # Keep only needed columns\n",
    "    keep = ['date','investigator','team','role','y']\n",
    "    df = df[keep].sort_values('date')\n",
    "    # Ensure non-negative counts\n",
    "    df['y'] = df['y'].clip(lower=0)\n",
    "    return df\n",
    "\n",
    "# Try in-memory first\n",
    "df0 = _find_df_in_globals()\n",
    "if df0 is None:\n",
    "    p = Path(INPUT_CSV)\n",
    "    if p.exists():\n",
    "        df0 = pd.read_csv(p)\n",
    "        print(f'Loaded dataset from {p}')\n",
    "    else:\n",
    "        raise FileNotFoundError('No suitable DataFrame found in memory and INPUT_CSV does not exist. Update INPUT_CSV or run the build cells above.')\n",
    "else:\n",
    "    print('Using dataset found in memory.')\n",
    "\n",
    "df = _standardise_columns(df0)\n",
    "if MAX_TRAIN_ROWS is not None and len(df) > MAX_TRAIN_ROWS:\n",
    "    df = df.sample(MAX_TRAIN_ROWS, random_state=42).sort_values('date')\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print('Training rows:', len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Feature Engineering (Calendar & Encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    import holidays\n",
    "    _has_holidays = True\n",
    "except Exception:\n",
    "    _has_holidays = False\n",
    "\n",
    "# Day-of-week and holiday flag (England & Wales if available)\n",
    "df = df.copy()\n",
    "df['dow'] = df['date'].dt.dayofweek.astype(int)\n",
    "if _has_holidays:\n",
    "    years = range(df['date'].dt.year.min(), df['date'].dt.year.max() + 3)\n",
    "    uk = holidays.country_holidays('GB', subdiv='ENG', years=years)\n",
    "    df['is_holiday'] = df['date'].dt.date.astype('datetime64')\n",
    "    df['is_holiday'] = df['is_holiday'].apply(lambda d: 1 if d in uk else 0).astype(int)\n",
    "else:\n",
    "    df['is_holiday'] = 0\n",
    "\n",
    "# Encode categories to integer indices\n",
    "inv_codes, inv_idx = pd.factorize(df['investigator'], sort=True)\n",
    "team_codes, team_idx = pd.factorize(df['team'], sort=True)\n",
    "role_codes, role_idx = pd.factorize(df['role'], sort=True)\n",
    "\n",
    "df['inv_idx'] = inv_idx.astype(int)\n",
    "df['team_idx'] = team_idx.astype(int)\n",
    "df['role_idx'] = role_idx.astype(int)\n",
    "\n",
    "n_inv = len(inv_codes)\n",
    "n_team = len(team_codes)\n",
    "n_role = len(role_codes)\n",
    "print({'n_inv': n_inv, 'n_team': n_team, 'n_role': n_role})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Fit Hierarchical Negative Binomial (PyMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Note â€” Why Bayesian here (PyMC)\n",
    "\n",
    "**For data scientists (math/stats):**\n",
    "- Likelihood: $y_i \\sim \\text{NegBin}(\\mu_i, \\alpha)$ with log link $\\log \\mu_i = X_i\\beta + b_{\\text{inv}[i]} + b_{\\text{team}[i]} + b_{\\text{role}[i]} + \\dots$.\n",
    "- We infer the full posterior $p(\\beta, b, \\alpha \\mid y) \\propto \\prod_i p(y_i \\mid \\mu_i, \\alpha)\\, p(\\beta) p(b) p(\\alpha)$ using NUTS (HMC).\n",
    "- Random intercepts yield **partial pooling**, stabilising estimates for sparse investigators/teams and reducing overfitting.\n",
    "- Priors act as **regularisation**; posterior predictive checks (PPC) assess calibration and overdispersion.\n",
    "\n",
    "**For non-experts (plain English):**\n",
    "- Shares information across people/teams so small groups don't swing wildly.\n",
    "- Gives **ranges** (credible intervals) rather than a single numberâ€”better for planning under uncertainty.\n",
    "- Learns patterns like weekdays and holidays and updates as new data arrives.\n",
    "\n",
    "See the README section **â€œWhy Poissonâ€“Gamma (Negative Binomial) for daily case counts?â€** in `README_Investigations_Backlog_Documentation.md` for the reasoning behind the count likelihood and overdispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import aesara.tensor as at\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "DRAWS = 1000   # Increase for production\n",
    "TUNE = 1000\n",
    "TARGET_ACCEPT = 0.9\n",
    "\n",
    "# Build model with MutableData so we can switch to future covariates later\n",
    "with pm.Model() as model:\n",
    "    inv_idx_data = pm.MutableData('inv_idx', df['inv_idx'].values)\n",
    "    team_idx_data = pm.MutableData('team_idx', df['team_idx'].values)\n",
    "    role_idx_data = pm.MutableData('role_idx', df['role_idx'].values)\n",
    "    dow_data = pm.MutableData('dow', df['dow'].values)\n",
    "    hol_data = pm.MutableData('hol', df['is_holiday'].values)\n",
    "    y_obs = df['y'].values\n",
    "\n",
    "    # Hyperpriors for random intercepts\n",
    "    sigma_inv = pm.HalfNormal('sigma_inv', 0.5)\n",
    "    sigma_team = pm.HalfNormal('sigma_team', 0.5)\n",
    "    sigma_role = pm.HalfNormal('sigma_role', 0.5)\n",
    "\n",
    "    z_inv = pm.Normal('z_inv', 0, 1, shape=n_inv)\n",
    "    z_team = pm.Normal('z_team', 0, 1, shape=n_team)\n",
    "    z_role = pm.Normal('z_role', 0, 1, shape=n_role)\n",
    "\n",
    "    inv_eff = pm.Deterministic('inv_eff', z_inv * sigma_inv)\n",
    "    team_eff = pm.Deterministic('team_eff', z_team * sigma_team)\n",
    "    role_eff = pm.Deterministic('role_eff', z_role * sigma_role)\n",
    "\n",
    "    # Fixed effects\n",
    "    beta_intercept = pm.Normal('beta_intercept', 0, 2)\n",
    "    beta_dow = pm.Normal('beta_dow', 0, 0.5, shape=7)\n",
    "    beta_hol = pm.Normal('beta_hol', 0, 0.5)\n",
    "\n",
    "    # Linear predictor\n",
    "    mu_lin = (\n",
    "        beta_intercept\n",
    "        + inv_eff[inv_idx_data]\n",
    "        + team_eff[team_idx_data]\n",
    "        + role_eff[role_idx_data]\n",
    "        + beta_dow[dow_data]\n",
    "        + beta_hol * hol_data\n",
    "    )\n",
    "    lam = pm.Deterministic('lam', at.exp(mu_lin))\n",
    "\n",
    "    # Overdispersion for Negative Binomial\n",
    "    alpha = pm.HalfNormal('alpha', 1.0)\n",
    "\n",
    "    y = pm.NegativeBinomial('y', mu=lam, alpha=alpha, observed=y_obs)\n",
    "\n",
    "    trace = pm.sample(DRAWS, tune=TUNE, target_accept=TARGET_ACCEPT, chains=4, random_seed=RANDOM_SEED)\n",
    "\n",
    "    # In-sample posterior predictive checks\n",
    "    ppc_insample = pm.sample_posterior_predictive(trace, var_names=['y'])\n",
    "print('Model fit complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Forecast Next 90 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path('/mnt/data/forecasts')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build future calendar (next 90 days)\n",
    "last_day = df['date'].max()\n",
    "future_dates = pd.date_range(last_day + pd.Timedelta(days=1), periods=90, freq='D')\n",
    "\n",
    "# Use all observed investigator/team/role combinations\n",
    "units = df[['investigator','team','role','inv_idx','team_idx','role_idx']].drop_duplicates()\n",
    "future = units.assign(key=1).merge(pd.DataFrame({'date': future_dates, 'key':1}), on='key').drop('key', axis=1)\n",
    "\n",
    "# Add features to future\n",
    "future['dow'] = future['date'].dt.dayofweek.astype(int)\n",
    "if 'is_holiday' in df.columns and df['is_holiday'].max() in [0,1]:\n",
    "    # Recompute holidays for the new date range if possible\n",
    "    try:\n",
    "        import holidays\n",
    "        years = range(future['date'].dt.year.min(), future['date'].dt.year.max() + 1)\n",
    "        uk = holidays.country_holidays('GB', subdiv='ENG', years=years)\n",
    "        future['is_holiday'] = future['date'].dt.date.astype('datetime64')\n",
    "        future['is_holiday'] = future['is_holiday'].apply(lambda d: 1 if d in uk else 0).astype(int)\n",
    "    except Exception:\n",
    "        future['is_holiday'] = 0\n",
    "else:\n",
    "    future['is_holiday'] = 0\n",
    "\n",
    "# Switch the model's data to the future design\n",
    "with model:\n",
    "    pm.set_data({\n",
    "        'inv_idx': future['inv_idx'].values,\n",
    "        'team_idx': future['team_idx'].values,\n",
    "        'role_idx': future['role_idx'].values,\n",
    "        'dow': future['dow'].values,\n",
    "        'hol': future['is_holiday'].values,\n",
    "    })\n",
    "    ppc_future = pm.sample_posterior_predictive(trace, var_names=['y'])\n",
    "\n",
    "# Summarise posterior predictive for each row\n",
    "draws = ppc_future['y']  # shape: (draws*chains, N)\n",
    "if draws.ndim == 3:\n",
    "    # Newer PyMC returns (chains, draws, N)\n",
    "    draws = draws.reshape((-1, draws.shape[-1]))\n",
    "means = draws.mean(axis=0)\n",
    "medians = np.median(draws, axis=0)\n",
    "low90 = np.quantile(draws, 0.05, axis=0)\n",
    "high90 = np.quantile(draws, 0.95, axis=0)\n",
    "low50 = np.quantile(draws, 0.25, axis=0)\n",
    "high50 = np.quantile(draws, 0.75, axis=0)\n",
    "\n",
    "future_out = future.copy()\n",
    "future_out['pred_mean'] = means\n",
    "future_out['pred_median'] = medians\n",
    "future_out['pred_p05'] = low90\n",
    "future_out['pred_p95'] = high90\n",
    "future_out['pred_p25'] = low50\n",
    "future_out['pred_p75'] = high50\n",
    "\n",
    "# Save investigator-level forecasts\n",
    "inv_path = OUT_DIR / 'investigator_daily_forecast_90d.csv'\n",
    "future_out.to_csv(inv_path, index=False)\n",
    "print(f'Saved investigator-level forecasts to: {inv_path}')\n",
    "\n",
    "# Aggregations: team-level, role-level, and org-level\n",
    "team_out = (future_out\n",
    "            .groupby(['date','team'], as_index=False)\n",
    "            [['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']]\n",
    "            .sum())\n",
    "role_out = (future_out\n",
    "            .groupby(['date','role'], as_index=False)\n",
    "            [['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']]\n",
    "            .sum())\n",
    "org_out = (future_out\n",
    "           .groupby(['date'], as_index=False)\n",
    "           [['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']]\n",
    "           .sum())\n",
    "\n",
    "team_path = OUT_DIR / 'team_daily_forecast_90d.csv'\n",
    "role_path = OUT_DIR / 'role_daily_forecast_90d.csv'\n",
    "org_path = OUT_DIR / 'org_daily_forecast_90d.csv'\n",
    "team_out.to_csv(team_path, index=False)\n",
    "role_out.to_csv(role_path, index=False)\n",
    "org_out.to_csv(org_path, index=False)\n",
    "print(f'Saved team-level forecasts to: {team_path}')\n",
    "print(f'Saved role-level forecasts to: {role_path}')\n",
    "print(f'Saved org-level forecasts to: {org_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Quick Visual: Organisation-wide Forecast (Totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "org_path = Path('/mnt/data/forecasts/org_daily_forecast_90d.csv')\n",
    "org = pd.read_csv(org_path, parse_dates=['date'])\n",
    "plt.figure()\n",
    "plt.plot(org['date'], org['pred_mean'], label='Forecast mean (next 90 days)')\n",
    "plt.title('Organisation-wide investigated cases: 90-day forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cases')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Plot displayed. Image not saved by default to keep the notebook tidy.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Notes & Tips\n",
    "- To speed up first runs, lower `DRAWS`/`TUNE` or set `MAX_TRAIN_ROWS` to a smaller number.\n",
    "- For richer structure, extend the model with time trends, seasonal splines, or random slopes.\n",
    "- If you prefer formulas, you can port this to **Bambi** with `y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)` and `family='negativebinomial'`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Alternative: Bambi (Formula Interface)\n",
    "**Added:** 2025-10-27 10:48:20Z (UTC)\n",
    "\n",
    "This section mirrors the PyMC approach using **Bambi**, a high-level formula interface\n",
    "built on top of PyMC. It fits a **Negative Binomial** model with random intercepts for\n",
    "**investigator**, **team**, and **role**, plus **day-of-week** and **holiday** effects.\n",
    "\n",
    "Formula used:\n",
    "\n",
    "```\n",
    "y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "- 90-day daily forecasts per investigator, team, and role.\n",
    "- Aggregations to team/role/org.\n",
    "- If supported by your Bambi version, posterior predictive intervals; otherwise, mean forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install if missing (uncomment if needed)\n",
    "# %pip install bambi arviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Bambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Expect df with columns: date, y, dow, is_holiday, investigator, team, role\n",
    "# If df not present, try to load from the same CSV used above (or adjust INPUT_CSV)\n",
    "if 'df' not in globals():\n",
    "    try:\n",
    "        INPUT_CSV\n",
    "    except NameError:\n",
    "        INPUT_CSV = '/mnt/data/investigator_daily.csv'\n",
    "    p = Path(INPUT_CSV)\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        print(f'Loaded dataset from {p}')\n",
    "    else:\n",
    "        raise FileNotFoundError('Expected DataFrame `df` not found in memory and INPUT_CSV does not exist. Please run the build cells above or update INPUT_CSV.')\n",
    "\n",
    "# Standardise expected columns (if needed)\n",
    "df = df.copy()\n",
    "lc = {c: c.lower() for c in df.columns}\n",
    "df.rename(columns=lc, inplace=True)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "if 'dow' not in df.columns:\n",
    "    df['dow'] = df['date'].dt.dayofweek.astype(int)\n",
    "if 'is_holiday' not in df.columns:\n",
    "    df['is_holiday'] = 0\n",
    "if 'y' not in df.columns:\n",
    "    # Heuristic to find a count column\n",
    "    for c in ['cases_investigated','investigated','num_investigated','completed_cases','cases_completed']:\n",
    "        if c in df.columns:\n",
    "            df['y'] = pd.to_numeric(df[c], errors='coerce').fillna(0).astype(int)\n",
    "            break\n",
    "    assert 'y' in df.columns, 'No count column detected; please add column y.'\n",
    "# Ensure categorical variables are categorical (Bambi handles strings too, but categories are explicit)\n",
    "for col in ['investigator','team','role']:\n",
    "    df[col] = df[col].astype('category')\n",
    "print(df[['date','investigator','team','role','y','dow','is_holiday']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Fit Bambi Negative Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Note â€” Why Bayesian here (Bambi)\n",
    "\n",
    "Bambi uses the same Bayesian engine (PyMC) with a formula interface.\n",
    "\n",
    "**For data scientists (math/stats):**\n",
    "- Model: `y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)` with `family='negativebinomial'`.\n",
    "- Hierarchical random intercepts implement partial pooling; priors regularise parameters; NUTS samples the joint posterior.\n",
    "- Use PPC and coverage of credible intervals to validate fit.\n",
    "\n",
    "**For non-experts (plain English):**\n",
    "- Same benefits as the PyMC block, but simpler syntaxâ€”handy for quick iteration and explainability.\n",
    "- Produces forecast **ranges** you can plan around.\n",
    "\n",
    "For the choice of the Negative Binomial (Poissonâ€“Gamma) count model, see `README_Investigations_Backlog_Documentation.md` â†’ *Why Poissonâ€“Gamma (Negative Binomial) for daily case counts?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bambi as bmb\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "DRAWS = 1000   # Increase for production\n",
    "TUNE = 1000\n",
    "TARGET_ACCEPT = 0.9\n",
    "\n",
    "formula = 'y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)'\n",
    "model_bmb = bmb.Model(formula, df, family='negativebinomial')\n",
    "idata_bmb = model_bmb.fit(draws=DRAWS, tune=TUNE, target_accept=TARGET_ACCEPT, chains=4, random_seed=RANDOM_SEED)\n",
    "print('Bambi model fit complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Forecast Next 90 Days with Bambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path('/mnt/data/forecasts')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build future calendar (next 90 days)\n",
    "last_day = df['date'].max()\n",
    "future_dates = pd.date_range(last_day + pd.Timedelta(days=1), periods=90, freq='D')\n",
    "\n",
    "# All observed unit combos\n",
    "units = df[['investigator','team','role']].drop_duplicates()\n",
    "future_bmb = units.assign(key=1).merge(pd.DataFrame({'date': future_dates, 'key':1}), on='key').drop('key', axis=1)\n",
    "future_bmb['dow'] = future_bmb['date'].dt.dayofweek.astype(int)\n",
    "if 'is_holiday' in df.columns and df['is_holiday'].max() in [0,1]:\n",
    "    try:\n",
    "        import holidays\n",
    "        years = range(future_bmb['date'].dt.year.min(), future_bmb['date'].dt.year.max() + 1)\n",
    "        uk = holidays.country_holidays('GB', subdiv='ENG', years=years)\n",
    "        future_bmb['is_holiday'] = future_bmb['date'].dt.date.astype('datetime64')\n",
    "        future_bmb['is_holiday'] = future_bmb['is_holiday'].apply(lambda d: 1 if d in uk else 0).astype(int)\n",
    "    except Exception:\n",
    "        future_bmb['is_holiday'] = 0\n",
    "else:\n",
    "    future_bmb['is_holiday'] = 0\n",
    "\n",
    "# Ensure categorical types align with training\n",
    "for col in ['investigator','team','role']:\n",
    "    future_bmb[col] = future_bmb[col].astype('category')\n",
    "    # align categories with training df\n",
    "    future_bmb[col] = future_bmb[col].cat.set_categories(df[col].cat.categories)\n",
    "\n",
    "def _summarise_pps(draws_array):\n",
    "    # draws_array expected shape: (samples, N)\n",
    "    means = draws_array.mean(axis=0)\n",
    "    medians = np.median(draws_array, axis=0)\n",
    "    p05 = np.quantile(draws_array, 0.05, axis=0)\n",
    "    p95 = np.quantile(draws_array, 0.95, axis=0)\n",
    "    p25 = np.quantile(draws_array, 0.25, axis=0)\n",
    "    p75 = np.quantile(draws_array, 0.75, axis=0)\n",
    "    return means, medians, p05, p95, p25, p75\n",
    "\n",
    "pred_cols = ['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']\n",
    "future_out_bmb = future_bmb.copy()\n",
    "\n",
    "try:\n",
    "    # Preferred: posterior predictive samples\n",
    "    pps = model_bmb.predict(idata_bmb, data=future_bmb, kind='pps')\n",
    "    # Try to convert to a (samples, N) array robustly\n",
    "    import numpy as np\n",
    "    arr = None\n",
    "    # Newer Bambi returns xarray DataArray\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        if isinstance(pps, xr.DataArray):\n",
    "            if set(['chain','draw']).issubset(set(pps.dims)):\n",
    "                arr = pps.stack(sample=('chain','draw')).transpose('sample','obs').values\n",
    "            else:\n",
    "                arr = pps.values\n",
    "    except Exception:\n",
    "        pass\n",
    "    if arr is None:\n",
    "        arr = np.asarray(pps)\n",
    "        if arr.ndim == 3:  # chains, draws, N\n",
    "            arr = arr.reshape((-1, arr.shape[-1]))\n",
    "    m, md, p05, p95, p25, p75 = _summarise_pps(arr)\n",
    "    future_out_bmb['pred_mean'] = m\n",
    "    future_out_bmb['pred_median'] = md\n",
    "    future_out_bmb['pred_p05'] = p05\n",
    "    future_out_bmb['pred_p95'] = p95\n",
    "    future_out_bmb['pred_p25'] = p25\n",
    "    future_out_bmb['pred_p75'] = p75\n",
    "    print('Used posterior predictive samples from Bambi for intervals.')\n",
    "except Exception as e:\n",
    "    print('Falling back to mean predictions only (intervals unavailable):', e)\n",
    "    mu = model_bmb.predict(idata_bmb, data=future_bmb, kind='mean')\n",
    "    future_out_bmb['pred_mean'] = pd.Series(mu).values\n",
    "    # Leave interval columns as NaN to signal they were not computed\n",
    "    for c in pred_cols[1:]:\n",
    "        future_out_bmb[c] = pd.NA\n",
    "\n",
    "# Save investigator-level forecasts (Bambi)\n",
    "inv_path = OUT_DIR / 'investigator_daily_forecast_90d_bambi.csv'\n",
    "future_out_bmb.to_csv(inv_path, index=False)\n",
    "print(f'Saved investigator-level forecasts (Bambi) to: {inv_path}')\n",
    "\n",
    "# Aggregations\n",
    "cols = ['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']\n",
    "team_out_bmb = (future_out_bmb.groupby(['date','team'], as_index=False)[cols].sum(min_count=1))\n",
    "role_out_bmb = (future_out_bmb.groupby(['date','role'], as_index=False)[cols].sum(min_count=1))\n",
    "org_out_bmb = (future_out_bmb.groupby(['date'], as_index=False)[cols].sum(min_count=1))\n",
    "\n",
    "team_path = OUT_DIR / 'team_daily_forecast_90d_bambi.csv'\n",
    "role_path = OUT_DIR / 'role_daily_forecast_90d_bambi.csv'\n",
    "org_path = OUT_DIR / 'org_daily_forecast_90d_bambi.csv'\n",
    "team_out_bmb.to_csv(team_path, index=False)\n",
    "role_out_bmb.to_csv(role_path, index=False)\n",
    "org_out_bmb.to_csv(org_path, index=False)\n",
    "print(f'Saved team-level forecasts (Bambi) to: {team_path}')\n",
    "print(f'Saved role-level forecasts (Bambi) to: {role_path}')\n",
    "print(f'Saved org-level forecasts (Bambi) to: {org_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Quick Visual: Organisation-wide Forecast (Bambi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "org_path = Path('/mnt/data/forecasts/org_daily_forecast_90d_bambi.csv')\n",
    "org = pd.read_csv(org_path, parse_dates=['date'])\n",
    "plt.figure()\n",
    "plt.plot(org['date'], org['pred_mean'], label='Bambi forecast mean (next 90 days)')\n",
    "plt.title('Organisation-wide investigated cases: 90-day forecast (Bambi)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cases')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Plot displayed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
