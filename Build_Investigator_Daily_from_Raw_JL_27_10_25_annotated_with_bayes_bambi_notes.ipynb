{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Investigations Backlog â€“ Annotated Build Notebook\n",
    "**Date:** 2025-10-27\n",
    "\n",
    "This version of the notebook is automatically annotated with:\n",
    "- Line-by-line comments in code cells to explain what each statement is doing.\n",
    "- Brief summaries before each code cell describing the main purpose.\n",
    "- Pointers to comprehensive documentation: see `README_Investigations_Backlog_Documentation.md` in the same folder for the full end-to-end description (data engineering, predictive modelling, and Bayesian analysis approach).\n",
    "\n",
    "> Original source notebook: `Build_Investigator_Daily_from_Raw_JL.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "> **Original note:**\n",
    ">\n",
    "> # Build Investigator Daily Panel (from OPG raw extract)\n",
    "> This notebook mirrors the script flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### general processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!python -m venv .venv && . .venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Jake note regarding linking investigation data to LPA and staff data\n",
    "Iâ€™ve re-added the investigator names as requested. Iâ€™ve also added the LPA/Deputyship ID too. (so just the donors name and DOB is removed). The password remains as â€œbacklogâ€. \n",
    "\n",
    "On the analytical platform, the LPA number is stored as â€˜UIDâ€™ in the cases table in the opg_sirius_prod database. The investigations database has hyphens for these idâ€™s, but if you remove the hyphens you can then join the database with the data on the AP. Effectively the donor names can be re-accessed there, and other key variables such as the LPA registration dates can be retrieved (as these are not stored on the database but these are significant for inbounds).\n",
    "\n",
    "I have also added the FTE of the EO/AO investigators to this sheet, what youâ€™ll notice is that there are some members of staff who were previously EOâ€™s (and are now HEOâ€™s) so they are not on the staff list. The staff list is in a constant state of flux with the incoming cohorts/natural attrition, so Iâ€™d heavily recommend if any projections relating to resource levels are made I send a definitive list on a specific date so thereâ€™s a clear point of reference. In the temporary backlog model, I am manually reviewing the list each month with placeholders for the incoming cohorts, but for the more sophisticated model you may come up with a better solution. Itâ€™s something to discuss in next weekâ€™s meeting im sure.\n",
    "\n",
    "## Peter interpretation of cases left the allocation\n",
    "One question that I do have is, whilst maintaining anonymity can the records of closed cases be linked to individual investigators ? As you know the key problem that we are trying to investigate is how will changes in staff volumes impact OPGâ€™s ability to reduce the backlog, so we really need to understand the variation in workloads assigned to individuals.\n",
    "\n",
    "the cases closed or sent to court for legal review from the anlaytical point of view can be the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### imports and environment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries/modules for use below\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# Configure paths\n",
    "# Path to the raw investigation data\n",
    "RAW_PATH = Path('data/raw/raw.csv')\n",
    "# Path to the output/processed investigation data\n",
    "OUT_DIR = Path('data/out'); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Print if the path exists\n",
    "print(RAW_PATH.exists(), OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Function used in this notebook:\n",
    "normalise_col, parse_date_series, hash_id, month_to_season, is_term_month, load_raw, col, engineer, date_horizon, build_event_log, build_wip_series, build_backlog_series, build_daily_panel, and summarise_daily_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### imports and environment setup, date parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ðŸ§¹ DATA PRE-PROCESSING SECTION\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Define a set of string patterns that represent missing or null values.\n",
    "# These strings will be treated as equivalent to NaN during cleaning.\n",
    "NULL_STRINGS = {\n",
    "    '', 'na', 'n/a', 'none', 'null', '-', '--', 'unknown',\n",
    "    'not completed', 'not complete', 'tbc', 'n\\\\a'\n",
    "}\n",
    "\n",
    "\n",
    "def normalise_col(c: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a column name for consistency.\n",
    "\n",
    "    This function cleans up and standardizes column names by:\n",
    "    - Converting to lowercase\n",
    "    - Removing leading/trailing whitespace\n",
    "    - Replacing multiple spaces with a single space\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c : str\n",
    "        The original column name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A cleaned and standardized version of the column name.\n",
    "    \"\"\"\n",
    "    # Convert to string, remove extra spaces, and make lowercase.\n",
    "    return re.sub(r'\\s+', ' ', str(c).strip().lower())\n",
    "\n",
    "\n",
    "def parse_date_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse and clean a pandas Series of date strings.\n",
    "\n",
    "    This function:\n",
    "    - Handles various date formats\n",
    "    - Converts known null strings to NaT\n",
    "    - Removes ordinal suffixes (e.g., '1st', '2nd', '3rd')\n",
    "    - Fixes known typos\n",
    "    - Uses robust pandas date parsing with fallback strategies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : pd.Series\n",
    "        A pandas Series containing raw date values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series of datetime64[ns] values with cleaned and parsed dates.\n",
    "    \"\"\"\n",
    "\n",
    "    def _p(x):\n",
    "        \"\"\"Internal helper to parse a single date entry.\"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        # Return NaT if missing\n",
    "        if pd.isna(x):\n",
    "            return pd.NaT\n",
    "\n",
    "        # Convert to lowercase string\n",
    "        xs = str(x).strip().lower()\n",
    "\n",
    "        # Return NaT if in known null string set\n",
    "        if xs in NULL_STRINGS:\n",
    "            return pd.NaT\n",
    "\n",
    "        # Clean up common errors and ordinal suffixes\n",
    "        xs = re.sub(r'(\\d{1,2})(st|nd|rd|th)', r'\\1', xs).replace('legel', 'legal')\n",
    "\n",
    "        # Try strict parsing, then flexible fallback\n",
    "        try:\n",
    "            return pd.to_datetime(xs, dayfirst=True, errors='raise')\n",
    "        except Exception:\n",
    "            return pd.to_datetime(xs, infer_datetime_format=True, dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Apply the parser to each element of the Series\n",
    "    return s.apply(_p)\n",
    "\n",
    "\n",
    "def hash_id(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a short, anonymized hash-based identifier.\n",
    "\n",
    "    Creates a pseudonymized ID for text entries using SHA1 hashing.\n",
    "    Empty or missing values return an empty string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : str\n",
    "        The input text value (e.g., name, case number).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        An anonymized hash string prefixed with 'S', e.g., 'S1a2b3c4d'.\n",
    "    \"\"\"\n",
    "    # Return empty string for null or blank input\n",
    "    if pd.isna(t) or str(t).strip() == '':\n",
    "        return ''\n",
    "\n",
    "    # Create SHA1 hash and take first 8 characters for compact ID\n",
    "    return 'S' + hashlib.sha1(str(t).encode('utf-8')).hexdigest()[:8]\n",
    "\n",
    "\n",
    "def month_to_season(m: int) -> str:\n",
    "    \"\"\"\n",
    "    Convert a numeric month into a season name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Month number (1â€“12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The season corresponding to the month ('winter', 'spring', 'summer', or 'autumn').\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> month_to_season(4)\n",
    "    'spring'\n",
    "    >>> month_to_season(10)\n",
    "    'autumn'\n",
    "    \"\"\"\n",
    "    # Map month numbers to their respective seasons\n",
    "    return {\n",
    "        12: 'winter', 1: 'winter', 2: 'winter',\n",
    "        3: 'spring', 4: 'spring', 5: 'spring',\n",
    "        6: 'summer', 7: 'summer', 8: 'summer',\n",
    "        9: 'autumn', 10: 'autumn', 11: 'autumn'\n",
    "    }[int(m)]\n",
    "\n",
    "\n",
    "def is_term_month(m: int) -> int:\n",
    "    \"\"\"\n",
    "    Identify whether a month is a 'termination month'.\n",
    "\n",
    "    In the current logic, August (month 8) is excluded and returns 0.\n",
    "    All other months return 1, representing active/valid months.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Month number (1â€“12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if the month is August, else 1.\n",
    "    \"\"\"\n",
    "    # Return binary flag based on month value\n",
    "    return 0 if int(m) == 8 else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### imports and environment setup, data loading, joining/merging datasets, aggregation/grouping, pivot/reshape, data cleaning, sorting, feature engineering, exporting outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# ðŸ§© DATA LOADING AND FEATURE ENGINEERING\n",
    "# -------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: load_raw()\n",
    "# -------------------------------------------------------------\n",
    "def load_raw(p: Path, force_encoding: str | None = None):\n",
    "    \"\"\"\n",
    "    Load a CSV or Excel file into a pandas DataFrame with robust encoding handling.\n",
    "\n",
    "    This function attempts to open and read raw data files safely, even when\n",
    "    character encodings vary or are unknown. It tries multiple encodings in order\n",
    "    until one succeeds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Path\n",
    "        Path to the input file.\n",
    "    force_encoding : str, optional\n",
    "        If provided, forces the use of a specific encoding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (df, colmap)\n",
    "        df : pd.DataFrame\n",
    "            Cleaned dataframe containing the raw data.\n",
    "        colmap : dict\n",
    "            Mapping of normalized column names (lowercased, trimmed) to original column headers.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the file path does not exist.\n",
    "    RuntimeError\n",
    "        If all encoding attempts fail.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check file existence\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "    # Excel files typically do not have encoding issues\n",
    "    if p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(p, dtype=str)\n",
    "    else:\n",
    "        tried = []\n",
    "        # Build list of encodings to try\n",
    "        encodings_to_try = (\n",
    "            [force_encoding] if force_encoding else\n",
    "            [\"utf-8-sig\", \"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-16\", \"utf-16le\", \"utf-16be\"]\n",
    "        )\n",
    "\n",
    "        df = None\n",
    "        last_err = None\n",
    "\n",
    "        # Try to read using multiple encodings\n",
    "        for enc in encodings_to_try:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p, dtype=str, sep=None, engine=\"python\",\n",
    "                    encoding=enc, encoding_errors=\"strict\"\n",
    "                )\n",
    "                break\n",
    "            except UnicodeDecodeError as e:\n",
    "                tried.append(enc)\n",
    "                last_err = e\n",
    "            except Exception as e:\n",
    "                # Continue trying other encodings\n",
    "                tried.append(enc)\n",
    "                last_err = e\n",
    "\n",
    "        # Fallback: attempt to decode with cp1252 and replace bad bytes\n",
    "        if df is None:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p, dtype=str, sep=None, engine=\"python\",\n",
    "                    encoding=\"cp1252\", encoding_errors=\"replace\"\n",
    "                )\n",
    "                print(f\"[load_raw] WARNING: used cp1252 with replacement after failed encodings: {tried}\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to read CSV. Tried encodings {tried}. Last error: {last_err}\"\n",
    "                ) from e\n",
    "\n",
    "    # Strip whitespace from all string values\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Create mapping of normalized column names â†’ original names\n",
    "    colmap = {re.sub(r\"\\s+\", \" \", str(c).strip().lower()): c for c in df.columns}\n",
    "\n",
    "    return df, colmap\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: col()\n",
    "# -------------------------------------------------------------\n",
    "def col(df: pd.DataFrame, colmap: dict, name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Retrieve a column from a DataFrame by fuzzy name matching.\n",
    "\n",
    "    This function normalises the requested column name and searches the column map\n",
    "    for an exact or partial match. Returns a Series of NaNs if not found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The source DataFrame.\n",
    "    colmap : dict\n",
    "        Mapping of normalised column names to original names.\n",
    "    name : str\n",
    "        Column name to look up.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The column data if found, otherwise a Series of NaN values.\n",
    "    \"\"\"\n",
    "    k = normalise_col(name)\n",
    "\n",
    "    # Exact match first\n",
    "    if k in colmap:\n",
    "        return df[colmap[k]]\n",
    "\n",
    "    # Partial match fallback\n",
    "    for kk, v in colmap.items():\n",
    "        if k in kk or kk in k:\n",
    "            return df[v]\n",
    "\n",
    "    # Default: return empty column of NaNs\n",
    "    return pd.Series([np.nan] * len(df))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: engineer()\n",
    "# -------------------------------------------------------------\n",
    "def engineer(df: pd.DataFrame, colmap: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer standardised and typed columns from raw investigation data.\n",
    "\n",
    "    This function extracts and converts the key variables such as case IDs, investigators,\n",
    "    FTEs, and multiple date columns from the raw file using reusable helper functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe from load_raw().\n",
    "    colmap : dict\n",
    "        Column name mapping from load_raw().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned and feature-engineered dataframe (reallocated-only) ready for modeling.\n",
    "        The returned dataframe includes ONLY records where 'Reallocated Case' is 'yes'\n",
    "        (case-insensitive; also accepts y/true/1).\n",
    "    \"\"\"\n",
    "    \n",
    "    out = pd.DataFrame({\n",
    "        'case_id': col(df, colmap, 'ID'),\n",
    "        'investigator': col(df, colmap, 'Investigator'),\n",
    "        'team': col(df, colmap, 'Team'),\n",
    "        'fte': pd.to_numeric(col(df, colmap, 'Investigator FTE'), errors='coerce'),\n",
    "        'reallocated_case': col(df, colmap, 'Reallocated Case'),\n",
    "        'weighting': pd.to_numeric(col(df, colmap, 'Weighting'), errors='coerce'),\n",
    "        'case_type': col(df, colmap, 'Case Type'),\n",
    "        'concern_type': col(df, colmap, 'Concern Type'),\n",
    "        'status': col(df, colmap, 'Status'),\n",
    "        'days_to_pg_signoff': pd.to_numeric(col(df, colmap, 'Days to PG sign off'), errors='coerce'),\n",
    "    })\n",
    "    \n",
    "    # Parse and standardise relevant date columns \n",
    "    out['dt_received_inv']  = parse_date_series(col(df, colmap, 'Date Received in Investigations'))\n",
    "    out['dt_alloc_invest']  = parse_date_series(col(df, colmap, 'Date allocated to current investigator'))\n",
    "    out['dt_alloc_team']    = parse_date_series(col(df, colmap, 'Date allocated to team'))\n",
    "    out['dt_pg_signoff']    = parse_date_series(col(df, colmap, 'PG Sign off date'))\n",
    "    out['dt_close']         = parse_date_series(col(df, colmap, 'Closure Date'))\n",
    "    out['dt_legal_req_1']   = parse_date_series(col(df, colmap, 'Date of Legal Review Request 1'))\n",
    "    out['dt_legal_rej_1']   = parse_date_series(col(df, colmap, 'Date Legal Rejects 1'))\n",
    "    out['dt_legal_req_2']   = parse_date_series(col(df, colmap, 'Date of Legal Review Request 2'))\n",
    "    out['dt_legal_rej_2']   = parse_date_series(col(df, colmap, 'Date Legal Rejects 2'))\n",
    "    out['dt_legal_req_3']   = parse_date_series(col(df, colmap, 'Date of Legel Review Request 3'))\n",
    "    out['dt_legal_approval']= parse_date_series(col(df, colmap, 'Legal Approval Date'))\n",
    "    out['dt_date_of_order'] = parse_date_series(col(df, colmap, 'Date Of Order'))\n",
    "    out['dt_flagged']       = parse_date_series(col(df, colmap, 'Flagged Date'))\n",
    "    out['dt_sent_to_ca']    = parse_date_series(col(df, colmap, 'Date Sent To CA'))\n",
    "\n",
    "    # Fill missing FTEs with 1.0, hash investigator names for anonymization, and add placeholders\n",
    "    # Defaults, anonymisation, and placeholders\n",
    "    out['fte'] = out['fte'].fillna(1.0)\n",
    "    out['staff_id'] = out['investigator'].apply(hash_id)\n",
    "    out['role'] = ''\n",
    "\n",
    "    # ----- Filter: keep only reallocated cases -----\n",
    "    reall = out['reallocated_case'].astype(str).str.strip().str.lower()\n",
    "    mask_realloc = reall.isin({'yes', 'y', 'true', '1'})\n",
    "    out = out.loc[mask_realloc].reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Function: date_horizon()\n",
    "# -------------------------------------------------------------\n",
    "def date_horizon(typed: pd.DataFrame, \n",
    "                 pad_days: int = 14,\n",
    "                fallback_to_all_dates: bool = True\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Primary rule:\n",
    "      - start := earliest non-null value in 'dt_received_inv'\n",
    "      - end   := latest non-null value in 'dt_pg_signoff'\n",
    "\n",
    "    Optional fallback:\n",
    "      If either start or end cannot be determined (column missing or all NaT)\n",
    "      *and* fallback_to_all_dates is True, compute:\n",
    "        - start := min across ALL columns starting with 'dt_'\n",
    "        - end   := max across ALL columns starting with 'dt_'\n",
    "\n",
    "    Finally, apply `pad_days` to the end date. If still missing after fallback,\n",
    "    default to a 30-day lookback for start and today for end (+ padding).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Feature-engineered dataset with standardized date columns.\n",
    "    pad_days : int, default=14\n",
    "        Number of days to extend the end horizon.\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        Whether to fall back to scanning all `dt_` columns when the primary\n",
    "        columns are unavailable or empty.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of pd.Timestamp\n",
    "        (start, end) normalised date range.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Falls back to recent 30 days if dt_received_inv or dt_pg_signoff\n",
    "    are missing or contain no valid dates.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from datetime import datetime\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05'), pd.NaT],\n",
    "    ...     'dt_alloc_invest': [pd.NaT, pd.Timestamp('2025-01-10')],\n",
    "    ...     'dt_alloc_team': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.NaT, pd.Timestamp('2025-02-01')],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> s, e = date_horizon(df, pad_days=7)\n",
    "    >>> isinstance(s, pd.Timestamp) and isinstance(e, pd.Timestamp)\n",
    "    True\n",
    "    >>> (e - s).days >= (pd.Timestamp('2025-02-01') - pd.Timestamp('2025-01-05')).days\n",
    "    True\n",
    "    \"\"\"\n",
    "    #start = pd.concat([typed['dt_received_inv'], typed['dt_alloc_invest'], typed['dt_alloc_team']]).min()\n",
    "    #end = pd.concat([typed['dt_close'], typed['dt_pg_signoff'], typed['dt_date_of_order']]).max()\n",
    "    \n",
    "    # --- Primary computation from specified columns ---\n",
    "    start = pd.NaT\n",
    "    end = pd.NaT\n",
    "\n",
    "    if 'dt_received_inv' in typed:\n",
    "        start = typed['dt_received_inv'].dropna().min()\n",
    "\n",
    "    if 'dt_pg_signoff' in typed:\n",
    "        end = typed['dt_pg_signoff'].dropna().max()\n",
    "\n",
    "    # --- Optional fallback over all dt_ columns ---\n",
    "    if (pd.isna(start) or pd.isna(end)) and fallback_to_all_dates:\n",
    "        dt_cols = [c for c in typed.columns if c.startswith('dt_')]\n",
    "        if dt_cols:\n",
    "            all_dates = pd.concat([typed[c] for c in dt_cols], ignore_index=True).dropna()\n",
    "            if pd.isna(start) and not all_dates.empty:\n",
    "                start = all_dates.min()\n",
    "            if pd.isna(end) and not all_dates.empty:\n",
    "                end = all_dates.max()\n",
    "\n",
    "    # --- Final graceful defaults if still missing ---\n",
    "    today = pd.Timestamp.today().normalize()\n",
    "    if pd.isna(start):\n",
    "        start = today - pd.Timedelta(days=30)\n",
    "    if pd.isna(end):\n",
    "        end = today\n",
    "\n",
    "    # --- Apply padding to end and normalize ---\n",
    "    end = (end + pd.Timedelta(days=pad_days)).normalize()\n",
    "    return start.normalize(), end\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_event_log()\n",
    "# -------------------------------------------------------------\n",
    "def build_event_log(typed: pd.DataFrame, \n",
    "                    pad_days: int = 14, \n",
    "                    fallback_to_all_dates: bool = True\n",
    "                   ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct a staff-day event log from feature-engineered investigation data.\n",
    "    \n",
    "    Each row represents a dated event for a specific case and staff member.\n",
    "    For example, â€œInvestigator S1 picked up case C1 on 2025-01-10.â€\n",
    "    \n",
    "    For each case, this function creates dated event records (e.g., new case pickup,\n",
    "    legal requests/approvals, court orders) at the staff-day level.\n",
    "    \n",
    "    Events emitted (if their date exists):\n",
    "      received         -> dt_received_inv\n",
    "      alloc_team       -> dt_alloc_team\n",
    "      newcase          -> dt_alloc_invest\n",
    "      sent_to_ca       -> dt_sent_to_ca\n",
    "      legal_request    -> dt_legal_req_1, dt_legal_req_2, dt_legal_req_3\n",
    "      legal_reject     -> dt_legal_rej_1, dt_legal_rej_2\n",
    "      legal_approval   -> dt_legal_approval\n",
    "      pg_signoff       -> dt_pg_signoff\n",
    "      court_order      -> dt_date_of_order\n",
    "      closed           -> dt_close\n",
    "      flagged          -> dt_flagged\n",
    "\n",
    "      The output is restricted to the date horizon determined by date_horizon()\n",
    "      using dt_received_inv for start and dt_pg_signoff for end (with padding).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Output of engineer(); typically already filtered to reallocated cases.\n",
    "        Expected columns include identifiers, staffing info, and the dt_* fields.\n",
    "    pad_days : int, default=14\n",
    "        Extra days added to end horizon via date_horizon().\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        If start/end cannot be derived from the primary columns, allow\n",
    "        date_horizon() to fallback across all dt_* columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['date','staff_id','team','fte','case_id','event','meta']\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Includes lightweight, structured meta (JSON) with weighting, case_type, concern_type, status, and days_to_pg_signoff.\n",
    "    to keep contextual attributes about that case alongside each event (for later analysis or auditing), such as:\n",
    "    Case weighting (e.g., 2.5 for complexity or workload)\n",
    "    Case type (Financial / Welfare / etc.)\n",
    "    Concern type (Neglect / Abuse / etc.)\n",
    "    Current status (Open / Closed / etc.)\n",
    "    Days to PG sign-off (performance metric)\n",
    "\n",
    "    Instead of duplicating these as separate columns for every event â€” which would make the event log wide, repetitive, \n",
    "    and harder to serialize â€” we store them compactly in a single column named meta.\n",
    "    Each meta cell is a JSON string encoding those extra attributes.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from datetime import datetime\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id': ['S1'],\n",
    "    ...     'team': ['A'],\n",
    "    ...     'fte': [1.0],\n",
    "    ...     'case_id': ['C1'],\n",
    "    ...     'weighting': [2.5],\n",
    "    ...     'case_type': ['Financial'],\n",
    "    ...     'concern_type': ['Neglect'],\n",
    "    ...     'status': ['Open'],\n",
    "    ...     'days_to_pg_signoff': [15],\n",
    "    ...     # Key timeline dates\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05')],\n",
    "    ...     'dt_alloc_team': [pd.Timestamp('2025-01-08')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-10')],\n",
    "    ...     'dt_sent_to_ca': [pd.Timestamp('2025-01-12')],\n",
    "    ...     'dt_legal_req_1': [pd.Timestamp('2025-01-14')],\n",
    "    ...     'dt_legal_req_2': [pd.NaT],\n",
    "    ...     'dt_legal_req_3': [pd.NaT],\n",
    "    ...     'dt_legal_rej_1': [pd.NaT],\n",
    "    ...     'dt_legal_rej_2': [pd.NaT],\n",
    "    ...     'dt_legal_approval': [pd.Timestamp('2025-01-20')],\n",
    "    ...     'dt_pg_signoff': [pd.Timestamp('2025-01-25')],\n",
    "    ...     'dt_date_of_order': [pd.NaT],\n",
    "    ...     'dt_close': [pd.Timestamp('2025-02-01')],\n",
    "    ...     'dt_flagged': [pd.NaT],\n",
    "    ... })\n",
    "    >>> ev = build_event_log(typed)\n",
    "    >>> sorted(ev['event'].unique().tolist())\n",
    "    ['alloc_team', 'closed', 'legal_approval', 'legal_request',\n",
    "     'newcase', 'pg_signoff', 'received', 'sent_to_ca']\n",
    "    >>> set(ev.columns) >= {'date','staff_id','team','fte','case_id','event','meta'}\n",
    "    True\n",
    "    >>> # Each meta cell contains structured JSON metadata:\n",
    "    >>> import json\n",
    "    >>> json.loads(ev.loc[0, 'meta'])\n",
    "    {'weighting': 2.5,\n",
    "     'case_type': 'Financial',\n",
    "     'concern_type': 'Neglect',\n",
    "     'status': 'Open',\n",
    "     'days_to_pg_signoff': 15.0}\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd, json\n",
    "    >>> # Two cases, two investigators, showcasing more event types\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id': ['S1', 'S2'],\n",
    "    ...     'team': ['A', 'B'],\n",
    "    ...     'fte': [1.0, 0.8],\n",
    "    ...     'case_id': ['C1', 'C2'],\n",
    "    ...     'weighting': [2.5, 1.0],\n",
    "    ...     'case_type': ['Financial', 'Welfare'],\n",
    "    ...     'concern_type': ['Neglect', 'Abuse'],\n",
    "    ...     'status': ['Open', 'Open'],\n",
    "    ...     'days_to_pg_signoff': [15, pd.NA],\n",
    "    ...     # Timeline dates (C1 has a full path incl. pg_signoff; C2 shows rejects, no signoff)\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05'), pd.Timestamp('2025-01-07')],\n",
    "    ...     'dt_alloc_team': [pd.Timestamp('2025-01-08'), pd.Timestamp('2025-01-09')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-10'), pd.Timestamp('2025-01-11')],\n",
    "    ...     'dt_sent_to_ca': [pd.Timestamp('2025-01-12'), pd.NaT],\n",
    "    ...     'dt_legal_req_1': [pd.Timestamp('2025-01-14'), pd.Timestamp('2025-01-15')],\n",
    "    ...     'dt_legal_req_2': [pd.NaT, pd.Timestamp('2025-01-18')],\n",
    "    ...     'dt_legal_req_3': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_legal_rej_1': [pd.NaT, pd.Timestamp('2025-01-17')],\n",
    "    ...     'dt_legal_rej_2': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_legal_approval': [pd.Timestamp('2025-01-20'), pd.NaT],\n",
    "    ...     'dt_pg_signoff': [pd.Timestamp('2025-01-25'), pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.Timestamp('2025-02-01'), pd.NaT],\n",
    "    ...     'dt_flagged': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> ev = build_event_log(typed)  # uses date_horizon(start=dt_received_inv, end=dt_pg_signoff+pad)\n",
    "    >>> # Unique event types emitted\n",
    "    >>> sorted(ev['event'].unique().tolist())\n",
    "    ['alloc_team', 'closed', 'legal_approval', 'legal_reject', 'legal_request',\n",
    "     'newcase', 'pg_signoff', 'received', 'sent_to_ca']\n",
    "    >>> # Schema check\n",
    "    >>> set(ev.columns) >= {'date','staff_id','team','fte','case_id','event','meta'}\n",
    "    True\n",
    "    >>> # Per-case event counts (C1 has a full pathway, C2 has requests + a reject)\n",
    "    >>> ev.groupby('case_id')['event'].count().to_dict()  # doctest: +ELLIPSIS\n",
    "    {'C1': 8, 'C2': 6}\n",
    "    >>> # meta is JSON with contextual fields\n",
    "    >>> m = json.loads(ev.loc[ev['case_id'].eq('C2')].iloc[0]['meta'])\n",
    "    >>> set(m.keys()) == {'weighting','case_type','concern_type','status','days_to_pg_signoff'}\n",
    "    True\n",
    "    >>> m['case_type'], m['concern_type'], m['weighting']\n",
    "    ('Welfare', 'Abuse', 1.0)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "\n",
    "    # Ensure expected minimal columns exist\n",
    "    base_cols = ['staff_id', 'team', 'fte', 'case_id']\n",
    "    for c in base_cols:\n",
    "        if c not in typed.columns:\n",
    "            raise KeyError(f\"build_event_log: required column '{c}' missing from 'typed'.\")\n",
    "\n",
    "    # Compute the date horizon\n",
    "    start, end = date_horizon(typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates)\n",
    "\n",
    "    # Helper to safely read a column if present\n",
    "    def getcol(name: str):\n",
    "        return typed[name] if name in typed.columns else pd.Series([pd.NaT] * len(typed), index=typed.index)\n",
    "\n",
    "    # Pre-pull columns used in meta (safe if absent)\n",
    "    weighting       = typed['weighting'] if 'weighting' in typed.columns else pd.Series([pd.NA]*len(typed), index=typed.index)\n",
    "    case_type       = typed['case_type'] if 'case_type' in typed.columns else pd.Series([pd.NA]*len(typed), index=typed.index)\n",
    "    concern_type    = typed['concern_type'] if 'concern_type' in typed.columns else pd.Series([pd.NA]*len(typed), index=typed.index)\n",
    "    status          = typed['status'] if 'status' in typed.columns else pd.Series([pd.NA]*len(typed), index=typed.index)\n",
    "    days_to_pg      = typed['days_to_pg_signoff'] if 'days_to_pg_signoff' in typed.columns else pd.Series([pd.NA]*len(typed), index=typed.index)\n",
    "\n",
    "    # Map of event names to the corresponding date columns to scan (one or many)\n",
    "    event_map = {\n",
    "        'received':      ['dt_received_inv'],\n",
    "        'alloc_team':    ['dt_alloc_team'],\n",
    "        'newcase':       ['dt_alloc_invest'],\n",
    "        'sent_to_ca':    ['dt_sent_to_ca'],\n",
    "        'legal_request': ['dt_legal_req_1', 'dt_legal_req_2', 'dt_legal_req_3'],\n",
    "        'legal_reject':  ['dt_legal_rej_1', 'dt_legal_rej_2'],\n",
    "        'legal_approval':['dt_legal_approval'],\n",
    "        'pg_signoff':    ['dt_pg_signoff'],\n",
    "        'court_order':   ['dt_date_of_order'],\n",
    "        'closed':        ['dt_close'],\n",
    "        'flagged':       ['dt_flagged'],\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    # Iterate row-wise to emit events per case\n",
    "    for i, r in typed.iterrows():\n",
    "        sid, team, fte, cid = r['staff_id'], r['team'], r['fte'], r['case_id']\n",
    "\n",
    "        # Build the meta payload once per row\n",
    "        meta_dict = {\n",
    "            'weighting': None if pd.isna(weighting.iloc[i]) else weighting.iloc[i],\n",
    "            'case_type': None if pd.isna(case_type.iloc[i]) else str(case_type.iloc[i]),\n",
    "            'concern_type': None if pd.isna(concern_type.iloc[i]) else str(concern_type.iloc[i]),\n",
    "            'status': None if pd.isna(status.iloc[i]) else str(status.iloc[i]),\n",
    "            'days_to_pg_signoff': None if pd.isna(days_to_pg.iloc[i]) else float(days_to_pg.iloc[i]),\n",
    "        }\n",
    "        meta_json = json.dumps(meta_dict, ensure_ascii=False)\n",
    "\n",
    "        # Emit events for each configured date column\n",
    "        for etype, cols in event_map.items():\n",
    "            for c in cols:\n",
    "                if c in typed.columns:\n",
    "                    dt = r[c]\n",
    "                    if pd.notna(dt):\n",
    "                        dtn = pd.to_datetime(dt).normalize()\n",
    "                        # Keep only within the computed horizon\n",
    "                        if start <= dtn <= end:\n",
    "                            records.append({\n",
    "                                'date': dtn,\n",
    "                                'staff_id': sid,\n",
    "                                'team': team,\n",
    "                                'fte': fte,\n",
    "                                'case_id': cid,\n",
    "                                'event': etype,\n",
    "                                'meta': meta_json\n",
    "                            })\n",
    "\n",
    "    ev = pd.DataFrame.from_records(records)\n",
    "\n",
    "    if ev.empty:\n",
    "        return pd.DataFrame(columns=['date','staff_id','team','fte','case_id','event','meta'])\n",
    "\n",
    "    # Deduplicate identical events (same staff/case/date/type)\n",
    "    ev = ev.drop_duplicates(subset=['date','staff_id','case_id','event']).sort_values(['date','staff_id','case_id','event']).reset_index(drop=True)\n",
    "\n",
    "    # Ensure dtypes are tidy\n",
    "    ev['date'] = pd.to_datetime(ev['date']).dt.normalize()\n",
    "    ev['fte']  = pd.to_numeric(ev['fte'], errors='coerce')\n",
    "\n",
    "    return ev\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Goal: build a day-by-day series showing how many cases each investigator has â€œin progressâ€ (WIP), and an optional workload measure that accounts for case complexity and staff FTE.\n",
    "\n",
    "- A case is counted as WIP from the day itâ€™s allocated to an investigator until the earliest of:\n",
    "    - it is closed, it gets PG sign-off, or we reach the reporting end date.\n",
    "    - \n",
    "- We want a daily time series showing, for each staff member, how many cases they are actively working (WIP = Work In Progress) and a simple workload measure that adjusts for case complexity and staff capacity.\n",
    "    - A case counts as WIP from the day it is allocated to an investigator (dt_alloc_invest) until the earliest of:\n",
    "        - the case is closed (dt_close), or\n",
    "        - it receives PG sign-off (dt_pg_signoff), or\n",
    "        - we reach the reporting end date.\n",
    "\n",
    "- Output is one row per date Ã— staff member Ã— team, with:\n",
    "    - wip (how many cases they have on the go) and wip_load (a proxy for workload = weighting Ã· FTE), summed over their active cases.\n",
    "        - A complex case (higher weighting) increases load.\n",
    "        - A part-time FTE increases load (same case is a bigger share of their time).\n",
    "\n",
    "- If you donâ€™t provide the start and end dates, the function works them out automatically using your project rules:\n",
    "    - Start horizon comes from the earliest dt_received_inv;\n",
    "    - End horizon comes from the latest dt_pg_signoff, plus a padding window.\n",
    "\n",
    "- It uses your official milestones (dt_alloc_invest, dt_close, dt_pg_signoff) to decide when a case is actively being worked.\n",
    "\n",
    "- Fast & scalable: It uses a delta method (add +1 at the start date, âˆ’1 after the end date) so it can efficiently build daily WIP counts even for thousands of cases.\n",
    "\n",
    "- It gives both a count (wip) and a load (wip_load = weighting Ã· FTE) so you can see not just how many cases someone has, but how heavy that workload likely is.\n",
    "\n",
    "- If some dates are missing, it falls back sensibly (e.g., if a case never closes, it stays WIP until the end horizon).\n",
    "\n",
    "\n",
    "## A tiny mental model\n",
    "- Think of each case as a bar on a timeline (from allocation to close/signoff).\n",
    "- We lay all bars for a person on top of each other.\n",
    "- For any given day, how many bars overlap? Thatâ€™s wip.\n",
    "- If some bars are â€œheavierâ€ (higher weighting) or the staff member has lower FTE, the overlap total becomes wip_load.\n",
    "\n",
    "## Common edge cases handled\n",
    "- Open cases with no close/signoff â†’ they count as WIP until the report end date.\n",
    "- Missing weighting/FTE â†’ sensible defaults keep the math stable.\n",
    "- No cases for a person â†’ they simply wonâ€™t appear in the output (or will have zeros after merge/accumulation).\n",
    "\n",
    "## Tiny visual example (intuition)\n",
    "If a case runs from Jan 2 to Jan 5:\n",
    "- We add +1 on Jan 2.\n",
    "- We add âˆ’1 on Jan 6 (the day after it finishes).\n",
    "- Cumulative sum across days produces:\n",
    "Jan 1: 0\n",
    "Jan 2: 1\n",
    "Jan 3: 1\n",
    "Jan 4: 1\n",
    "Jan 5: 1\n",
    "Jan 6: 0\n",
    "Now imagine multiple cases overlappingâ€”WIP is just the sum of overlaps each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Function: build_wip_series()\n",
    "# -------------------------------------------------------------\n",
    "def build_wip_series(\n",
    "    typed: pd.DataFrame,\n",
    "    start: pd.Timestamp | None = None,\n",
    "    end: pd.Timestamp | None = None,\n",
    "    pad_days: int = 14,\n",
    "    fallback_to_all_dates: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a Work-In-Progress (WIP) daily series per staff member.\n",
    "\n",
    "    A case is considered WIP from dt_alloc_invest (inclusive) to the earliest of:\n",
    "      - dt_close\n",
    "      - dt_pg_signoff\n",
    "      - provided/computed `end` horizon\n",
    "\n",
    "    If `start`/`end` are not provided, they are derived via `date_horizon()` with the\n",
    "    rule: start from dt_received_inv, end from dt_pg_signoff (+ pad_days).\n",
    "\n",
    "    Inputs and defaults:\n",
    "    typed: engineered table (one row per case).\n",
    "    start, end: optional date limits for the report.\n",
    "    If start or end are missing, it calls date_horizon() to derive them from the data using the rule (received â†’ pg_signoff + padding).\n",
    "    Then it normalises them to whole dates (midnight).\n",
    "\n",
    "    Output includes:\n",
    "      - `wip`       : number of active cases (count-based)\n",
    "      - `wip_load`  : workload proxy, defined as weighting / fte (fallbacks to 1.0 if absent)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Expected columns:\n",
    "          identifiers: ['staff_id','team','case_id'] (case_id optional for debugging)\n",
    "          dates: ['dt_alloc_invest','dt_close','dt_pg_signoff'] (+ others for date_horizon)\n",
    "          optional: ['weighting','fte']\n",
    "    start : pd.Timestamp | None\n",
    "        Start of the reporting horizon (normalised to date). If None, computed via date_horizon().\n",
    "    end : pd.Timestamp | None\n",
    "        End of the reporting horizon (normalised to date). If None, computed via date_horizon().\n",
    "    pad_days : int, default=14\n",
    "        Only used if start/end are not supplied; passed to date_horizon().\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        Passed to date_horizon().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['date','staff_id','team','wip','wip_load']\n",
    "        - One row per (date, staff_id, team).\n",
    "        - `wip` is guaranteed non-negative.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> # Two cases for S1; second case has PG sign-off. Includes weighting & fte for wip_load.\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id': ['S1','S1'],\n",
    "    ...     'team': ['A','A'],\n",
    "    ...     'case_id': ['C1','C2'],\n",
    "    ...     'fte': [1.0, 0.5],\n",
    "    ...     'weighting': [2.0, 1.0],\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-01')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-05')],\n",
    "    ...     'dt_close': [pd.Timestamp('2025-01-03'), pd.NaT],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.Timestamp('2025-01-07')],\n",
    "    ... })\n",
    "    >>> # Explicit horizon\n",
    "    >>> wip = build_wip_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-10'))\n",
    "    >>> set(wip.columns) == {'date','staff_id','team','wip','wip_load'}\n",
    "    True\n",
    "    >>> wip['wip'].ge(0).all()\n",
    "    True\n",
    "    >>> # On 2025-01-06, both cases are WIP -> wip >= 1\n",
    "    >>> int(wip.loc[wip['date'].eq(pd.Timestamp('2025-01-06')), 'wip'].max()) >= 1\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Compute horizon (if needed) ---\n",
    "    # If you donâ€™t pass start/end, the function figures them out using your project rule:\n",
    "    # start = earliest dt_received_inv\n",
    "    # end = latest dt_pg_signoff plus a small padding window\n",
    "    if start is None or end is None:\n",
    "        s, e = date_horizon(typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates)\n",
    "        if start is None:\n",
    "            start = s\n",
    "        if end is None:\n",
    "            end = e\n",
    "            \n",
    "    # Normalise\n",
    "    start = pd.to_datetime(start).normalize()\n",
    "    end = pd.to_datetime(end).normalize()\n",
    "\n",
    "    # --- Guard: required columns for interval construction ---\n",
    "    # Verifies key columns exist: staff_id, team, dt_alloc_invest.\n",
    "    # If any are missing, it raises a helpful error explaining whatâ€™s needed.\n",
    "    for c in ['staff_id', 'team', 'dt_alloc_invest']:\n",
    "        if c not in typed.columns:\n",
    "            raise KeyError(f\"build_wip_series: required column '{c}' missing from 'typed'.\")\n",
    "\n",
    "    # --- Prepare per-case start/end ---\n",
    "    # Start of work\n",
    "    s_col = pd.to_datetime(typed['dt_alloc_invest'], errors='coerce')\n",
    "\n",
    "    # Earliest of dt_close and dt_pg_signoff per row; then fallback to provided/computed end\n",
    "    # End of work = the earliest of dt_close and dt_pg_signoff.\n",
    "    # If both are missing, the end defaults to the overall report end date (so open cases remain WIP).\n",
    "    close_candidates = pd.concat(\n",
    "        [\n",
    "            pd.to_datetime(typed['dt_close'], errors='coerce') if 'dt_close' in typed else pd.Series(pd.NaT, index=typed.index),\n",
    "            pd.to_datetime(typed['dt_pg_signoff'], errors='coerce') if 'dt_pg_signoff' in typed else pd.Series(pd.NaT, index=typed.index),\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    row_end = close_candidates.min(axis=1)  # earliest available milestone\n",
    "    row_end = row_end.fillna(end)\n",
    "\n",
    "    # Case load for wip_load: weighting / fte (with robust fallbacks)\n",
    "    # If weighting is missing, it uses 1.0 (assume average complexity).\n",
    "    if 'weighting' in typed.columns:\n",
    "        w_series = pd.to_numeric(typed['weighting'], errors='coerce').fillna(1.0)\n",
    "    else:\n",
    "        w_series = pd.Series(1.0, index=typed.index)\n",
    "    # If fte is missing or zero, it uses 1.0 (avoid division by zero and keep a sane baseline).\n",
    "    if 'fte' in typed.columns:\n",
    "        fte_series = pd.to_numeric(typed['fte'], errors='coerce').replace(0, pd.NA).fillna(1.0)\n",
    "    else:\n",
    "        fte_series = pd.Series(1.0, index=typed.index)\n",
    "\n",
    "    # Load per case = weighting Ã· fte\n",
    "    load = (w_series / fte_series).astype(float)\n",
    "\n",
    "    # Creates a small table with one row per case showing:\n",
    "    # staff_id, team, start (allocation), end (close/signoff/report end), and load.\n",
    "    intervals = pd.DataFrame({\n",
    "        'staff_id': typed['staff_id'],\n",
    "        'team': typed['team'],\n",
    "        'start': s_col,\n",
    "        'end': row_end,\n",
    "        'load': load\n",
    "    }).dropna(subset=['start', 'end'])\n",
    "\n",
    "    # --- Build delta encoding (inclusive start, inclusive end) ---\n",
    "    # Delta encoding (efficient daily accumulation)\n",
    "    # Creates a full daily calendar and applies the cumulative sum of deltas.\n",
    "    deltas = []\n",
    "    horizon_start, horizon_end = start, end\n",
    "    for _, r in intervals.iterrows():\n",
    "        s = pd.to_datetime(r['start']).normalize()\n",
    "        e = pd.to_datetime(r['end']).normalize()\n",
    "\n",
    "        # Skip if outside horizon\n",
    "        if s > horizon_end or e < horizon_start:\n",
    "            continue\n",
    "\n",
    "        s = max(s, horizon_start)\n",
    "        e = min(e, horizon_end)\n",
    "        \n",
    "        # For each case interval: Add a +1 (and +load) on the start date.\n",
    "        # Add a âˆ’1 (and âˆ’load) on the day after the end date.\n",
    "        # +1 case and +load at start; -1 and -load at day after end\n",
    "        deltas.append((r['staff_id'], r['team'], s,  1.0,  r['load']))\n",
    "        deltas.append((r['staff_id'], r['team'], e + pd.Timedelta(days=1), -1.0, -r['load']))\n",
    "    # This means when we later cumulatively sum these daily changes, we get the number of active cases (and total load) each day.\n",
    "    if not deltas:\n",
    "        return pd.DataFrame(columns=['date', 'staff_id', 'team', 'wip', 'wip_load'])\n",
    "\n",
    "    deltas = pd.DataFrame(deltas, columns=['staff_id', 'team', 'date', 'd_cases', 'd_load'])\n",
    "    # Builds a continuous list of dates from start to end\n",
    "    all_dates = pd.DataFrame({'date': pd.date_range(horizon_start, horizon_end, freq='D')})\n",
    "\n",
    "    # --- Accumulate per staff/team over the horizon ---\n",
    "    # For each staff Ã— team group:\n",
    "    # Merges the deltas onto the daily grid.\n",
    "    # Cumulative sums to get wip (counts) and wip_load (load).\n",
    "    # Clips at zero to avoid negative values if data has gaps.\n",
    "    out_rows = []\n",
    "    for (sid, team), g in deltas.groupby(['staff_id', 'team'], sort=False):\n",
    "        gg = g.groupby('date', as_index=False)[['d_cases', 'd_load']].sum()\n",
    "        grid = all_dates.merge(gg, on='date', how='left').fillna({'d_cases': 0.0, 'd_load': 0.0})\n",
    "        # clip(lower=0) ensures small data glitches canâ€™t produce negatives.\n",
    "        grid['wip'] = grid['d_cases'].cumsum().clip(lower=0)           # case count\n",
    "        grid['wip_load'] = grid['d_load'].cumsum().clip(lower=0.0)     # workload proxy\n",
    "        grid['staff_id'] = sid\n",
    "        grid['team'] = team\n",
    "        out_rows.append(grid[['date', 'staff_id', 'team', 'wip', 'wip_load']])\n",
    "\n",
    "    out = pd.concat(out_rows, ignore_index=True) if out_rows else pd.DataFrame(\n",
    "        columns=['date', 'staff_id', 'team', 'wip', 'wip_load']\n",
    "    )\n",
    "\n",
    "    # Ensure dtypes / normalisation\n",
    "    out['date'] = pd.to_datetime(out['date']).dt.normalize()\n",
    "    out['wip'] = pd.to_numeric(out['wip'], errors='coerce').fillna(0).astype(float)\n",
    "    out['wip_load'] = pd.to_numeric(out['wip_load'], errors='coerce').fillna(0.0).astype(float)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Build a daily time series showing the size of the allocation backlog: \n",
    "# how many cases have been received into Investigations but not yet allocated to an investigator.\n",
    "\n",
    "## It builds a timeline of the allocation backlog â€” how many cases have arrived in Investigations but havenâ€™t yet been allocated to an investigator â€” day by day (or week by week).\n",
    "\n",
    "- It counts Received (cases entering the queue) and Allocated (cases leaving to a person) per day.\n",
    "- It then takes a running total (cumulative) of each and computes:\n",
    "    - Backlog = Total Received so far âˆ’ Total Allocated so far.\n",
    "- optionally:\n",
    "    - Exclude weekends/holidays to focus on working days only.\n",
    "    - Resample weekly or monthly, keeping the last cumulative value per period (the correct way for running totals).\n",
    "    - Compute a weighted backlog (if some cases are heavier/more complex) using a weighting column.\n",
    "\n",
    "- Received means dt_received_inv (case enters the Investigations queue).\n",
    "- Allocated means dt_alloc_invest (case leaves the queue and goes to a person).\n",
    "- Backlog available (on any day) = total received so far âˆ’ total allocated so far.\n",
    "- If we donâ€™t provide a reporting window, the function figures it out using your rules:\n",
    "    - Start from the earliest dt_received_inv.\n",
    "    - End at the latest dt_pg_signoff, with a padding window added.\n",
    "\n",
    "- Thereâ€™s also an optional weighted backlog, which treats some cases as â€œheavierâ€ based on weighting (e.g., complexity).\n",
    "- Matches our operational definition of backlog (waiting to be allocated to an investigator).\n",
    "- Operationally accurate: matches the definition of backlog (awaiting allocation).\n",
    "- Transparent: shows both cumulative inputs (received/allocated) and the resulting backlog; we publish cumulative received and allocated alongside the backlog so you can audit the numbers.\n",
    "- Robust: it works even if some days have no activity; it also can clip the backlog at zero to avoid confusing negatives; prevents negative backlog and handles days with no activity cleanly.\n",
    "- Flexible & practical: business-day filtering and weekly/monthly views match how teams actually review performance; it can compute a weighted version if you want a complexity-aware measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_backlog_series()\n",
    "# -------------------------------------------------------------\n",
    "def build_backlog_series(\n",
    "    typed: pd.DataFrame,\n",
    "    start: pd.Timestamp | None = None,\n",
    "    end: pd.Timestamp | None = None,\n",
    "    pad_days: int = 14,\n",
    "    fallback_to_all_dates: bool = True,\n",
    "    clip_zero: bool = True,\n",
    "    compute_weighted: bool = False,\n",
    "    exclude_weekends: bool = False,\n",
    "    holidays: list | pd.Series | None = None,\n",
    "    freq: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a daily backlog series where:\n",
    "        backlog = cumulative received âˆ’ cumulative allocated.\n",
    "\n",
    "    Definitions\n",
    "    -----------\n",
    "    - Received: cases entering Investigations (dt_received_inv)=(case enters Investigations queue).\n",
    "    - Allocated: cases allocated to an investigator (dt_alloc_invest)=(case leaves queue to an investigator).\n",
    "    - Backlog available: cases received but not yet allocated.\n",
    "\n",
    "    Horizon\n",
    "    -------\n",
    "    If `start`/`end` are not provided, they are derived via `date_horizon()`:\n",
    "      start := earliest dt_received_inv, \n",
    "      end := latest dt_pg_signoff (+ pad_days),\n",
    "      with optional fallback to all dt_* columns if primary dates are missing.\n",
    "\n",
    "    Options\n",
    "    -------\n",
    "    - clip_zero:        Prevent negative backlog (recommended).\n",
    "    - compute_weighted: Also compute weighted backlog using 'weighting' if present.\n",
    "    - exclude_weekends: Remove Saturdays/Sundays from the time axis.\n",
    "    - holidays:         Iterable of dates to exclude (e.g., UK bank holidays).\n",
    "    - freq:             Optional resampling frequency (e.g., 'W-MON', 'W-FRI', 'MS').\n",
    "                        For cumulative series, we take the last value per period.\n",
    "                        \n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Expected columns:\n",
    "          - dates: ['dt_received_inv','dt_alloc_invest']  (others allowed but not required)\n",
    "          - optional: ['weighting'] if compute_weighted=True\n",
    "        Note: this frame is already filtered to reallocated cases per your earlier requirement.\n",
    "    start, end : pd.Timestamp | None\n",
    "        Reporting horizon (inclusive). If None, computed via date_horizon().\n",
    "    pad_days : int, default=14\n",
    "        Only used when deriving start/end via date_horizon().\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        Passed to date_horizon().\n",
    "    clip_zero : bool, default=True\n",
    "        If True, backlog cannot go below 0 (defensive; improves interpretability).\n",
    "    compute_weighted : bool, default=False\n",
    "        If True and 'weighting' is present, also compute backlog_weighted\n",
    "        using the same logic but summing weights instead of counts.\n",
    "    exclude_weekends : bool, default=False\n",
    "        If True drop Saturdays/Sundays from the series\n",
    "    holidays : bool, default=False\n",
    "        If True drop a custom list/series of dates (e.g., UK bank holidays)\n",
    "    freq : str | None\n",
    "        optional resampling (e.g., 'W-MON', 'W-FRI', 'MS' for month-start). \n",
    "        For cumulative series, we take the last value per period.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with at least following Columns (daily):\n",
    "          - date\n",
    "          - received_cum      : cumulative count of received\n",
    "          - allocated_cum     : cumulative count of allocated\n",
    "          - backlog_available : received_cum - allocated_cum (clipped at 0 if clip_zero)\n",
    "          - (optional: and, if compute_weighted) received_weighted_cum, allocated_weighted_cum, backlog_weighted\n",
    "            \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ... })\n",
    "    >>> backlog = build_backlog_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05'))\n",
    "    >>> list(backlog.columns)\n",
    "    ['date', 'received_cum', 'allocated_cum', 'backlog_available']\n",
    "    >>> backlog.iloc[-1]['backlog_available']  # 2 received, 1 allocated -> 1\n",
    "    1.0\n",
    "\n",
    "    >>> # Weighted example (if 'weighting' present)\n",
    "    >>> typed2 = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ...     'weighting': [2.0, 0.5],\n",
    "    ... })\n",
    "    >>> backlog_w = build_backlog_series(typed2, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05'), compute_weighted=True)\n",
    "    >>> {'backlog_available', 'backlog_weighted'}.issubset(backlog_w.columns)\n",
    "    True\n",
    "\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ... })\n",
    "    >>> # Daily (default calendar)\n",
    "    >>> build_backlog_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05')).tail(1)[['backlog_available']].iloc[0,0]\n",
    "    1.0\n",
    "    \n",
    "    >>> # Business days only (excludes weekends)\n",
    "    >>> business = build_backlog_series(\n",
    "    ...     typed,\n",
    "    ...     pd.Timestamp('2025-01-01'),\n",
    "    ...     pd.Timestamp('2025-01-10'),\n",
    "    ...     exclude_weekends=True\n",
    "    ... )\n",
    "    \n",
    "    >>> # With holidays excluded and weekly roll-up (end-of-week values)\n",
    "    >>> holidays = [pd.Timestamp('2025-01-06')]\n",
    "    >>> weekly = build_backlog_series(\n",
    "    ...     typed,\n",
    "    ...     pd.Timestamp('2025-01-01'),\n",
    "    ...     pd.Timestamp('2025-01-31'),\n",
    "    ...     exclude_weekends=True,\n",
    "    ...     holidays=holidays,\n",
    "    ...     freq='W-FRI'\n",
    "    ... )\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Derive horizon if needed  ---\n",
    "    # If we didnâ€™t pass start/end, we derive them with date_horizon()\n",
    "    if start is None or end is None:\n",
    "        s, e = date_horizon(typed, pad_days=pad_days, \n",
    "                            fallback_to_all_dates=fallback_to_all_dates)\n",
    "        if start is None:\n",
    "            start = s\n",
    "        if end is None:\n",
    "            end = e\n",
    "    # Normalise them to dates (no times).\n",
    "    start = pd.to_datetime(start).normalize()\n",
    "    end = pd.to_datetime(end).normalize()\n",
    "\n",
    "    # --- Extract and normalise event dates ---\n",
    "    rec_dates = pd.to_datetime(\n",
    "        typed.get('dt_received_inv', pd.Series([], dtype='datetime64[ns]')),\n",
    "        errors='coerce'\n",
    "    ).dropna().dt.normalize()\n",
    "    alloc_dates = pd.to_datetime(\n",
    "        typed.get('dt_alloc_invest', pd.Series([], dtype='datetime64[ns]')),\n",
    "        errors='coerce'\n",
    "    ).dropna().dt.normalize()\n",
    "    \n",
    "    # --- Daily counts (received / allocated) ---\n",
    "    # Daily counts â†’ cumulative totals\n",
    "    # Count how many received and allocated events happen per day.\n",
    "    received_daily = rec_dates.value_counts().sort_index()\n",
    "    allocated_daily = alloc_dates.value_counts().sort_index()\n",
    "\n",
    "    # --- Build full daily index over the horizon ---\n",
    "    idx = pd.date_range(start, end, freq='D')\n",
    "\n",
    "    # Optional calendar filtering (weekends and/or holidays)\n",
    "    if exclude_weekends:\n",
    "        idx = idx[idx.weekday < 5]  # 0=Mon ... 4=Fri\n",
    "    if holidays is not None and len(pd.Index(holidays)) > 0:\n",
    "        hol = pd.to_datetime(pd.Index(holidays)).normalize()\n",
    "        idx = idx.difference(hol)\n",
    "\n",
    "    # Helper to reindex to possibly filtered calendar and cumulate\n",
    "    def cumulate(series_counts: pd.Series, index: pd.DatetimeIndex) -> pd.Series:\n",
    "        # We need the *full* daily cumsum first, then realign to filtered index\n",
    "        full_range = pd.date_range(start, end, freq='D')\n",
    "        full_cum = series_counts.reindex(full_range, fill_value=0).cumsum().astype(float)\n",
    "        # If calendar is filtered, take values at the kept dates\n",
    "        return full_cum.reindex(index, method='ffill').fillna(0.0)\n",
    "        \n",
    "    # --- Cumulate counts over the horizon (missing days = 0) ---\n",
    "    # Reindex missing days as zeros and cumulatively sum to get â€œtotal so farâ€.\n",
    "    received_cum = received_daily.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "    allocated_cum = allocated_daily.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "\n",
    "    # Backlog is the gap between total received and total allocated.\n",
    "    backlog = (received_cum - allocated_cum)\n",
    "    # Optionally clip at 0 (defensive, avoids negative values if historical allocations\n",
    "    #  predate the first received in the window).\n",
    "    if clip_zero:\n",
    "        backlog = backlog.clip(lower=0.0)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'date': idx,\n",
    "        'received_cum': received_cum.values,\n",
    "        'allocated_cum': allocated_cum.values,\n",
    "        'backlog_available': backlog.values\n",
    "    })\n",
    "\n",
    "    # --- Optional weighted backlog ---\n",
    "    # Sum weights per day at receipt and at allocation, then cumulate and subtract.\n",
    "    # Same structure as counts, but with weights instead of 1s.\n",
    "    if compute_weighted:\n",
    "        # If weighting missing, assume 1.0 for rows with the date present, else 0\n",
    "        weights = pd.to_numeric(typed.get('weighting', pd.Series([1.0] * len(typed))), errors='coerce').fillna(1.0)\n",
    "        \n",
    "        # Map weights to dates for received and allocated events\n",
    "        def weighted_daily(dates: pd.Series, weight_series: pd.Series) -> pd.Series:\n",
    "            if len(dates) == 0:\n",
    "                return pd.Series(dtype=float)\n",
    "            tmp = pd.DataFrame({'date': dates.reset_index(drop=True)})\n",
    "            # Align weights to the same original row positions as 'dates'\n",
    "            tmp['weight'] = weight_series.loc[dates.index].values\n",
    "            return tmp.groupby('date')['weight'].sum().sort_index()\n",
    "\n",
    "        # Build per-date weight sums for received and allocated\n",
    "        rec_w_daily = weighted_daily(rec_dates, weights)\n",
    "        alloc_w_daily = weighted_daily(alloc_dates, weights)\n",
    "\n",
    "        # reindex\n",
    "        rec_w_cum = cumulate(rec_w_daily, idx)\n",
    "        alloc_w_cum = cumulate(alloc_w_daily, idx)\n",
    "        \n",
    "        # # Build per-date weight sums for received and allocated\n",
    "        # rec_weights = (\n",
    "        #     pd.DataFrame({'date': rec_dates.reset_index(drop=True)})\n",
    "        #     .assign(weight=weights.loc[rec_dates.index].values if len(rec_dates) else [])\n",
    "        #     .groupby('date')['weight'].sum()\n",
    "        #     if len(rec_dates) else pd.Series(dtype=float)\n",
    "        # )\n",
    "\n",
    "        # alloc_weights = (\n",
    "        #     pd.DataFrame({'date': alloc_dates.reset_index(drop=True)})\n",
    "        #     .assign(weight=weights.loc[alloc_dates.index].values if len(alloc_dates) else [])\n",
    "        #     .groupby('date')['weight'].sum()\n",
    "        #     if len(alloc_dates) else pd.Series(dtype=float)\n",
    "        # )\n",
    "\n",
    "        # rec_w_cum = rec_weights.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "        # alloc_w_cum = alloc_weights.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "\n",
    "        backlog_w = (rec_w_cum - alloc_w_cum)\n",
    "        if clip_zero:\n",
    "            backlog_w = backlog_w.clip(lower=0.0)\n",
    "\n",
    "        out['received_weighted_cum'] = rec_w_cum.values\n",
    "        out['allocated_weighted_cum'] = alloc_w_cum.values\n",
    "        out['backlog_weighted'] = backlog_w.values\n",
    "\n",
    "    # --- Optional resampling (weekly/monthly views)\n",
    "    if freq is not None:\n",
    "        # Set index for resampling, then take \"last\" per period for cumulative metrics.\n",
    "        out = out.set_index('date').sort_index()\n",
    "        agg_map = {\n",
    "            'received_cum': 'last',\n",
    "            'allocated_cum': 'last',\n",
    "            'backlog_available': 'last',\n",
    "        }\n",
    "        if compute_weighted:\n",
    "            agg_map.update({\n",
    "                'received_weighted_cum': 'last',\n",
    "                'allocated_weighted_cum': 'last',\n",
    "                'backlog_weighted': 'last',\n",
    "            })\n",
    "        out = out.resample(freq).agg(agg_map).dropna(how='all').reset_index()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Builds the daily picture of staff activity and backlog pressure across the investigation process.\n",
    "- Each row in the output shows, for each investigator on each date:\n",
    "    - how many cases they were working on (wip)\n",
    "    - how heavy that workload was (wip_load)\n",
    "    - what events happened that day (e.g., new case, legal step, PG sign-off)\n",
    "    - how long since they last picked up a case\n",
    "    - whether they are new in post (less than 4 weeks)\n",
    "    - what the system backlog looked like that day\n",
    "    - day-of-week, term, season, and bank holiday context\n",
    "- The result feeds directly into forecasting models, dashboards, or simulation inputs.\n",
    "- Combines everything: merges workload, case flow, and events into a single daily dataset.\n",
    "- Flexible: supports working-day calendars, holiday exclusions, and weekly backlog summaries.\n",
    "- Transparent: every part comes from separate, auditable builder functions â€” nothing hidden.\n",
    "- Scalable: runs efficiently even for many staff over long periods.\n",
    "\n",
    "- Step-by-step logic\n",
    "    1. Determine the date range (start/end) using date_horizon().\n",
    "    2.  Build core inputs:\n",
    "        - events = timeline of case milestones.\n",
    "        - wip = ongoing cases per staff/day.\n",
    "        - backlog = unallocated cases per day (received âˆ’ allocated).\n",
    "    3. Create a daily grid for all staff and all working dates.\n",
    "    4. Merge in WIP and events, turning event names into flag columns (0/1).\n",
    "    5. Compute features:\n",
    "        - time since last new case pickup\n",
    "        - week, season, term, holiday, and new starter status\n",
    "    6. Join backlog context to every dayâ€™s record.\n",
    "    7. Return three consistent datasets for downstream modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_daily_panel()\n",
    "# -------------------------------------------------------------\n",
    "def build_daily_panel(\n",
    "    typed: pd.DataFrame,\n",
    "    start: pd.Timestamp | None = None,\n",
    "    end: pd.Timestamp | None = None,\n",
    "    *,\n",
    "    pad_days: int = 14,\n",
    "    fallback_to_all_dates: bool = True,\n",
    "    # Pass-through options to backlog & WIP builders\n",
    "    backlog_kwargs: dict | None = None,\n",
    "    wip_kwargs: dict | None = None,\n",
    "    # Panel calendar options (also forwarded into backlog unless overridden there)\n",
    "    exclude_weekends: bool = False,\n",
    "    holidays: list | pd.Series | None = None,\n",
    "    backlog_freq: str | None = None,  # e.g. 'W-FRI', 'W-MON', 'MS'\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a fully-featured daily staff panel for modelling and analytics.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    This function combines outputs from:\n",
    "      - build_event_log()     â†’ daily operational events (e.g., newcase, legal, sign-off)\n",
    "      - build_wip_series()    â†’ daily work-in-progress (active cases, workloads)\n",
    "      - build_backlog_series()â†’ daily system backlog (received minus allocated)\n",
    "    into one unified dataset at the **staff Ã— date** level.\n",
    "\n",
    "    Calendar controls\n",
    "    -----------------\n",
    "    exclude_weekends : if True, panel dates will exclude Saturdays/Sundays\n",
    "    holidays         : iterable of dates marked as bank holidays in the panel;\n",
    "                       passed to backlog as exclusions too (unless overridden).\n",
    "    backlog_freq     : resampling frequency for backlog only (e.g., 'W-FRI', 'MS').\n",
    "                       Daily panel remains daily (or business-day if exclude_weekends=True).\n",
    "                       \n",
    "    Horizon:\n",
    "    If `start` and `end` are not provided, the function automatically determines\n",
    "    the date range using `date_horizon()` based on your projectâ€™s rule:\n",
    "      start := earliest dt_received_inv\n",
    "      end   := latest `dt_pg_signoff` (+ padding of `pad_days`)\n",
    "    Set fallback_to_all_dates=True to allow scanning all dt_* if primaries are missing.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Event flags derived from build_event_log(): newcase, alloc_team, sent_to_ca,\n",
    "      legal_request, legal_reject, legal_approval, pg_signoff, court_order, closed, flagged.\n",
    "    - Compact flags provided: event_newcase, event_legal, event_court, event_pg_signoff,\n",
    "      event_sent_to_ca, event_flagged.\n",
    "    - WIP uses dt_alloc_invest â†’ earliest(dt_close, dt_pg_signoff, end).\n",
    "    \n",
    "       typed : pd.DataFrame\n",
    "        Feature-engineered dataframe from `engineer()`, typically filtered\n",
    "        to reallocated cases.\n",
    "        Must include:\n",
    "          - Identifiers: `case_id`, `staff_id`, `team`, `role`, `fte`\n",
    "          - Core dates:  `dt_received_inv`, `dt_alloc_invest`, `dt_pg_signoff`,\n",
    "                         `dt_close` (and optionally legal & court milestones)\n",
    "        Optional columns (used if present):\n",
    "          - `weighting`, `status`, `case_type`, `concern_type`,\n",
    "            `days_to_pg_signoff`, etc.\n",
    "\n",
    "    start, end : pd.Timestamp | None, default None\n",
    "        Reporting horizon. If not given, derived automatically from `date_horizon()`.\n",
    "\n",
    "    pad_days : int, default 14\n",
    "        Number of days to extend the end horizon when deriving automatically.\n",
    "\n",
    "    fallback_to_all_dates : bool, default True\n",
    "        When true, allows `date_horizon()` to use all dt_* columns if the primary\n",
    "        (received / PG sign-off) columns are missing or incomplete.\n",
    "\n",
    "    backlog_kwargs : dict | None\n",
    "        Extra keyword arguments forwarded to `build_backlog_series()`.\n",
    "        Examples:\n",
    "            {'compute_weighted': True, 'clip_zero': True,\n",
    "             'exclude_weekends': False, 'holidays': holidays,\n",
    "             'freq': 'W-FRI'}\n",
    "\n",
    "    wip_kwargs : dict | None\n",
    "        Extra keyword arguments forwarded to `build_wip_series()`.\n",
    "        Example:\n",
    "            {'pad_days': 14, 'fallback_to_all_dates': True}\n",
    "\n",
    "    exclude_weekends : bool, default False\n",
    "        If True, weekends (Saturday/Sunday) are excluded from the daily panel\n",
    "        and from the backlog calculation.\n",
    "\n",
    "    holidays : list | pd.Series | None, default None\n",
    "        List or Series of public holidays to exclude from the panel timeline\n",
    "        and mark with `bank_holiday = 1`.\n",
    "\n",
    "    backlog_freq : str | None, default None\n",
    "        Optional resampling frequency for backlog only.\n",
    "        Examples: 'W-FRI' (weekly, Friday close), 'MS' (month-start).\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    Returns\n",
    "    -----------------------------------------------------------------------\n",
    "    tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "        (daily, backlog, events)\n",
    "\n",
    "        **daily** : pd.DataFrame  \n",
    "        One row per (date Ã— staff Ã— team), containing:\n",
    "          - Workload:  `wip`, `wip_load`\n",
    "          - Backlog context: `backlog_available`\n",
    "          - Event flags: `event_newcase`, `event_legal`, `event_court`,\n",
    "                         `event_pg_signoff`, `event_sent_to_ca`, `event_flagged`\n",
    "          - Calendar features: `dow`, `season`, `term_flag`, `bank_holiday`\n",
    "          - Tenure features: `weeks_since_start`, `is_new_starter`\n",
    "          - Temporal context: `time_since_last_pickup`\n",
    "\n",
    "        **backlog** : pd.DataFrame  \n",
    "        System-level backlog series built by `build_backlog_series()` with optional\n",
    "        business-day or weekly/monthly resampling.\n",
    "\n",
    "        **events** : pd.DataFrame  \n",
    "        Event log built by `build_event_log()`, containing granular dated events\n",
    "        per staff, case, and team.\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    Examples\n",
    "    -----------------------------------------------------------------------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'case_id': ['C1','C2'],\n",
    "    ...     'investigator': ['Alice','Bob'],\n",
    "    ...     'team': ['T1','T1'],\n",
    "    ...     'role': ['Investigator','Investigator'],\n",
    "    ...     'fte': [1.0, 0.8],\n",
    "    ...     'staff_id': ['S1','S2'],\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-02')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_legal_req_1': [pd.NaT, pd.Timestamp('2025-01-04')],\n",
    "    ...     'dt_legal_approval': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> start, end = pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05')\n",
    "    >>> daily, backlog, events = build_daily_panel(\n",
    "    ...     typed,\n",
    "    ...     start=start,\n",
    "    ...     end=end,\n",
    "    ...     exclude_weekends=True,\n",
    "    ...     holidays=[pd.Timestamp('2025-01-03')],\n",
    "    ...     backlog_freq='W-FRI',\n",
    "    ...     backlog_kwargs={'compute_weighted': True}\n",
    "    ... )\n",
    "    >>> # Daily panel has one row per staff per day\n",
    "    >>> set({'date','staff_id','team','fte','wip','event_newcase'}).issubset(daily.columns)\n",
    "    True\n",
    "    >>> # Backlog matches the number of working days\n",
    "    >>> len(backlog) <= (end - start).days + 1\n",
    "    True\n",
    "    >>> # Event log contains expected event types\n",
    "    >>> {'newcase','legal_request'}.issubset(set(events['event'].unique())) if not events.empty else True\n",
    "    True\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    backlog_kwargs = {} if backlog_kwargs is None else dict(backlog_kwargs)\n",
    "    wip_kwargs = {} if wip_kwargs is None else dict(wip_kwargs)\n",
    "\n",
    "    # --- 1) Determine horizon (uses your updated rule) ---\n",
    "    if start is None or end is None:\n",
    "        s, e = date_horizon(typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates)\n",
    "        start = s if start is None else start\n",
    "        end = e if end is None else end\n",
    "    start = pd.to_datetime(start).normalize()\n",
    "    end = pd.to_datetime(end).normalize()\n",
    "\n",
    "    # --- 2) Build the three core artefacts from the pipeline (events, WIP, backlog)---\n",
    "    events = build_event_log(typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates)\n",
    "\n",
    "    # WIP stays daily across full horizon; the panel may later filter dates\n",
    "    wip = build_wip_series(typed, start=start, end=end, **wip_kwargs)\n",
    "\n",
    "    # Ensure panel-level calendar options are forwarded to backlog unless explicitly set\n",
    "    backlog_defaults = {\n",
    "        'pad_days': pad_days,\n",
    "        'fallback_to_all_dates': fallback_to_all_dates,\n",
    "        'exclude_weekends': exclude_weekends,\n",
    "        'holidays': holidays,\n",
    "        'freq': backlog_freq,\n",
    "    }\n",
    "    for k, v in backlog_defaults.items():\n",
    "        backlog_kwargs.setdefault(k, v)\n",
    "\n",
    "    backlog = build_backlog_series(\n",
    "        typed,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        **backlog_kwargs\n",
    "    )\n",
    "\n",
    "    # 3) Panel date index (daily or business-day)\n",
    "    date_index = pd.date_range(start, end, freq='D')\n",
    "    if exclude_weekends:\n",
    "        date_index = date_index[date_index.weekday < 5]\n",
    "    if holidays is not None and len(pd.Index(holidays)) > 0:\n",
    "        hol = pd.to_datetime(pd.Index(holidays)).normalize()\n",
    "        date_index = date_index.difference(hol)\n",
    "    dates = pd.DataFrame({'date': date_index})\n",
    "\n",
    "    # 5) Merge WIP data (wip & wip_load). If grid has filtered dates, merge naturally subsets.\n",
    "    grid = grid.merge(wip, on=['date', 'staff_id', 'team'], how='left')\n",
    "    for c, default in [('wip', 0.0), ('wip_load', 0.0)]:\n",
    "        grid[c] = pd.to_numeric(grid.get(c, default), errors='coerce').fillna(default).astype(float)\n",
    "\n",
    "\n",
    "    # 6) Pivot events â†’ daily flags per staff\n",
    "    if not events.empty:\n",
    "        ev_flags = (\n",
    "            events.assign(flag=1)\n",
    "                  .pivot_table(index=['date', 'staff_id'], columns='event', values='flag', aggfunc='max')\n",
    "                  .reset_index()\n",
    "        )\n",
    "        \n",
    "        # Merge at staff-day; team may differ if staff moved teams, but WIP merge above anchors team\n",
    "        grid = grid.merge(ev_flags, on=['date', 'staff_id'], how='left')\n",
    "\n",
    "    # Ensure a stable set of event columns exists\n",
    "    event_cols = [\n",
    "        'newcase', 'alloc_team', 'sent_to_ca',\n",
    "        'legal_request', 'legal_reject', 'legal_approval',\n",
    "        'pg_signoff', 'court_order', 'closed', 'flagged'\n",
    "    ]\n",
    "    for c in event_cols:\n",
    "        grid[c] = grid.get(c, 0)\n",
    "        grid[c] = grid[c].fillna(0).astype(int)\n",
    "\n",
    "    # Compact event groupings useful for modelling\n",
    "    grid['event_newcase']    = grid['newcase'].astype(int)\n",
    "    grid['event_legal']      = ((grid['legal_request'] + grid['legal_approval'] + grid['legal_reject']) > 0).astype(int)\n",
    "    grid['event_court']      = grid['court_order'].astype(int)\n",
    "    grid['event_pg_signoff'] = grid['pg_signoff'].astype(int)\n",
    "    grid['event_sent_to_ca'] = grid['sent_to_ca'].astype(int)\n",
    "    grid['event_flagged']    = grid['flagged'].astype(int)\n",
    "\n",
    "    # 7) Days since last pickup (per staff)\n",
    "    grid = grid.sort_values(['staff_id', 'date'])\n",
    "    def _days_since_last_pickup(series: pd.Series) -> pd.Series:\n",
    "        out, last = [], None\n",
    "        for i, v in enumerate(series):\n",
    "            if v == 1:\n",
    "                last = i\n",
    "                out.append(0)\n",
    "            else:\n",
    "                out.append(i - last if last is not None else pd.NA)\n",
    "        return pd.Series(out, index=series.index)\n",
    "        \n",
    "    grid['time_since_last_pickup'] = (\n",
    "        grid.groupby('staff_id', group_keys=False)['event_newcase']\n",
    "            .apply(_days_since_last_pickup)\n",
    "            .fillna(99)\n",
    "            .astype(int)\n",
    "    )\n",
    "    \n",
    "    # 8) Calendar features\n",
    "    grid['dow'] = grid['date'].dt.day_name().str[:3]\n",
    "    grid['season'] = grid['date'].dt.month.map(month_to_season)\n",
    "    grid['term_flag'] = grid['date'].dt.month.map(is_term_month).astype(int)\n",
    "    # Bank holiday flag (1 if the date is in holidays)\n",
    "    if holidays is not None and len(pd.Index(holidays)) > 0:\n",
    "        hol = pd.to_datetime(pd.Index(holidays)).normalize()\n",
    "        grid['bank_holiday'] = grid['date'].isin(hol).astype(int)\n",
    "    else:\n",
    "        grid['bank_holiday'] = 0\n",
    "\n",
    "\n",
    "    # 9) New starter (tenure) features (weeks since first allocation per staff)\n",
    "    first_alloc = (\n",
    "        typed.dropna(subset=['dt_alloc_invest'])\n",
    "             .groupby('staff_id')['dt_alloc_invest'].min()\n",
    "             .rename('first_alloc')\n",
    "    )\n",
    "    grid = grid.merge(first_alloc, on='staff_id', how='left')\n",
    "    grid['weeks_since_start'] = (\n",
    "        (grid['date'] - grid['first_alloc']).dt.days // 7\n",
    "    ).fillna(0).clip(lower=0).astype(int)\n",
    "    grid['is_new_starter'] = (grid['weeks_since_start'] < 4).astype(int)\n",
    "    grid = grid.drop(columns=['first_alloc'])\n",
    "\n",
    "    # 10) Merge backlog (always by 'date'; backlog may be resampled)\n",
    "    # If backlog was resampled (e.g., weekly), forward-fill to panel dates.\n",
    "    if 'date' in backlog.columns and backlog['date'].is_monotonic_increasing:\n",
    "        back = backlog.set_index('date').sort_index()\n",
    "        # Keep only the core columns we need (avoid accidental merges)\n",
    "        keep_cols = [c for c in back.columns if c in {'received_cum','allocated_cum','backlog_available',\n",
    "                                                      'received_weighted_cum','allocated_weighted_cum','backlog_weighted'}]\n",
    "        back = back[keep_cols]\n",
    "        back = back.reindex(date_index, method='ffill')  # align to panel calendar\n",
    "        back = back.reset_index().rename(columns={'index': 'date'})\n",
    "    else:\n",
    "        back = backlog.copy()\n",
    "\n",
    "    grid = grid.merge(back, on='date', how='left')\n",
    "    grid['backlog_available'] = pd.to_numeric(grid.get('backlog_available', 0.0), errors='coerce').fillna(0.0)\n",
    "\n",
    "\n",
    "    # 11) Final tidy columns & order\n",
    "    cols = [\n",
    "        'date', 'staff_id', 'team', 'role', 'fte',\n",
    "        'wip', 'wip_load',\n",
    "        'time_since_last_pickup', 'weeks_since_start', 'is_new_starter',\n",
    "        'backlog_available', 'term_flag', 'season', 'dow', 'bank_holiday',\n",
    "        'event_newcase', 'event_legal', 'event_court', 'event_pg_signoff',\n",
    "        'event_sent_to_ca', 'event_flagged'\n",
    "    ]\n",
    "    cols = [c for c in cols if c in grid.columns]  # be tolerant\n",
    "    daily = grid[cols].sort_values(['staff_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "    return daily, backlog, events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily, backlog, events = build_daily_panel(\n",
    "    typed,\n",
    "    # optional: let it auto-derive start/end via date_horizon()\n",
    "    exclude_weekends=True,\n",
    "    holidays=[pd.Timestamp('2025-05-05'), pd.Timestamp('2025-08-25')],  # UK BHs (example)\n",
    "    backlog_freq='W-FRI',  # weekly backlog, last value each Friday\n",
    "    backlog_kwargs={'compute_weighted': True, 'clip_zero': True},  # weighted backlog too\n",
    "    wip_kwargs={'pad_days': 14, 'fallback_to_all_dates': True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Rolls up the detailed daily staff panel into team-level (or any custom grouping) time series, to quickly see trends like â€œtotal WIP per team per day/weekâ€ or â€œhow many new cases did Team A pick up last month?â€.\n",
    "\n",
    "- Practical: real reporting/forecasting often needs team- or org-level time series, not just staff-level detail.\n",
    "- Correct aggregation: it sums â€œflowâ€ metrics (e.g., events, WIP cases) and treats stateful metrics (like backlog levels) correctly when resampling by taking the last value per period (the right way to downsample cumulative/state variables).\n",
    "- Flexible: you pick the grouping keys, the resampling frequency, and can override the aggregation rules if needed.\n",
    "\n",
    "## How it works (step-by-step)\n",
    "1. Choose the grouping\n",
    "   By default it groups by date and team. You can change by to include role, or collapse to just date for an overall total.\n",
    "2. Aggregate daily\n",
    "   It sums WIP and WIP load across staff, sums events, takes the median time since last pickup (typical day for staff), and counts distinct staff on duty.\n",
    "3. (Optional) Resample to weekly/monthly\n",
    "   If you pass freq='W-FRI' (weekly Fridays) or 'MS' (month-start), it:\n",
    "   - Sums the â€œflowâ€ fields within each period (e.g., total new cases in that week/month).\n",
    "   - Takes the last value for stateful/level fields (e.g., backlog_available) so the weekly/monthly series reflects the end-of-period level.\n",
    "4. Return a tidy frame\n",
    "   With columns like: wip_sum, wip_load_sum, event_*_sum, backlog_available_mean (daily means) and, when resampled, last values for backlog-like metrics (you can change the list via resample_cum_last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Function: summarise_daily_panel()\n",
    "# -------------------------------------------------------------\n",
    "def summarise_daily_panel(\n",
    "    daily: pd.DataFrame,\n",
    "    by: list[str] = (\"date\", \"team\"),\n",
    "    *,\n",
    "    freq: str | None = None,\n",
    "    # How to aggregate each metric; sensible defaults provided\n",
    "    agg_map: dict | None = None,\n",
    "    # If resampling, how to aggregate cumulative-style fields\n",
    "    resample_cum_last: tuple[str, ...] = (\"backlog_available\",),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise the daily staff panel by date/team (or any grouping).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    daily : pd.DataFrame\n",
    "        Output of build_daily_panel()[0], with columns like:\n",
    "          ['date','staff_id','team','wip','wip_load','backlog_available',\n",
    "           'event_newcase','event_legal','event_court','event_pg_signoff',\n",
    "           'event_sent_to_ca','event_flagged','time_since_last_pickup', ...]\n",
    "    by : list[str], default ('date','team')\n",
    "        Grouping columns. Must include 'date' if you want a time series.\n",
    "        Examples: ('date',), ('date','team'), ('date','team','role')\n",
    "    freq : str | None, default None\n",
    "        Optional resampling frequency over time *after* grouping.\n",
    "        Examples: 'W-FRI', 'MS'. If None, returns daily resolution.\n",
    "    agg_map : dict | None, default None\n",
    "        Custom aggregation map. If None, a sensible default is used:\n",
    "          - Sum counts/loads/events\n",
    "          - Mean backlog_available\n",
    "          - Median time_since_last_pickup\n",
    "          - Distinct staff_count\n",
    "    resample_cum_last : tuple[str,...], default ('backlog_available',)\n",
    "        For resampling, fields treated as *cumulative/stateful* and aggregated\n",
    "        via 'last' per period (e.g., backlog_available).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per group (and per period if resampled). Includes:\n",
    "          - wip_sum, wip_load_sum\n",
    "          - backlog_available_mean (and backlog_available_last if resampled)\n",
    "          - events counts: newcase, legal, court, pg_signoff, sent_to_ca, flagged\n",
    "          - staff_count (distinct staff_id)\n",
    "          - time_since_last_pickup_median\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # team-level daily\n",
    "    >>> team_daily = summarise_daily_panel(daily, by=['date','team'])\n",
    "    >>> # team-level weekly (Friday)\n",
    "    >>> team_weekly = summarise_daily_panel(daily, by=['date','team'], freq='W-FRI')\n",
    "    \"\"\"\n",
    "    if \"date\" not in by:\n",
    "        raise ValueError(\"`by` must include 'date' to preserve time order (or set freq=None for a non-time summary).\")\n",
    "\n",
    "    # Default aggregation plan\n",
    "    default_agg = {\n",
    "        \"wip\": \"sum\",\n",
    "        \"wip_load\": \"sum\",\n",
    "        \"backlog_available\": \"mean\",  # daily mean backlog across staff on that date\n",
    "        \"event_newcase\": \"sum\",\n",
    "        \"event_legal\": \"sum\",\n",
    "        \"event_court\": \"sum\",\n",
    "        \"event_pg_signoff\": \"sum\",\n",
    "        \"event_sent_to_ca\": \"sum\",\n",
    "        \"event_flagged\": \"sum\",\n",
    "        \"time_since_last_pickup\": \"median\",\n",
    "        \"staff_id\": pd.Series.nunique,  # distinct headcount working that day\n",
    "    }\n",
    "    if agg_map is not None:\n",
    "        default_agg.update(agg_map)\n",
    "\n",
    "    # Group and aggregate on the daily grid\n",
    "    grouped = (\n",
    "        daily.groupby(list(by), dropna=False)\n",
    "             .agg(default_agg)\n",
    "             .rename(columns={\n",
    "                 \"wip\": \"wip_sum\",\n",
    "                 \"wip_load\": \"wip_load_sum\",\n",
    "                 \"backlog_available\": \"backlog_available_mean\",\n",
    "                 \"event_newcase\": \"event_newcase_sum\",\n",
    "                 \"event_legal\": \"event_legal_sum\",\n",
    "                 \"event_court\": \"event_court_sum\",\n",
    "                 \"event_pg_signoff\": \"event_pg_signoff_sum\",\n",
    "                 \"event_sent_to_ca\": \"event_sent_to_ca_sum\",\n",
    "                 \"event_flagged\": \"event_flagged_sum\",\n",
    "                 \"time_since_last_pickup\": \"time_since_last_pickup_median\",\n",
    "                 \"staff_id\": \"staff_count\",\n",
    "             })\n",
    "             .reset_index()\n",
    "    )\n",
    "\n",
    "    if freq is None:\n",
    "        # Return daily/grouped summary as-is\n",
    "        return grouped.sort_values(by).reset_index(drop=True)\n",
    "\n",
    "    # Resampling: we need a DatetimeIndex aligned on 'date'\n",
    "    out = []\n",
    "    other_keys = [k for k in by if k != \"date\"]\n",
    "    for keys, sub in grouped.groupby(other_keys, dropna=False):\n",
    "        # Ensure consistent frame and index\n",
    "        sub = sub.sort_values(\"date\").set_index(\"date\")\n",
    "\n",
    "        # For numeric fields, decide resampling rule:\n",
    "        # - For cumulative/state-like fields -> last\n",
    "        # - For flow-like fields (counts) -> sum\n",
    "        numeric_cols = sub.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "        # Prepare aggregation map for resample\n",
    "        resample_agg = {}\n",
    "        for col in numeric_cols:\n",
    "            if col in resample_cum_last:\n",
    "                resample_agg[col] = \"last\"\n",
    "            else:\n",
    "                resample_agg[col] = \"sum\"\n",
    "\n",
    "        sub_res = sub.resample(freq).agg(resample_agg)\n",
    "\n",
    "        # Keep grouping keys\n",
    "        if not isinstance(keys, tuple):\n",
    "            keys = (keys,)\n",
    "        for k, v in zip(other_keys, keys):\n",
    "            sub_res[k] = v\n",
    "\n",
    "        out.append(sub_res.reset_index())\n",
    "\n",
    "    resampled = pd.concat(out, ignore_index=True) if out else grouped\n",
    "    return resampled.sort_values(by if freq is None else ([\"date\"] + other_keys)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Team-level daily\n",
    "team_daily = summarise_daily_panel(daily, by=['date','team'])\n",
    "\n",
    "# 2) Team-level weekly (Friday), treating backlog as a level (last-of-week)\n",
    "team_weekly = summarise_daily_panel(\n",
    "    daily,\n",
    "    by=['date','team'],\n",
    "    freq='W-FRI',\n",
    "    resample_cum_last=('backlog_available',)  # keep as 'last' per week\n",
    ")\n",
    "\n",
    "# 3) Overall totals per day (collapse teams)\n",
    "org_daily = summarise_daily_panel(daily, by=['date'])\n",
    "\n",
    "# 4) Custom aggregation rules (e.g., use max backlog across staff instead of mean)\n",
    "custom = summarise_daily_panel(\n",
    "    daily,\n",
    "    by=['date','team'],\n",
    "    agg_map={'backlog_available': 'max'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- fabricate a tiny typed dataset ---\n",
    "typed = pd.DataFrame({\n",
    "    'case_id': ['C1','C2'],\n",
    "    'investigator': ['Alice','Bob'],\n",
    "    'team': ['T1','T1'],\n",
    "    'role': ['',''],\n",
    "    'fte': [1.0, 0.8],\n",
    "    'staff_id': ['S1','S2'],\n",
    "\n",
    "    # key dates\n",
    "    'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-02')],\n",
    "    'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-03')],\n",
    "    'dt_alloc_team': [pd.NaT, pd.NaT],\n",
    "    'dt_pg_signoff': [pd.NaT, pd.Timestamp('2025-01-08')],\n",
    "    'dt_close': [pd.Timestamp('2025-01-06'), pd.NaT],\n",
    "\n",
    "    # events\n",
    "    'dt_legal_req_1': [pd.NaT, pd.Timestamp('2025-01-04')],\n",
    "    'dt_legal_req_2': [pd.NaT, pd.NaT],\n",
    "    'dt_legal_req_3': [pd.NaT, pd.NaT],\n",
    "    'dt_legal_approval': [pd.NaT, pd.NaT],\n",
    "    'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    'dt_flagged': [pd.NaT, pd.NaT],\n",
    "})\n",
    "\n",
    "# --- run horizon, events, wip, and panel ---\n",
    "start, end = date_horizon(typed, pad_days=3)\n",
    "daily, backlog, events = build_daily_panel(typed, start, end)\n",
    "\n",
    "print(\"Start/End:\", start.date(), end.date())\n",
    "print(\"Daily shape:\", daily.shape)\n",
    "print(\"Backlog shape:\", backlog.shape)\n",
    "print(\"Events shape:\", events.shape)\n",
    "\n",
    "print(\"\\nDaily head:\\n\", daily.head())\n",
    "print(\"\\nBacklog tail:\\n\", backlog.tail())\n",
    "print(\"\\nEvents:\\n\", events.sort_values(['date','staff_id','event']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a reusable function to load raw data\n",
    "def load_raw(p: Path, force_encoding: str | None = None):\n",
    "    \"\"\"\n",
    "    Load CSV/XLSX with robust encoding handling.\n",
    "    - If force_encoding is given, use it.\n",
    "    - Otherwise try common encodings in order and fall back to a safe decode.\n",
    "    Returns: (df, colmap)\n",
    "    \"\"\"\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "# Conditional branch\n",
    "    if not p.exists():\n",
    "\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "# Excel files are not affected by CSV encoding issues\n",
    "# Conditional branch\n",
    "    if p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "# Load an Excel sheet into a DataFrame\n",
    "        df = pd.read_excel(p, dtype=str)\n",
    "# Fallback branch\n",
    "    else:\n",
    "        tried = []\n",
    "        encodings_to_try = (\n",
    "            [force_encoding] if force_encoding else\n",
    "            [\"utf-8-sig\", \"cp1252\", \"latin1\", \"iso-8859-1\", \"utf-16\", \"utf-16le\", \"utf-16be\"]\n",
    "        )\n",
    "\n",
    "        df = None\n",
    "        last_err = None\n",
    "# Loop over a sequence\n",
    "        for enc in encodings_to_try:\n",
    "# Try a block of code that may raise errors\n",
    "            try:\n",
    "# Load a CSV file into a DataFrame\n",
    "                df = pd.read_csv(p, dtype=str, sep=None, engine=\"python\",\n",
    "                                 encoding=enc, encoding_errors=\"strict\")\n",
    "                break\n",
    "# Handle errors from the try block\n",
    "            except UnicodeDecodeError as e:\n",
    "                tried.append(enc); last_err = e\n",
    "            except Exception as e:\n",
    "# Other parse errors (separator/quotes) â€“ keep trying other encodings\n",
    "                tried.append(enc); last_err = e\n",
    "\n",
    "# Last-resort: decode with cp1252 but *replace* bad bytes\n",
    "# Conditional branch\n",
    "        if df is None:\n",
    "# Try a block of code that may raise errors\n",
    "            try:\n",
    "# Load a CSV file into a DataFrame\n",
    "                df = pd.read_csv(p, dtype=str, sep=None, engine=\"python\",\n",
    "                                 encoding=\"cp1252\", encoding_errors=\"replace\")\n",
    "# Print a message or value\n",
    "                print(f\"[load_raw] WARNING: used cp1252 with replacement after failed encodings: {tried}\")\n",
    "# Handle errors from the try block\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to read CSV. Tried encodings {tried}. Last error: {last_err}\"\n",
    "                ) from e\n",
    "                \n",
    "# Trim whitespace across all string columns\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    colmap = {re.sub(r\"\\s+\", \" \", str(c).strip().lower()): c for c in df.columns}\n",
    "# Return a value from a function\n",
    "    return df, colmap\n",
    "\n",
    "\n",
    "# Define a reusable function\n",
    "def col(df, colmap, name):\n",
    "# Import libraries/modules for use below\n",
    "    import numpy as np\n",
    "    k=normalise_col(name)\n",
    "# Return a value from a function\n",
    "    if k in colmap: return df[colmap[k]]\n",
    "# Loop over a sequence\n",
    "    for kk,v in colmap.items():\n",
    "# Return a value from a function\n",
    "        if k in kk or kk in k: return df[v]\n",
    "# Use NumPy for numeric operations\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "# Define a reusable function\n",
    "def engineer(df, colmap):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Use pandas functionality to rename the most important column to the corresponding variables\n",
    "    out=pd.DataFrame({'case_id':col(df,colmap,'ID'),\n",
    "                      'investigator':col(df,colmap,'Investigator'),'team':col(df,colmap,'Team'),\n",
    "                      'fte':pd.to_numeric(col(df,colmap,'Investigator FTE'), errors='coerce')})\n",
    "    out['dt_received_inv']=parse_date_series(col(df,colmap,'Date Received in Investigations'))\n",
    "    out['dt_alloc_invest']=parse_date_series(col(df,colmap,'Date allocated to current investigator'))\n",
    "    out['dt_alloc_team']=parse_date_series(col(df,colmap,'Date allocated to team'))\n",
    "    out['dt_pg_signoff']=parse_date_series(col(df,colmap,'PG Sign off date'))\n",
    "    out['dt_close']=parse_date_series(col(df,colmap,'Closure Date'))\n",
    "    out['dt_legal_req_1']=parse_date_series(col(df,colmap,'Date of Legal Review Request 1'))\n",
    "    out['dt_legal_rej_1']=parse_date_series(col(df,colmap,'Date Legal Rejects 1'))\n",
    "    out['dt_legal_req_2']=parse_date_series(col(df,colmap,'Date of Legal Review Request 2'))\n",
    "    out['dt_legal_rej_2']=parse_date_series(col(df,colmap,'Date Legal Rejects 2'))\n",
    "    out['dt_legal_req_3']=parse_date_series(col(df,colmap,'Date of Legel Review Request 3'))\n",
    "    out['dt_legal_approval']=parse_date_series(col(df,colmap,'Legal Approval Date'))\n",
    "    out['dt_date_of_order']=parse_date_series(col(df,colmap,'Date Of Order'))\n",
    "    out['dt_flagged']=parse_date_series(col(df,colmap,'Flagged Date'))\n",
    "    out['fte']=out['fte'].fillna(1.0); out['staff_id']=out['investigator'].apply(hash_id); out['role']=''; return out\n",
    "\n",
    "    \n",
    "# Define a reusable function to define the horizon\n",
    "def date_horizon(typed, pad_days:int=14):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Use pandas functionality to contatinate three columns and find min as the satrt date\n",
    "    start=pd.concat([typed['dt_received_inv'],typed['dt_alloc_invest'],typed['dt_alloc_team']]).min()\n",
    "# Use pandas functionality to contatinate three columns and find max as the end date\n",
    "    end=pd.concat([typed['dt_close'],typed['dt_pg_signoff'],typed['dt_date_of_order']]).max()\n",
    "    if pd.isna(start): start=pd.Timestamp.today().normalize()-pd.Timedelta(days=30)\n",
    "    if pd.isna(end): end=pd.Timestamp.today().normalize()\n",
    "    end=end+pd.Timedelta(days=pad_days); return start.normalize(), end.normalize()\n",
    "\n",
    "    \n",
    "# Define a reusable function to build event log\n",
    "def build_event_log(typed):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "    rec=[]\n",
    "# Loop over a sequence\n",
    "    for _,r in typed.iterrows():\n",
    "        sid,team,fte,cid=r['staff_id'],r['team'],r['fte'],r['case_id']\n",
    "# Define a reusable function\n",
    "        def add(dt,etype):\n",
    "            if pd.isna(dt): return\n",
    "            rec.append({'date':dt.normalize(),'staff_id':sid,'team':team,'fte':fte,'case_id':cid,'event':etype,'meta':''})\n",
    "        add(r['dt_alloc_invest'],'newcase'); add(r['dt_legal_req_1'],'legal_request'); add(r['dt_legal_req_2'],'legal_request'); add(r['dt_legal_req_3'],'legal_request'); add(r['dt_legal_approval'],'legal_approval'); add(r['dt_date_of_order'],'court_order')\n",
    "# Use pandas functionality\n",
    "    ev=pd.DataFrame.from_records(rec)\n",
    "# Use pandas functionality\n",
    "    return ev if not ev.empty else pd.DataFrame(columns=['date','staff_id','team','fte','case_id','event','meta'])\n",
    "# Define a reusable function to build wip\n",
    "def build_wip_series(typed,start,end):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Fill missing values with a default\n",
    "    end_dt=typed['dt_close'].fillna(typed['dt_pg_signoff']).fillna(end)\n",
    "# Drop rows with missing values\n",
    "    intervals=pd.DataFrame({'staff_id':typed['staff_id'],'team':typed['team'],'start':typed['dt_alloc_invest'],'end':end_dt}).dropna()\n",
    "\n",
    "    deltas=[]\n",
    "# Loop over a sequence\n",
    "    for _,r in intervals.iterrows():\n",
    "        s=r['start'].normalize(); e=r['end'].normalize()\n",
    "        if s>end or e<start: continue\n",
    "        s=max(s,start); e=min(e,end)\n",
    "        deltas.append((r['staff_id'],r['team'],s,1)); deltas.append((r['staff_id'],r['team'],e+pd.Timedelta(days=1),-1))\n",
    "    if not deltas: return pd.DataFrame(columns=['date','staff_id','team','wip'])\n",
    "    deltas=pd.DataFrame(deltas, columns=['staff_id','team','date','delta'])\n",
    "    all_dates=pd.DataFrame({'date':pd.date_range(start,end,freq='D')})\n",
    "\n",
    "    rows=[]\n",
    "# Group rows and compute aggregations\n",
    "    for (sid,team),g in deltas.groupby(['staff_id','team']):\n",
    "# Group rows and compute aggregations\n",
    "        gg=g.groupby('date', as_index=False)['delta'].sum()\n",
    "# Fill missing values with a default\n",
    "        grid=all_dates.merge(gg,on='date', how='left').fillna({'delta':0})\n",
    "        grid['wip']=grid['delta'].cumsum(); grid['staff_id']=sid; grid['team']=team; rows.append(grid[['date','staff_id','team','wip']])\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['date','staff_id','team','wip'])\n",
    "\n",
    "    \n",
    "# Define a reusable function to build backlog series\n",
    "def build_backlog_series(typed,start,end):\n",
    "# Import libraries/modules for use below\n",
    "    import pandas as pd\n",
    "# Drop rows with missing values\n",
    "    accepted=typed[['dt_received_inv']].dropna().assign(date=lambda d:d['dt_received_inv'].dt.normalize())['date'].value_counts().sort_index()\n",
    "# Drop rows with missing values\n",
    "    allocated=typed[['dt_alloc_invest']].dropna().assign(date=lambda d:d['dt_alloc_invest'].dt.normalize())['date'].value_counts().sort_index()\n",
    "    idx=pd.date_range(start,end,freq='D'); acc=accepted.reindex(idx, fill_value=0).cumsum(); allo=allocated.reindex(idx, fill_value=0).cumsum()\n",
    "# Rename columns for clarity/consistency\n",
    "    backlog=(acc-allo).rename('backlog_available').to_frame(); backlog.index.name='date'; return backlog.reset_index()\n",
    "\n",
    "\n",
    "# Define a reusable function to build daily panel\n",
    "def build_daily_panel(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp):\n",
    "    \"\"\"Combine WIP, event log, and calendar features. Returns: (daily, backlog, events).\"\"\"\n",
    "    ev = build_event_log(typed)\n",
    "    wip = build_wip_series(typed, start, end)\n",
    "    backlog = build_backlog_series(typed, start, end)\n",
    "\n",
    "# Base grid: all staff x all dates\n",
    "    staff = typed[[\"staff_id\",\"team\",\"role\",\"fte\"]].drop_duplicates()\n",
    "    dates = pd.DataFrame({\"date\": pd.date_range(start, end, freq=\"D\")})\n",
    "# Combine tables by key columns\n",
    "    grid = dates.assign(key=1).merge(staff.assign(key=1), on=\"key\").drop(columns=\"key\")\n",
    "\n",
    "# Merge WIP\n",
    "# Fill missing values with a default\n",
    "    grid = grid.merge(wip, on=[\"date\",\"staff_id\",\"team\"], how=\"left\").fillna({\"wip\":0})\n",
    "\n",
    "# Event flags\n",
    "# Conditional branch\n",
    "    if not ev.empty:\n",
    "        ev_flags = (\n",
    "# Create or transform columns\n",
    "            ev.assign(flag=1)\n",
    "              .pivot_table(index=[\"date\",\"staff_id\"], columns=\"event\", values=\"flag\", aggfunc=\"max\")\n",
    "# Reset index to turn group keys into columns\n",
    "              .reset_index().rename_axis(None, axis=1)\n",
    "        )\n",
    "# Combine tables by key columns\n",
    "        grid = grid.merge(ev_flags, on=[\"date\",\"staff_id\"], how=\"left\")\n",
    "# Loop over a sequence\n",
    "    for c in [\"newcase\",\"legal_request\",\"legal_approval\",\"court_order\"]:\n",
    "# Conditional branch\n",
    "        if c not in grid:\n",
    "            grid[c] = 0\n",
    "# Fallback branch\n",
    "        else:\n",
    "# Cast column(s) to a specific dtype\n",
    "            grid[c] = grid[c].fillna(0).astype(int)\n",
    "\n",
    "# --- SAFE time_since_last_pickup (no index mismatch) ---\n",
    "# Sort rows by specified columns\n",
    "    grid = grid.sort_values([\"staff_id\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "    grp = grid.groupby(\"staff_id\", sort=False)\n",
    "    runs = grp[\"newcase\"].transform(lambda s: (s == 1).cumsum())\n",
    "# Group rows and compute aggregations\n",
    "    grid[\"time_since_last_pickup\"] = grid.groupby([grid[\"staff_id\"], runs]).cumcount()\n",
    "    mask_no_pickups = grp[\"newcase\"].transform(\"sum\") == 0\n",
    "# Select/assign rows/columns by label/position\n",
    "    grid.loc[mask_no_pickups, \"time_since_last_pickup\"] = 99\n",
    "\n",
    "# Calendar\n",
    "    grid[\"dow\"] = grid[\"date\"].dt.day_name().str[:3]\n",
    "    grid[\"season\"] = grid[\"date\"].dt.month.map(month_to_season)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"term_flag\"] = grid[\"date\"].dt.month.map(is_term_month).astype(int)\n",
    "    grid[\"bank_holiday\"] = 0\n",
    "\n",
    "# New starters\n",
    "    first_alloc = (\n",
    "# Drop rows with missing values\n",
    "        typed.dropna(subset=[\"dt_alloc_invest\"])\n",
    "# Group rows and compute aggregations\n",
    "             .groupby(\"staff_id\")[\"dt_alloc_invest\"].min()\n",
    "# Rename columns for clarity/consistency\n",
    "             .rename(\"first_alloc\")\n",
    "    )\n",
    "    \n",
    "# Combine tables by key columns\n",
    "    grid = grid.merge(first_alloc, on=\"staff_id\", how=\"left\")\n",
    "    grid[\"weeks_since_start\"] = (\n",
    "        (grid[\"date\"] - grid[\"first_alloc\"]).dt.days // 7\n",
    "# Cast column(s) to a specific dtype\n",
    "    ).fillna(0).clip(lower=0).astype(int)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"is_new_starter\"] = (grid[\"weeks_since_start\"] < 4).astype(int)\n",
    "\n",
    "# Default flags\n",
    "    grid[\"mentoring_flag\"] = 0\n",
    "    grid[\"trainee_flag\"] = 0\n",
    "\n",
    "# Backlog (same for all staff/day)\n",
    "# Fill missing values with a default\n",
    "    grid = grid.merge(backlog, on=\"date\", how=\"left\").fillna({\"backlog_available\":0})\n",
    "\n",
    "# Final columns\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"event_newcase\"] = grid[\"newcase\"].astype(int)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"event_legal\"]   = ((grid[\"legal_request\"] + grid[\"legal_approval\"]) > 0).astype(int)\n",
    "# Cast column(s) to a specific dtype\n",
    "    grid[\"event_court\"]   = grid[\"court_order\"].astype(int)\n",
    "    grid = grid.drop(columns=[\"newcase\",\"legal_request\",\"legal_approval\",\"court_order\",\"first_alloc\"])\n",
    "\n",
    "    cols = [\"date\",\"staff_id\",\"team\",\"role\",\"fte\",\n",
    "\n",
    "            \"is_new_starter\",\"weeks_since_start\",\n",
    "\n",
    "            \"wip\",\"time_since_last_pickup\",\n",
    "\n",
    "            \"mentoring_flag\",\"trainee_flag\",\n",
    "\n",
    "            \"backlog_available\",\"term_flag\",\"season\",\"dow\",\"bank_holiday\",\n",
    "\n",
    "            \"event_newcase\",\"event_legal\",\"event_court\"]\n",
    "# Reset index to turn group keys into columns\n",
    "    daily = grid[cols].sort_values([\"staff_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# <-- IMPORTANT: return the frames\n",
    "# Return a value from a function\n",
    "    return daily, backlog, ev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### data loading, exporting outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file into a DataFrame\n",
    "df_test = pd.read_csv(RAW_PATH, dtype=str, sep=None, engine=\"python\", encoding=\"cp1252\")\n",
    "\n",
    "df_test.head()\n",
    "\n",
    "df_raw, colmap = load_raw(RAW_PATH)\n",
    "# print(f\"df_raw: \", df_raw)\n",
    "# print(f\"colmap: \", df_raw)\n",
    "\n",
    "typed = engineer(df_raw, colmap)\n",
    "# Print a message or value\n",
    "print(\"typed: \", typed)\n",
    "\n",
    "start, end = date_horizon(typed, 14)\n",
    "# Print a message or value\n",
    "print(\"start: \", start)\n",
    "# Print a message or value\n",
    "print(\"end: \", end)\n",
    "\n",
    "daily, backlog, events = build_daily_panel(typed, start, end)\n",
    "\n",
    "# (optional) save to disk\n",
    "# Save a DataFrame to CSV\n",
    "daily.to_csv(OUT_DIR / \"investigator_daily.csv\", index=False)\n",
    "# Save a DataFrame to CSV\n",
    "backlog.to_csv(OUT_DIR / \"backlog_series.csv\", index=False)\n",
    "# Save a DataFrame to CSV\n",
    "events.to_csv(OUT_DIR / \"event_log.csv\", index=False)\n",
    "\n",
    "# # Print a message or value\n",
    "# print(f\"{len(daily):,} daily rows\")\n",
    "# # Print a message or value\n",
    "# print(\"Date range:\", daily[\"date\"].min().date(), \"â†’\", daily[\"date\"].max().date())\n",
    "# # Print a message or value\n",
    "# print(\"Investigators:\", daily[\"staff_id\"].nunique())\n",
    "# # Print a message or value\n",
    "# print(\"Total new case events:\", int(daily[\"event_newcase\"].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## > # === Stage 2 extension: historical \"investigated so far\" + 90-day daily predictions\n",
    "> # Assumes Stage-2 output exists at data/out/investigator_daily.csv\n",
    "> # \"Investigated\" here = daily pickups (event_newcase). Swap to a different event if needed.\n",
    "> \n",
    "> - This model builds the historical time series of how many cases were investigated (interpreted as new case pickups = event_newcase) per investigator, role, and team, including the cumulative (â€œso farâ€) curves;\n",
    "> \n",
    "> - fits simple Gammaâ€“Poisson posteriors and produces 90-day daily predictions (mean and 5â€“95% credible interval) for each investigator/role/team.\n",
    "> \n",
    "> - It saves six CSVs into data/out/ so we can join/plot later.\n",
    "> \n",
    "> **For â€œinvestigatedâ€ = completed rather than picked up, just change the column used from event_newcase to the right completion flag (e.g., if you track completions per day, swap it in where noted).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### imports and environment setup, data loading, joining/merging datasets, aggregation/grouping, data cleaning, sorting, feature engineering, exporting outputs, prediction/forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Stage 2 extension: historical \"investigated so far\" + 90-day daily predictions\n",
    "\n",
    "# Assumes Stage-2 output exists at data/out/investigator_daily.csv\n",
    "\n",
    "# \"Investigated\" here = daily pickups (event_newcase). Swap to a different event if needed.\n",
    "\n",
    "\n",
    "from scipy.stats import nbinom  # Negative Binomial for Gammaâ€“Poisson posterior predictive\n",
    "\n",
    "\n",
    "OUT = Path(\"data/out\")\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_path = OUT / \"investigator_daily.csv\"\n",
    "\n",
    "# ---------- Load ----------\n",
    "# Load a CSV file into a DataFrame\n",
    "daily = pd.read_csv(daily_path, parse_dates=[\"date\"])\n",
    "\n",
    "# If you want \"investigated\" to mean something else, swap this column:\n",
    "\n",
    "target_col = \"event_newcase\"   # <--- change if needed (e.g., 'event_court' or a completion flag)\n",
    "\n",
    "# Cast column(s) to a specific dtype\n",
    "daily[target_col] = pd.to_numeric(daily[target_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "# Fill missing values with a default\n",
    "daily[\"team\"] = daily.get(\"team\", pd.Series(index=daily.index)).fillna(\"Unknown\")\n",
    "# Fill missing values with a default\n",
    "daily[\"role\"] = daily.get(\"role\", pd.Series(index=daily.index)).fillna(\"Unknown\")\n",
    "\n",
    "last_date = daily[\"date\"].max()\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "# 1) HISTORICAL: daily counts + cumulative (\"so far\") per entity\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "# Investigator\n",
    "# Group rows and compute aggregations\n",
    "hist_inv = (daily.groupby([\"date\",\"staff_id\",\"team\",\"role\"], as_index=False)[target_col]\n",
    "\n",
    "                 .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                 .rename(columns={target_col:\"daily_pickups\"}))\n",
    "# Sort rows by specified columns\n",
    "hist_inv = hist_inv.sort_values([\"staff_id\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "hist_inv[\"cum_pickups\"] = hist_inv.groupby(\"staff_id\")[\"daily_pickups\"].cumsum()\n",
    "# Save a DataFrame to CSV\n",
    "hist_inv.to_csv(OUT / \"hist_pickups_investigator.csv\", index=False)\n",
    "\n",
    "\n",
    "# Role\n",
    "# Group rows and compute aggregations\n",
    "hist_role = (daily.groupby([\"date\",\"role\"], as_index=False)[target_col]\n",
    "\n",
    "                  .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                  .rename(columns={target_col:\"daily_pickups\"}))\n",
    "# Sort rows by specified columns\n",
    "hist_role = hist_role.sort_values([\"role\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "hist_role[\"cum_pickups\"] = hist_role.groupby(\"role\")[\"daily_pickups\"].cumsum()\n",
    "# Save a DataFrame to CSV\n",
    "hist_role.to_csv(OUT / \"hist_pickups_role.csv\", index=False)\n",
    "\n",
    "\n",
    "# Team\n",
    "# Group rows and compute aggregations\n",
    "hist_team = (daily.groupby([\"date\",\"team\"], as_index=False)[target_col]\n",
    "\n",
    "                  .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                  .rename(columns={target_col:\"daily_pickups\"}))\n",
    "# Sort rows by specified columns\n",
    "hist_team = hist_team.sort_values([\"team\",\"date\"])\n",
    "# Group rows and compute aggregations\n",
    "hist_team[\"cum_pickups\"] = hist_team.groupby(\"team\")[\"daily_pickups\"].cumsum()\n",
    "# Save a DataFrame to CSV\n",
    "hist_team.to_csv(OUT / \"hist_pickups_team.csv\", index=False)\n",
    "\n",
    "# =====================================================================\n",
    "\n",
    "# 2) PREDICTIONS: 90-day daily counts per entity (Gammaâ€“Poisson)\n",
    "#    Posterior (rate-param Gamma prior Î±0=1, Î²0=1):\n",
    "#      For a single day ahead, y ~ NegBinom(r=Î±_post, p=Î²_post/(Î²_post+1)),\n",
    "#      E[y] = Î±_post / Î²_post, with 5â€“95% credible interval from NB quantiles.\n",
    "# =====================================================================\n",
    "\n",
    "# Define a reusable function\n",
    "def posterior_by_key(daily_df: pd.DataFrame, key_cols: list[str]) -> pd.DataFrame:\n",
    "    # Aggregate to per-day counts for the entity\n",
    "# Group rows and compute aggregations\n",
    "    g_daily = (daily_df.groupby(key_cols + [\"date\"], as_index=False)[target_col]\n",
    "\n",
    "                      .sum()\n",
    "# Rename columns for clarity/consistency\n",
    "                      .rename(columns={target_col:\"y\"}))\n",
    "    # Total counts and exposure days (T = # unique dates observed for that entity)\n",
    "# Group rows and compute aggregations\n",
    "    g_total = (g_daily.groupby(key_cols, as_index=False)\n",
    "# Apply aggregation(s) to grouped data\n",
    "                      .agg(y_total=(\"y\",\"sum\"),\n",
    "\n",
    "                           T=(\"date\",\"nunique\")))\n",
    "\n",
    "    # Weak prior\n",
    "    alpha0, beta0 = 1.0, 1.0\n",
    "\n",
    "    g_total[\"alpha_post\"] = alpha0 + g_total[\"y_total\"]\n",
    "\n",
    "    g_total[\"beta_post\"]  = beta0 + g_total[\"T\"]\n",
    "\n",
    "    # Negative Binomial params for 1-day-ahead predictive:\n",
    "    # In scipy: nbinom(n=r, p) has mean = r*(1-p)/p. Choose p = Î²/(Î²+1) â†’ mean = Î±/Î²\n",
    "\n",
    "    g_total[\"p_nb\"] = g_total[\"beta_post\"] / (g_total[\"beta_post\"] + 1.0)\n",
    "\n",
    "    g_total[\"r_nb\"] = g_total[\"alpha_post\"]\n",
    "\n",
    "    # Daily expected value and 90% credible interval\n",
    "\n",
    "    g_total[\"mean\"] = g_total[\"r_nb\"] * (1 - g_total[\"p_nb\"]) / g_total[\"p_nb\"]\n",
    "\n",
    "    g_total[\"p05\"]  = nbinom.ppf(0.05, n=g_total[\"r_nb\"], p=g_total[\"p_nb\"])\n",
    "\n",
    "    g_total[\"p95\"]  = nbinom.ppf(0.95, n=g_total[\"r_nb\"], p=g_total[\"p_nb\"])\n",
    "# Return a value from a function\n",
    "    return g_total[key_cols + [\"mean\",\"p05\",\"p95\"]]\n",
    "\n",
    "\n",
    "H = 90\n",
    "# Use pandas functionality\n",
    "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=H, freq=\"D\")\n",
    "\n",
    "# Investigator predictions\n",
    "post_inv = posterior_by_key(daily, [\"staff_id\"])\n",
    "\n",
    "# Join convenient labels (first observed team/role for each staff)\n",
    "\n",
    "first_map = daily[[\"staff_id\",\"team\",\"role\"]].drop_duplicates(\"staff_id\")\n",
    "# Combine tables by key columns\n",
    "post_inv = post_inv.merge(first_map, on=\"staff_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Create or transform columns\n",
    "f_inv = (post_inv.assign(key=1)\n",
    "# Combine tables by key columns\n",
    "                .merge(pd.DataFrame({\"date\":future_dates, \"key\":1}), on=\"key\")\n",
    "\n",
    "                .drop(columns=\"key\"))[[\"date\",\"staff_id\",\"team\",\"role\",\"mean\",\"p05\",\"p95\"]]\n",
    "# Save a DataFrame to CSV\n",
    "f_inv.to_csv(OUT / \"forecast_pickups_investigator.csv\", index=False)\n",
    "\n",
    "\n",
    "# Role predictions\n",
    "post_role = posterior_by_key(daily, [\"role\"])\n",
    "# Create or transform columns\n",
    "f_role = (post_role.assign(key=1)\n",
    "# Combine tables by key columns\n",
    "          .merge(pd.DataFrame({\"date\":future_dates, \"key\":1}), on=\"key\")\n",
    "\n",
    "          .drop(columns=\"key\"))[[\"date\",\"role\",\"mean\",\"p05\",\"p95\"]]\n",
    "# Save a DataFrame to CSV\n",
    "f_role.to_csv(OUT / \"forecast_pickups_role.csv\", index=False)\n",
    "\n",
    "\n",
    "# Team predictions\n",
    "post_team = posterior_by_key(daily, [\"team\"])\n",
    "# Create or transform columns\n",
    "f_team = (post_team.assign(key=1)\n",
    "# Combine tables by key columns\n",
    "          .merge(pd.DataFrame({\"date\":future_dates, \"key\":1}), on=\"key\")\n",
    "\n",
    "          .drop(columns=\"key\"))[[\"date\",\"team\",\"mean\",\"p05\",\"p95\"]]\n",
    "# Save a DataFrame to CSV\n",
    "f_team.to_csv(OUT / \"forecast_pickups_team.csv\", index=False)\n",
    "\n",
    "# Print\n",
    "print(\"Saved:\\n -\", OUT / \"hist_pickups_investigator.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"hist_pickups_role.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"hist_pickups_team.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"forecast_pickups_investigator.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"forecast_pickups_role.csv\",\n",
    "\n",
    "      \"\\n -\", OUT / \"forecast_pickups_team.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "\n",
    "> # Bayesian Stage-3 implementation \n",
    "> works directly off the outputs already generated (data/out/investigator_daily.csv and data/out/backlog_series.csv). \n",
    "> It does two things:\n",
    "> 1. Backlog forecasting (next 90 days) via a conjugate Bayesian linear model on daily backlog deltas with an AR(1) feature + weekday + annual seasonality â€” returns full predictive uncertainty.\n",
    "> \n",
    "> 2. Per-investigator pickup rates via Gammaâ€“Poisson posteriors (hierarchical-by-investigator baseline), including 7-day and 28-day expected pickups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "> **Original note:**\n",
    ">\n",
    "> # === Stage 3: Bayesian predictive models (Backlog + Investigator pickups) ===\n",
    "> \n",
    "> The backlog model This model is a conjugate Bayesian regression on daily change, with:\n",
    ">     1. AR(1) feature via yesterdayâ€™s delta,\n",
    ">     2. Weekday effects,\n",
    ">     3. Annual seasonality (sin/cos).\n",
    "> \n",
    "> It produces proper predictive uncertainty and naturally handles short histories (weak priors).\n",
    "> \n",
    "> If you want team-level or role-level pickup posteriors, copy the per-investigator block and replace groupby(\"staff_id\") with groupby(\"team\") or any other cohort, with the same Gammaâ€“Poisson math.\n",
    "> \n",
    "> If you have bank holiday flags in your daily panel, you can add them as another regressor column in X (just remember to include it consistently when building make_x_row for the forecast).\n",
    "> \n",
    "> ## This model generate the following outputs\n",
    "> 1. Backlog forecaster â€” predicts how many cases will be waiting on each of the next 90 days.\n",
    "> 2. Pickup-rate estimator â€” estimates how often each investigator typically picks up a new case, and how many theyâ€™re likely to pick up over the next 1â€“4 weeks.\n",
    "> \n",
    "> ## Inputs\n",
    "> 1. The daily backlog totals from Stage-2 (backlog_series.csv).\n",
    "> 2. The per-investigator daily table from Stage-2 (investigator_daily.csv) which includes the daily â€œnew case picked upâ€ flag.\n",
    "> \n",
    "> \n",
    "> ## 1) Backlog forecaster (daily totals)\n",
    "> Instead of predicting the raw backlog level directly, the code predicts the daily change in backlog (todayâ€™s backlog minus yesterdayâ€™s).\n",
    "> That change tends to be:\n",
    "> - a bit like yesterdayâ€™s change (momentum),\n",
    "> - slightly different on different weekdays (e.g., Mondays vs Fridays),\n",
    "> - and nudged by time-of-year patterns (seasonality).\n",
    "> \n",
    "> ### Bayesian\n",
    "> - We start with very weak, generic expectations (â€œpriorsâ€), look at the data, and update our beliefs to a â€œposterior.â€ \n",
    "> - Then we simulate many possible futures consistent with what we learned. This gives not just a single forecast, but a spread (best guess + uncertainty bands).\n",
    "> \n",
    "> ### What the code actually does\n",
    "> - Builds a simple recipe for daily change: **todayâ€™s change â‰ˆ intercept + (yesterdayâ€™s change) + weekday effect + seasonal wiggle + random noise**\n",
    "> \n",
    "> - Fits that recipe with a Bayes method thatâ€™s efficient (a conjugate prior). This gives us a clean way to learn from the data and quantify uncertainty.\n",
    "> \n",
    "> - Simulates thousands of future paths day-by-day: each new dayâ€™s change depends on the previous simulated dayâ€™s change (so it keeps momentum).\n",
    "> \n",
    "> - Converts those simulated changes back into backlog levels, and clips at zero (no negative backlog).\n",
    "> \n",
    "> - Summarises the simulations for each future date as:\n",
    ">     - mean/median (central forecasts),\n",
    ">     - p05/p95 (a 90% â€œcredible intervalâ€),\n",
    ">     - p20/p80 (a tighter middle band).\n",
    "> Example: **If p05 = 120 and p95 = 180 on a date, the model is saying â€œgiven history and patterns, thereâ€™s about a 90% chance backlog will be between 120 and 180 that day.â€**\n",
    "> \n",
    "> \n",
    "> ## 2) Investigator pickup rates (how often investigator take new cases)\n",
    "> Each investigatorâ€™s daily pickup count is treated as a â€œcounting processâ€ (like number of arrivals per day). \n",
    "> Some people pick up more, some less, and some have sparse histories. We want fair estimates that stabilise when data is thin.\n",
    "> \n",
    "> ### Gammaâ€“Poisson\n",
    "> - We assume daily pickups follow a Poisson process (a common, simple model for counts).\n",
    "> - We put a Gamma prior on each personâ€™s true underlying daily rate (how often they pick up).\n",
    "> - Combining those gives a neat closed-form update (no heavy computation): you get a posterior for each personâ€™s rate that blends their data with a little stabilising prior.\n",
    "> \n",
    "> ### What the code outputs\n",
    "> For each investigator:\n",
    "> 1. A posterior mean daily pickup rate (our best estimate),\n",
    "> 2. A credible range (p05â€“p95) to show uncertainty,\n",
    "> 3. Expected pickups over the next 7 and 28 days (rate Ã— days).\n",
    "> \n",
    "> Example of useability: \n",
    "> **Rank investigators by posterior mean (or lower-bound like p05 for conservative planning) to understand expected intake capacity in the short term.**\n",
    "> \n",
    "> Outputs:\n",
    "> 1. backlog_forecast_bayes.csv: one row per future day with mean, median, p05, p20, p80, p95.\n",
    "> 2. investigator_pickup_posterior.csv: one row per investigator with posterior rate and 7-/28-day expectations.\n",
    "> \n",
    "> ## Assumptions (and what to tweak)\n",
    "> 1. Momentum matters: tomorrowâ€™s change tends to resemble yesterdayâ€™s.\n",
    "> 2. Weekdays differ: e.g., fewer allocations on weekends.\n",
    "> 3. Seasonality: simple annual pattern (sine/cosine); can add school terms or fiscal periods.\n",
    "> 4. Counts are Poisson: good first pass; if pickups bunch up or are capped, consider a richer model later.\n",
    "> 5. Data quality: the forecast inherits any biases or gaps; adding bank holidays (already optional in your notebook) helps.\n",
    "> \n",
    "> ## Future developement\n",
    "> 1. Add bank holiday and term time flags as extra predictors in the backlog model.\n",
    "> 2. Estimate pickup rates by team/role (swap the group-by key).\n",
    "> 3. Move to a hierarchical pickup model (shares strength across investigators/teams) if data per person is very sparse.\n",
    "> \n",
    "> ### Why this is useful\n",
    "> 1. We can get actionable ranges, not just a single numberâ€”great for planning under uncertainty.\n",
    "> 2. The pickup posteriors turn noisy daily events into a stable, comparable measure of capacity.\n",
    "> 3. Itâ€™s all fast and transparent, so you can iterate quickly as new data arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### imports and environment setup, data loading, aggregation/grouping, data cleaning, sorting, exporting outputs, prediction/forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries/modules for use below\n",
    "from pathlib import Path\n",
    "# Import libraries/modules for use below\n",
    "from scipy.stats import invgamma, gamma as gamma_dist\n",
    "\n",
    "# ---- Locations ----\n",
    "BASE = Path(\"data\")\n",
    "\n",
    "OUT  = BASE / \"out\"\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "daily_path   = OUT / \"investigator_daily.csv\"\n",
    "\n",
    "backlog_path = OUT / \"backlog_series.csv\"\n",
    "\n",
    "# ---- Load outputs from Stage-2 ----\n",
    "# Load a CSV file into a DataFrame\n",
    "daily   = pd.read_csv(daily_path, parse_dates=[\"date\"])\n",
    "# Load a CSV file into a DataFrame\n",
    "backlog = pd.read_csv(backlog_path, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---- Build daily delta series for backlog ----\n",
    "\n",
    "backlog[\"delta\"] = backlog[\"backlog_available\"].diff()\n",
    "# Drop rows with missing values\n",
    "backlog = backlog.dropna(subset=[\"delta\"]).reset_index(drop=True)\n",
    "\n",
    "# Design matrix for a conjugate Bayesian linear model:\n",
    "\n",
    "# y_t = delta_t ~ N(X_t beta, sigma^2), with X_t = [1, lag_delta, sin, cos, DOW dummies]\n",
    "\n",
    "df = backlog.copy()\n",
    "\n",
    "df[\"lag_delta\"] = df[\"delta\"].shift(1)\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=[\"lag_delta\"]).reset_index(drop=True)\n",
    "\n",
    "# Weekday effects (Mon=0..Sun=6), drop_first to avoid dummy trap\n",
    "\n",
    "df[\"dow\"] = df[\"date\"].dt.dayofweek\n",
    "# Use pandas functionality\n",
    "dow_dummies = pd.get_dummies(df[\"dow\"], prefix=\"dow\", drop_first=True)\n",
    "\n",
    "# Annual seasonality with sin/cos (period ~ 365.25)\n",
    "# Cast column(s) to a specific dtype\n",
    "day_of_year = df[\"date\"].dt.dayofyear.astype(float)\n",
    "# Use NumPy for numeric operations\n",
    "df[\"sin_annual\"] = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "# Use NumPy for numeric operations\n",
    "df[\"cos_annual\"] = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "\n",
    "# Use pandas functionality\n",
    "X = pd.concat([\n",
    "    pd.Series(1.0, index=df.index, name=\"intercept\"),\n",
    "\n",
    "    df[[\"lag_delta\", \"sin_annual\", \"cos_annual\"]],\n",
    "\n",
    "    dow_dummies\n",
    "\n",
    "], axis=1)\n",
    "\n",
    "y = df[\"delta\"].to_numpy(float)\n",
    "\n",
    "X_mat = X.to_numpy(float)\n",
    "\n",
    "# ---- Conjugate Normalâ€“Inverse-Gamma posterior ----\n",
    "\n",
    "# Prior: beta|sigma^2 ~ N(m0, sigma^2 V0),  sigma^2 ~ InvGamma(a0, b0)\n",
    "n, p = X_mat.shape\n",
    "# Use NumPy for numeric operations\n",
    "m0   = np.zeros(p)\n",
    "V0   = np.eye(p) * 1e6         # weakly-informative\n",
    "\n",
    "a0   = 2.0\n",
    "# Use NumPy for numeric operations\n",
    "yvar = float(np.var(y)) if np.isfinite(np.var(y)) and np.var(y) > 0 else 1.0\n",
    "b0   = yvar * (a0 - 1)\n",
    "\n",
    "\n",
    "XtX    = X_mat.T @ X_mat\n",
    "V0inv  = np.linalg.inv(V0)\n",
    "Vn     = np.linalg.inv(XtX + V0inv)\n",
    "mn     = Vn @ (V0inv @ m0 + X_mat.T @ y)\n",
    "an     = a0 + n/2.0\n",
    "bn     = b0 + 0.5*(y @ y + m0 @ V0inv @ m0 - mn @ np.linalg.inv(Vn) @ mn)\n",
    "\n",
    "# ---- Posterior predictive: forward simulate next H days with AR(1) lag ----\n",
    "\n",
    "H = 90         # forecast horizon (days)\n",
    "S = 4000       # posterior draws\n",
    "\n",
    "# Select/assign rows/columns by label/position\n",
    "last_delta   = float(df.iloc[-1][\"delta\"])\n",
    "# Select/assign rows/columns by label/position\n",
    "last_backlog = float(backlog.iloc[-1][\"backlog_available\"])\n",
    "# Select/assign rows/columns by label/position\n",
    "last_date    = df.iloc[-1][\"date\"]\n",
    "\n",
    "# Use pandas functionality\n",
    "future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=H, freq=\"D\")\n",
    "future_dow   = future_dates.dayofweek\n",
    "# Use NumPy for numeric operations\n",
    "future_sin   = np.sin(2 * np.pi * future_dates.dayofyear / 365.25)\n",
    "# Use NumPy for numeric operations\n",
    "future_cos   = np.cos(2 * np.pi * future_dates.dayofyear / 365.25)\n",
    "dow_cols     = [c for c in X.columns if c.startswith(\"dow_\")]\n",
    "\n",
    "# Define a reusable function\n",
    "def make_x_row(lag_delta_val, idx):\n",
    "\n",
    "    # Build X* in the same column order as training X\n",
    "    dow = int(future_dow[idx])\n",
    "# Use NumPy for numeric operations\n",
    "    dd = np.zeros(len(dow_cols))\n",
    "# Loop over a sequence\n",
    "    for j, c in enumerate(dow_cols):\n",
    "# Try a block of code that may raise errors\n",
    "        try:\n",
    "            target = int(c.split(\"_\")[1])  # 'dow_3' -> 3\n",
    "# Handle errors from the try block\n",
    "        except Exception:\n",
    "            target = None\n",
    "\n",
    "        dd[j] = 1.0 if (target is not None and dow == target) else 0.0\n",
    "# Use NumPy for numeric operations\n",
    "    return np.concatenate(([1.0, lag_delta_val, future_sin[idx], future_cos[idx]], dd))\n",
    "\n",
    "# Use NumPy for numeric operations\n",
    "rng = np.random.default_rng(2025)\n",
    "\n",
    "# Robust Cholesky (add tiny jitter if near-singular)\n",
    "# Use NumPy for numeric operations\n",
    "evals = np.linalg.eigvals(Vn)\n",
    "# Conditional branch\n",
    "if np.min(np.real(evals)) < 1e-12:\n",
    "# Use NumPy for numeric operations\n",
    "    Vn = Vn + np.eye(p) * 1e-10\n",
    "# Use NumPy for numeric operations\n",
    "L = np.linalg.cholesky(Vn)\n",
    "\n",
    "# Sample (sigma^2, beta) from posterior\n",
    "\n",
    "sigma2 = invgamma.rvs(a=an, scale=bn, size=S, random_state=rng)\n",
    "\n",
    "z      = rng.standard_normal((S, p))\n",
    "# Use NumPy for numeric operations\n",
    "beta   = mn + np.sqrt(sigma2)[:, None] * (z @ L.T)\n",
    "\n",
    "# Simulate daily deltas forward with AR lag in X\n",
    "# Use NumPy for numeric operations\n",
    "delta_draws = np.zeros((S, H))\n",
    "# Loop over a sequence\n",
    "for s in range(S):\n",
    "\n",
    "    lag = last_delta\n",
    "# Use NumPy for numeric operations\n",
    "    bs  = beta[s]; sig = np.sqrt(sigma2[s])\n",
    "# Loop over a sequence\n",
    "    for h in range(H):\n",
    "\n",
    "        xh = make_x_row(lag, h)\n",
    "\n",
    "        mean_h = float(xh @ bs)\n",
    "\n",
    "        delta_h = mean_h + rng.normal(0.0, sig)\n",
    "\n",
    "        delta_draws[s, h] = delta_h\n",
    "\n",
    "        lag = delta_h\n",
    "\n",
    "# Transform to backlog levels; clip at zero\n",
    "# Use NumPy for numeric operations\n",
    "backlog_paths = last_backlog + np.cumsum(delta_draws, axis=1)\n",
    "# Use NumPy for numeric operations\n",
    "backlog_paths = np.clip(backlog_paths, 0, None)\n",
    "\n",
    "# Summaries\n",
    "\n",
    "q = [0.05, 0.2, 0.5, 0.8, 0.95]\n",
    "# Use NumPy for numeric operations\n",
    "Q = np.quantile(backlog_paths, q, axis=0).T\n",
    "# Use pandas functionality\n",
    "forecast_df = pd.DataFrame({\n",
    "\n",
    "    \"date\":  future_dates,\n",
    "\n",
    "    \"mean\":  backlog_paths.mean(axis=0),\n",
    "\n",
    "    \"median\":Q[:, 2],\n",
    "\n",
    "    \"p05\":   Q[:, 0],\n",
    "\n",
    "    \"p20\":   Q[:, 1],\n",
    "\n",
    "    \"p80\":   Q[:, 3],\n",
    "\n",
    "    \"p95\":   Q[:, 4],\n",
    "\n",
    "})\n",
    "# Save a DataFrame to CSV\n",
    "forecast_df.to_csv(OUT / \"backlog_forecast_bayes.csv\", index=False)\n",
    "\n",
    "# ---- Per-investigator pickup rates: Gammaâ€“Poisson posteriors ----\n",
    "\n",
    "# For each staff_id, y_i ~ Poisson(theta_i * T_i) with daily exposure T_i (days).\n",
    "\n",
    "# Prior theta_i ~ Gamma(alpha0, beta0) (rate parameterization) => posterior Gamma(alpha0 + y, beta0 + T).\n",
    "\n",
    "di = daily.copy()\n",
    "# Cast column(s) to a specific dtype\n",
    "di[\"event_newcase\"] = pd.to_numeric(di[\"event_newcase\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Group rows and compute aggregations\n",
    "per_staff = (di.groupby(\"staff_id\", as_index=False)\n",
    "# Apply aggregation(s) to grouped data\n",
    "               .agg(y_total=(\"event_newcase\",\"sum\"),\n",
    "\n",
    "                    days=(\"date\",\"nunique\")))\n",
    "\n",
    "\n",
    "alpha0, beta0 = 1.0, 1.0\n",
    "\n",
    "per_staff[\"alpha_post\"] = alpha0 + per_staff[\"y_total\"]\n",
    "\n",
    "per_staff[\"beta_post\"]  = beta0 + per_staff[\"days\"]\n",
    "\n",
    "# Posterior summaries for daily rate theta_i\n",
    "\n",
    "per_staff[\"rate_mean\"]   = per_staff[\"alpha_post\"] / per_staff[\"beta_post\"]\n",
    "\n",
    "per_staff[\"rate_median\"] = gamma_dist.ppf(0.5, a=per_staff[\"alpha_post\"], scale=1.0/per_staff[\"beta_post\"])\n",
    "\n",
    "per_staff[\"rate_p05\"]    = gamma_dist.ppf(0.05, a=per_staff[\"alpha_post\"], scale=1.0/per_staff[\"beta_post\"])\n",
    "\n",
    "per_staff[\"rate_p95\"]    = gamma_dist.ppf(0.95, a=per_staff[\"alpha_post\"], scale=1.0/per_staff[\"beta_post\"])\n",
    "\n",
    "# Expected pickups in next horizons\n",
    "\n",
    "per_staff[\"exp_7d_mean\"]  = per_staff[\"rate_mean\"] * 7.0\n",
    "\n",
    "per_staff[\"exp_28d_mean\"] = per_staff[\"rate_mean\"] * 28.0\n",
    "\n",
    "# Save a DataFrame to CSV\n",
    "per_staff.to_csv(OUT / \"investigator_pickup_posterior.csv\", index=False)\n",
    "\n",
    "# Print a message or value\n",
    "print(\"Done.\")\n",
    "# Print a message or value\n",
    "print(\"Saved:\", OUT / \"backlog_forecast_bayes.csv\")\n",
    "# Print a message or value\n",
    "print(\"Saved:\", OUT / \"investigator_pickup_posterior.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Code cell purpose: general processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Stage 3 â€” Bayesian Forecasting (Ready-to-Run)\n",
    "**Added:** 2025-10-27 10:45:02Z (UTC)\n",
    "\n",
    "This section fits a **hierarchical Bayesian model** (PyMC) to predict **daily investigated cases** for the next **90 days** for each **investigator, role, and team**.\n",
    "It is designed to work with the daily dataset built earlier in this notebook. If the dataset is not found in memory, you can point the loader to a CSV.\n",
    "\n",
    "**What you'll get:**\n",
    "- Posterior predictive draws and summary statistics per investigator Ã— team Ã— role Ã— day (next 90 days).\n",
    "- Aggregations to team-level, role-level, and org-level totals.\n",
    "- A quick plot of the org-level total forecast.\n",
    "\n",
    "> Tip: First run the data build section above so the in-memory DataFrame is available for immediate modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if needed (uncomment if missing).\n",
    "# %pip install pymc bambi arviz holidays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Configuration & Dataset Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Config ----\n",
    "INPUT_CSV = '/mnt/data/investigator_daily.csv'  # Set to a file if you prefer to load from disk\n",
    "COUNT_COL_CANDIDATES = ['cases_investigated','investigated','num_investigated','completed_cases','cases_completed']\n",
    "DATE_COL_CANDIDATES = ['date','activity_date','day']\n",
    "INVESTIGATOR_COL_KEYS = ['investigator','assignee','user']\n",
    "TEAM_COL_KEYS = ['team','squad']\n",
    "ROLE_COL_KEYS = ['role','grade']\n",
    "MAX_TRAIN_ROWS = None  # e.g., 250_000 to subsample for faster initial runs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_df_in_globals():\n",
    "    \"\"\"Heuristically find a pandas DataFrame with the expected columns in the current namespace.\"\"\"\n",
    "    candidates = []\n",
    "    for name, obj in globals().items():\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            cols_lower = [c.lower() for c in obj.columns]\n",
    "            has_date = any(c in cols_lower for c in DATE_COL_CANDIDATES)\n",
    "            has_count = any(c in cols_lower for c in COUNT_COL_CANDIDATES)\n",
    "            has_inv = any(any(k in c for k in INVESTIGATOR_COL_KEYS) for c in cols_lower)\n",
    "            has_team = any(any(k in c for k in TEAM_COL_KEYS) for c in cols_lower)\n",
    "            has_role = any(any(k in c for k in ROLE_COL_KEYS) for c in cols_lower)\n",
    "            if has_date and has_count and has_inv and has_team and has_role:\n",
    "                candidates.append((name, obj))\n",
    "    return candidates[0][1] if candidates else None\n",
    "\n",
    "def _standardise_columns(df):\n",
    "    df = df.copy()\n",
    "    lower_map = {c: c.lower() for c in df.columns}\n",
    "    df.rename(columns=lower_map, inplace=True)\n",
    "    # Date column\n",
    "    date_col = next((c for c in DATE_COL_CANDIDATES if c in df.columns), None)\n",
    "    assert date_col is not None, 'No date column found.'\n",
    "    df['date'] = pd.to_datetime(df[date_col]).dt.tz_localize(None)\n",
    "    # Count column\n",
    "    count_col = next((c for c in COUNT_COL_CANDIDATES if c in df.columns), None)\n",
    "    assert count_col is not None, 'No count column found.'\n",
    "    df['y'] = pd.to_numeric(df[count_col], errors='coerce').fillna(0).astype(int)\n",
    "    # Investigator/Team/Role\n",
    "    def _first_col_containing(keys):\n",
    "        for c in df.columns:\n",
    "            for k in keys:\n",
    "                if k in c:\n",
    "                    return c\n",
    "        return None\n",
    "    inv_col = _first_col_containing(INVESTIGATOR_COL_KEYS)\n",
    "    team_col = _first_col_containing(TEAM_COL_KEYS)\n",
    "    role_col = _first_col_containing(ROLE_COL_KEYS)\n",
    "    assert inv_col and team_col and role_col, 'Missing investigator/team/role columns.'\n",
    "    df['investigator'] = df[inv_col].astype(str)\n",
    "    df['team'] = df[team_col].astype(str)\n",
    "    df['role'] = df[role_col].astype(str)\n",
    "    # Keep only needed columns\n",
    "    keep = ['date','investigator','team','role','y']\n",
    "    df = df[keep].sort_values('date')\n",
    "    # Ensure non-negative counts\n",
    "    df['y'] = df['y'].clip(lower=0)\n",
    "    return df\n",
    "\n",
    "# Try in-memory first\n",
    "df0 = _find_df_in_globals()\n",
    "if df0 is None:\n",
    "    p = Path(INPUT_CSV)\n",
    "    if p.exists():\n",
    "        df0 = pd.read_csv(p)\n",
    "        print(f'Loaded dataset from {p}')\n",
    "    else:\n",
    "        raise FileNotFoundError('No suitable DataFrame found in memory and INPUT_CSV does not exist. Update INPUT_CSV or run the build cells above.')\n",
    "else:\n",
    "    print('Using dataset found in memory.')\n",
    "\n",
    "df = _standardise_columns(df0)\n",
    "if MAX_TRAIN_ROWS is not None and len(df) > MAX_TRAIN_ROWS:\n",
    "    df = df.sample(MAX_TRAIN_ROWS, random_state=42).sort_values('date')\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print('Training rows:', len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Feature Engineering (Calendar & Encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    import holidays\n",
    "    _has_holidays = True\n",
    "except Exception:\n",
    "    _has_holidays = False\n",
    "\n",
    "# Day-of-week and holiday flag (England & Wales if available)\n",
    "df = df.copy()\n",
    "df['dow'] = df['date'].dt.dayofweek.astype(int)\n",
    "if _has_holidays:\n",
    "    years = range(df['date'].dt.year.min(), df['date'].dt.year.max() + 3)\n",
    "    uk = holidays.country_holidays('GB', subdiv='ENG', years=years)\n",
    "    df['is_holiday'] = df['date'].dt.date.astype('datetime64')\n",
    "    df['is_holiday'] = df['is_holiday'].apply(lambda d: 1 if d in uk else 0).astype(int)\n",
    "else:\n",
    "    df['is_holiday'] = 0\n",
    "\n",
    "# Encode categories to integer indices\n",
    "inv_codes, inv_idx = pd.factorize(df['investigator'], sort=True)\n",
    "team_codes, team_idx = pd.factorize(df['team'], sort=True)\n",
    "role_codes, role_idx = pd.factorize(df['role'], sort=True)\n",
    "\n",
    "df['inv_idx'] = inv_idx.astype(int)\n",
    "df['team_idx'] = team_idx.astype(int)\n",
    "df['role_idx'] = role_idx.astype(int)\n",
    "\n",
    "n_inv = len(inv_codes)\n",
    "n_team = len(team_codes)\n",
    "n_role = len(role_codes)\n",
    "print({'n_inv': n_inv, 'n_team': n_team, 'n_role': n_role})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Fit Hierarchical Negative Binomial (PyMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Note â€” Why Bayesian here (PyMC)\n",
    "\n",
    "**For data scientists (math/stats):**\n",
    "- Likelihood: $y_i \\sim \\text{NegBin}(\\mu_i, \\alpha)$ with log link $\\log \\mu_i = X_i\\beta + b_{\\text{inv}[i]} + b_{\\text{team}[i]} + b_{\\text{role}[i]} + \\dots$.\n",
    "- We infer the full posterior $p(\\beta, b, \\alpha \\mid y) \\propto \\prod_i p(y_i \\mid \\mu_i, \\alpha)\\, p(\\beta) p(b) p(\\alpha)$ using NUTS (HMC).\n",
    "- Random intercepts yield **partial pooling**, stabilising estimates for sparse investigators/teams and reducing overfitting.\n",
    "- Priors act as **regularisation**; posterior predictive checks (PPC) assess calibration and overdispersion.\n",
    "\n",
    "**For non-experts (plain English):**\n",
    "- Shares information across people/teams so small groups don't swing wildly.\n",
    "- Gives **ranges** (credible intervals) rather than a single numberâ€”better for planning under uncertainty.\n",
    "- Learns patterns like weekdays and holidays and updates as new data arrives.\n",
    "\n",
    "See the README section **â€œWhy Poissonâ€“Gamma (Negative Binomial) for daily case counts?â€** in `README_Investigations_Backlog_Documentation.md` for the reasoning behind the count likelihood and overdispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import aesara.tensor as at\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "DRAWS = 1000   # Increase for production\n",
    "TUNE = 1000\n",
    "TARGET_ACCEPT = 0.9\n",
    "\n",
    "# Build model with MutableData so we can switch to future covariates later\n",
    "with pm.Model() as model:\n",
    "    inv_idx_data = pm.MutableData('inv_idx', df['inv_idx'].values)\n",
    "    team_idx_data = pm.MutableData('team_idx', df['team_idx'].values)\n",
    "    role_idx_data = pm.MutableData('role_idx', df['role_idx'].values)\n",
    "    dow_data = pm.MutableData('dow', df['dow'].values)\n",
    "    hol_data = pm.MutableData('hol', df['is_holiday'].values)\n",
    "    y_obs = df['y'].values\n",
    "\n",
    "    # Hyperpriors for random intercepts\n",
    "    sigma_inv = pm.HalfNormal('sigma_inv', 0.5)\n",
    "    sigma_team = pm.HalfNormal('sigma_team', 0.5)\n",
    "    sigma_role = pm.HalfNormal('sigma_role', 0.5)\n",
    "\n",
    "    z_inv = pm.Normal('z_inv', 0, 1, shape=n_inv)\n",
    "    z_team = pm.Normal('z_team', 0, 1, shape=n_team)\n",
    "    z_role = pm.Normal('z_role', 0, 1, shape=n_role)\n",
    "\n",
    "    inv_eff = pm.Deterministic('inv_eff', z_inv * sigma_inv)\n",
    "    team_eff = pm.Deterministic('team_eff', z_team * sigma_team)\n",
    "    role_eff = pm.Deterministic('role_eff', z_role * sigma_role)\n",
    "\n",
    "    # Fixed effects\n",
    "    beta_intercept = pm.Normal('beta_intercept', 0, 2)\n",
    "    beta_dow = pm.Normal('beta_dow', 0, 0.5, shape=7)\n",
    "    beta_hol = pm.Normal('beta_hol', 0, 0.5)\n",
    "\n",
    "    # Linear predictor\n",
    "    mu_lin = (\n",
    "        beta_intercept\n",
    "        + inv_eff[inv_idx_data]\n",
    "        + team_eff[team_idx_data]\n",
    "        + role_eff[role_idx_data]\n",
    "        + beta_dow[dow_data]\n",
    "        + beta_hol * hol_data\n",
    "    )\n",
    "    lam = pm.Deterministic('lam', at.exp(mu_lin))\n",
    "\n",
    "    # Overdispersion for Negative Binomial\n",
    "    alpha = pm.HalfNormal('alpha', 1.0)\n",
    "\n",
    "    y = pm.NegativeBinomial('y', mu=lam, alpha=alpha, observed=y_obs)\n",
    "\n",
    "    trace = pm.sample(DRAWS, tune=TUNE, target_accept=TARGET_ACCEPT, chains=4, random_seed=RANDOM_SEED)\n",
    "\n",
    "    # In-sample posterior predictive checks\n",
    "    ppc_insample = pm.sample_posterior_predictive(trace, var_names=['y'])\n",
    "print('Model fit complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Forecast Next 90 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path('/mnt/data/forecasts')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build future calendar (next 90 days)\n",
    "last_day = df['date'].max()\n",
    "future_dates = pd.date_range(last_day + pd.Timedelta(days=1), periods=90, freq='D')\n",
    "\n",
    "# Use all observed investigator/team/role combinations\n",
    "units = df[['investigator','team','role','inv_idx','team_idx','role_idx']].drop_duplicates()\n",
    "future = units.assign(key=1).merge(pd.DataFrame({'date': future_dates, 'key':1}), on='key').drop('key', axis=1)\n",
    "\n",
    "# Add features to future\n",
    "future['dow'] = future['date'].dt.dayofweek.astype(int)\n",
    "if 'is_holiday' in df.columns and df['is_holiday'].max() in [0,1]:\n",
    "    # Recompute holidays for the new date range if possible\n",
    "    try:\n",
    "        import holidays\n",
    "        years = range(future['date'].dt.year.min(), future['date'].dt.year.max() + 1)\n",
    "        uk = holidays.country_holidays('GB', subdiv='ENG', years=years)\n",
    "        future['is_holiday'] = future['date'].dt.date.astype('datetime64')\n",
    "        future['is_holiday'] = future['is_holiday'].apply(lambda d: 1 if d in uk else 0).astype(int)\n",
    "    except Exception:\n",
    "        future['is_holiday'] = 0\n",
    "else:\n",
    "    future['is_holiday'] = 0\n",
    "\n",
    "# Switch the model's data to the future design\n",
    "with model:\n",
    "    pm.set_data({\n",
    "        'inv_idx': future['inv_idx'].values,\n",
    "        'team_idx': future['team_idx'].values,\n",
    "        'role_idx': future['role_idx'].values,\n",
    "        'dow': future['dow'].values,\n",
    "        'hol': future['is_holiday'].values,\n",
    "    })\n",
    "    ppc_future = pm.sample_posterior_predictive(trace, var_names=['y'])\n",
    "\n",
    "# Summarise posterior predictive for each row\n",
    "draws = ppc_future['y']  # shape: (draws*chains, N)\n",
    "if draws.ndim == 3:\n",
    "    # Newer PyMC returns (chains, draws, N)\n",
    "    draws = draws.reshape((-1, draws.shape[-1]))\n",
    "means = draws.mean(axis=0)\n",
    "medians = np.median(draws, axis=0)\n",
    "low90 = np.quantile(draws, 0.05, axis=0)\n",
    "high90 = np.quantile(draws, 0.95, axis=0)\n",
    "low50 = np.quantile(draws, 0.25, axis=0)\n",
    "high50 = np.quantile(draws, 0.75, axis=0)\n",
    "\n",
    "future_out = future.copy()\n",
    "future_out['pred_mean'] = means\n",
    "future_out['pred_median'] = medians\n",
    "future_out['pred_p05'] = low90\n",
    "future_out['pred_p95'] = high90\n",
    "future_out['pred_p25'] = low50\n",
    "future_out['pred_p75'] = high50\n",
    "\n",
    "# Save investigator-level forecasts\n",
    "inv_path = OUT_DIR / 'investigator_daily_forecast_90d.csv'\n",
    "future_out.to_csv(inv_path, index=False)\n",
    "print(f'Saved investigator-level forecasts to: {inv_path}')\n",
    "\n",
    "# Aggregations: team-level, role-level, and org-level\n",
    "team_out = (future_out\n",
    "            .groupby(['date','team'], as_index=False)\n",
    "            [['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']]\n",
    "            .sum())\n",
    "role_out = (future_out\n",
    "            .groupby(['date','role'], as_index=False)\n",
    "            [['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']]\n",
    "            .sum())\n",
    "org_out = (future_out\n",
    "           .groupby(['date'], as_index=False)\n",
    "           [['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']]\n",
    "           .sum())\n",
    "\n",
    "team_path = OUT_DIR / 'team_daily_forecast_90d.csv'\n",
    "role_path = OUT_DIR / 'role_daily_forecast_90d.csv'\n",
    "org_path = OUT_DIR / 'org_daily_forecast_90d.csv'\n",
    "team_out.to_csv(team_path, index=False)\n",
    "role_out.to_csv(role_path, index=False)\n",
    "org_out.to_csv(org_path, index=False)\n",
    "print(f'Saved team-level forecasts to: {team_path}')\n",
    "print(f'Saved role-level forecasts to: {role_path}')\n",
    "print(f'Saved org-level forecasts to: {org_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Quick Visual: Organisation-wide Forecast (Totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "org_path = Path('/mnt/data/forecasts/org_daily_forecast_90d.csv')\n",
    "org = pd.read_csv(org_path, parse_dates=['date'])\n",
    "plt.figure()\n",
    "plt.plot(org['date'], org['pred_mean'], label='Forecast mean (next 90 days)')\n",
    "plt.title('Organisation-wide investigated cases: 90-day forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cases')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Plot displayed. Image not saved by default to keep the notebook tidy.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Notes & Tips\n",
    "- To speed up first runs, lower `DRAWS`/`TUNE` or set `MAX_TRAIN_ROWS` to a smaller number.\n",
    "- For richer structure, extend the model with time trends, seasonal splines, or random slopes.\n",
    "- If you prefer formulas, you can port this to **Bambi** with `y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)` and `family='negativebinomial'`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Alternative: Bambi (Formula Interface)\n",
    "**Added:** 2025-10-27 10:48:20Z (UTC)\n",
    "\n",
    "This section mirrors the PyMC approach using **Bambi**, a high-level formula interface\n",
    "built on top of PyMC. It fits a **Negative Binomial** model with random intercepts for\n",
    "**investigator**, **team**, and **role**, plus **day-of-week** and **holiday** effects.\n",
    "\n",
    "Formula used:\n",
    "\n",
    "```\n",
    "y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)\n",
    "```\n",
    "\n",
    "Outputs:\n",
    "- 90-day daily forecasts per investigator, team, and role.\n",
    "- Aggregations to team/role/org.\n",
    "- If supported by your Bambi version, posterior predictive intervals; otherwise, mean forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install if missing (uncomment if needed)\n",
    "# %pip install bambi arviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Bambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Expect df with columns: date, y, dow, is_holiday, investigator, team, role\n",
    "# If df not present, try to load from the same CSV used above (or adjust INPUT_CSV)\n",
    "if 'df' not in globals():\n",
    "    try:\n",
    "        INPUT_CSV\n",
    "    except NameError:\n",
    "        INPUT_CSV = '/mnt/data/investigator_daily.csv'\n",
    "    p = Path(INPUT_CSV)\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        print(f'Loaded dataset from {p}')\n",
    "    else:\n",
    "        raise FileNotFoundError('Expected DataFrame `df` not found in memory and INPUT_CSV does not exist. Please run the build cells above or update INPUT_CSV.')\n",
    "\n",
    "# Standardise expected columns (if needed)\n",
    "df = df.copy()\n",
    "lc = {c: c.lower() for c in df.columns}\n",
    "df.rename(columns=lc, inplace=True)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "if 'dow' not in df.columns:\n",
    "    df['dow'] = df['date'].dt.dayofweek.astype(int)\n",
    "if 'is_holiday' not in df.columns:\n",
    "    df['is_holiday'] = 0\n",
    "if 'y' not in df.columns:\n",
    "    # Heuristic to find a count column\n",
    "    for c in ['cases_investigated','investigated','num_investigated','completed_cases','cases_completed']:\n",
    "        if c in df.columns:\n",
    "            df['y'] = pd.to_numeric(df[c], errors='coerce').fillna(0).astype(int)\n",
    "            break\n",
    "    assert 'y' in df.columns, 'No count column detected; please add column y.'\n",
    "# Ensure categorical variables are categorical (Bambi handles strings too, but categories are explicit)\n",
    "for col in ['investigator','team','role']:\n",
    "    df[col] = df[col].astype('category')\n",
    "print(df[['date','investigator','team','role','y','dow','is_holiday']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Fit Bambi Negative Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Note â€” Why Bayesian here (Bambi)\n",
    "\n",
    "Bambi uses the same Bayesian engine (PyMC) with a formula interface.\n",
    "\n",
    "**For data scientists (math/stats):**\n",
    "- Model: `y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)` with `family='negativebinomial'`.\n",
    "- Hierarchical random intercepts implement partial pooling; priors regularise parameters; NUTS samples the joint posterior.\n",
    "- Use PPC and coverage of credible intervals to validate fit.\n",
    "\n",
    "**For non-experts (plain English):**\n",
    "- Same benefits as the PyMC block, but simpler syntaxâ€”handy for quick iteration and explainability.\n",
    "- Produces forecast **ranges** you can plan around.\n",
    "\n",
    "For the choice of the Negative Binomial (Poissonâ€“Gamma) count model, see `README_Investigations_Backlog_Documentation.md` â†’ *Why Poissonâ€“Gamma (Negative Binomial) for daily case counts?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bambi as bmb\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "DRAWS = 1000   # Increase for production\n",
    "TUNE = 1000\n",
    "TARGET_ACCEPT = 0.9\n",
    "\n",
    "formula = 'y ~ 1 + dow + is_holiday + (1|investigator) + (1|team) + (1|role)'\n",
    "model_bmb = bmb.Model(formula, df, family='negativebinomial')\n",
    "idata_bmb = model_bmb.fit(draws=DRAWS, tune=TUNE, target_accept=TARGET_ACCEPT, chains=4, random_seed=RANDOM_SEED)\n",
    "print('Bambi model fit complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Forecast Next 90 Days with Bambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path('/mnt/data/forecasts')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build future calendar (next 90 days)\n",
    "last_day = df['date'].max()\n",
    "future_dates = pd.date_range(last_day + pd.Timedelta(days=1), periods=90, freq='D')\n",
    "\n",
    "# All observed unit combos\n",
    "units = df[['investigator','team','role']].drop_duplicates()\n",
    "future_bmb = units.assign(key=1).merge(pd.DataFrame({'date': future_dates, 'key':1}), on='key').drop('key', axis=1)\n",
    "future_bmb['dow'] = future_bmb['date'].dt.dayofweek.astype(int)\n",
    "if 'is_holiday' in df.columns and df['is_holiday'].max() in [0,1]:\n",
    "    try:\n",
    "        import holidays\n",
    "        years = range(future_bmb['date'].dt.year.min(), future_bmb['date'].dt.year.max() + 1)\n",
    "        uk = holidays.country_holidays('GB', subdiv='ENG', years=years)\n",
    "        future_bmb['is_holiday'] = future_bmb['date'].dt.date.astype('datetime64')\n",
    "        future_bmb['is_holiday'] = future_bmb['is_holiday'].apply(lambda d: 1 if d in uk else 0).astype(int)\n",
    "    except Exception:\n",
    "        future_bmb['is_holiday'] = 0\n",
    "else:\n",
    "    future_bmb['is_holiday'] = 0\n",
    "\n",
    "# Ensure categorical types align with training\n",
    "for col in ['investigator','team','role']:\n",
    "    future_bmb[col] = future_bmb[col].astype('category')\n",
    "    # align categories with training df\n",
    "    future_bmb[col] = future_bmb[col].cat.set_categories(df[col].cat.categories)\n",
    "\n",
    "def _summarise_pps(draws_array):\n",
    "    # draws_array expected shape: (samples, N)\n",
    "    means = draws_array.mean(axis=0)\n",
    "    medians = np.median(draws_array, axis=0)\n",
    "    p05 = np.quantile(draws_array, 0.05, axis=0)\n",
    "    p95 = np.quantile(draws_array, 0.95, axis=0)\n",
    "    p25 = np.quantile(draws_array, 0.25, axis=0)\n",
    "    p75 = np.quantile(draws_array, 0.75, axis=0)\n",
    "    return means, medians, p05, p95, p25, p75\n",
    "\n",
    "pred_cols = ['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']\n",
    "future_out_bmb = future_bmb.copy()\n",
    "\n",
    "try:\n",
    "    # Preferred: posterior predictive samples\n",
    "    pps = model_bmb.predict(idata_bmb, data=future_bmb, kind='pps')\n",
    "    # Try to convert to a (samples, N) array robustly\n",
    "    import numpy as np\n",
    "    arr = None\n",
    "    # Newer Bambi returns xarray DataArray\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        if isinstance(pps, xr.DataArray):\n",
    "            if set(['chain','draw']).issubset(set(pps.dims)):\n",
    "                arr = pps.stack(sample=('chain','draw')).transpose('sample','obs').values\n",
    "            else:\n",
    "                arr = pps.values\n",
    "    except Exception:\n",
    "        pass\n",
    "    if arr is None:\n",
    "        arr = np.asarray(pps)\n",
    "        if arr.ndim == 3:  # chains, draws, N\n",
    "            arr = arr.reshape((-1, arr.shape[-1]))\n",
    "    m, md, p05, p95, p25, p75 = _summarise_pps(arr)\n",
    "    future_out_bmb['pred_mean'] = m\n",
    "    future_out_bmb['pred_median'] = md\n",
    "    future_out_bmb['pred_p05'] = p05\n",
    "    future_out_bmb['pred_p95'] = p95\n",
    "    future_out_bmb['pred_p25'] = p25\n",
    "    future_out_bmb['pred_p75'] = p75\n",
    "    print('Used posterior predictive samples from Bambi for intervals.')\n",
    "except Exception as e:\n",
    "    print('Falling back to mean predictions only (intervals unavailable):', e)\n",
    "    mu = model_bmb.predict(idata_bmb, data=future_bmb, kind='mean')\n",
    "    future_out_bmb['pred_mean'] = pd.Series(mu).values\n",
    "    # Leave interval columns as NaN to signal they were not computed\n",
    "    for c in pred_cols[1:]:\n",
    "        future_out_bmb[c] = pd.NA\n",
    "\n",
    "# Save investigator-level forecasts (Bambi)\n",
    "inv_path = OUT_DIR / 'investigator_daily_forecast_90d_bambi.csv'\n",
    "future_out_bmb.to_csv(inv_path, index=False)\n",
    "print(f'Saved investigator-level forecasts (Bambi) to: {inv_path}')\n",
    "\n",
    "# Aggregations\n",
    "cols = ['pred_mean','pred_median','pred_p05','pred_p95','pred_p25','pred_p75']\n",
    "team_out_bmb = (future_out_bmb.groupby(['date','team'], as_index=False)[cols].sum(min_count=1))\n",
    "role_out_bmb = (future_out_bmb.groupby(['date','role'], as_index=False)[cols].sum(min_count=1))\n",
    "org_out_bmb = (future_out_bmb.groupby(['date'], as_index=False)[cols].sum(min_count=1))\n",
    "\n",
    "team_path = OUT_DIR / 'team_daily_forecast_90d_bambi.csv'\n",
    "role_path = OUT_DIR / 'role_daily_forecast_90d_bambi.csv'\n",
    "org_path = OUT_DIR / 'org_daily_forecast_90d_bambi.csv'\n",
    "team_out_bmb.to_csv(team_path, index=False)\n",
    "role_out_bmb.to_csv(role_path, index=False)\n",
    "org_out_bmb.to_csv(org_path, index=False)\n",
    "print(f'Saved team-level forecasts (Bambi) to: {team_path}')\n",
    "print(f'Saved role-level forecasts (Bambi) to: {role_path}')\n",
    "print(f'Saved org-level forecasts (Bambi) to: {org_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Quick Visual: Organisation-wide Forecast (Bambi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "org_path = Path('/mnt/data/forecasts/org_daily_forecast_90d_bambi.csv')\n",
    "org = pd.read_csv(org_path, parse_dates=['date'])\n",
    "plt.figure()\n",
    "plt.plot(org['date'], org['pred_mean'], label='Bambi forecast mean (next 90 days)')\n",
    "plt.title('Organisation-wide investigated cases: 90-day forecast (Bambi)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cases')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Plot displayed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
