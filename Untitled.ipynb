{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell writes a reusable Python script that ingests the OPG-style raw case table\n",
    "# (columns like those shown in the screenshot) and produces the Stage‑2 daily panel\n",
    "# plus a backlog series. It handles UK-style dates, sparse/blank fields, and derives\n",
    "# events (new case start, legal markers, court order). It is written as a module-like\n",
    "# script with a main() you can run locally.\n",
    "#\n",
    "# Output files:\n",
    "#   /mnt/data/out/investigator_daily.csv\n",
    "#   /mnt/data/out/backlog_series.csv\n",
    "#   /mnt/data/out/event_log.csv\n",
    "#\n",
    "# You can download the script after execution:\n",
    "#   /mnt/data/build_investigator_daily_from_raw.py\n",
    "\n",
    "import os, textwrap, json, hashlib, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "script_path = Path(\"/mnt/data/build_investigator_daily_from_raw.py\")\n",
    "\n",
    "script_code = r'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build a per‑investigator daily panel and backlog series from an OPG investigations raw extract.\n",
    "\n",
    "Input: a CSV or XLSX/XLS with columns like (case‑insensitive, whitespace ignored):\n",
    "    ID, LPA or DeputyID, Investigator, Investigator FTE, Team, Reallocated Case, Weighting,\n",
    "    Client/Donor Title, Risk, Case Type, Concern Type, Status, Sub Status,\n",
    "    Date Received in OPG, Date Received in Investigations, Date allocated to team,\n",
    "    Date allocated to current investigator, Anticipated completion date, PG Sign off date,\n",
    "    Days on Hold, Currently hold from, Multiple ID, Lead Case, Days to PG sign off,\n",
    "    Closure Date, PG Sign off Hold days, PG sign off to Close days, Last Status,\n",
    "    Referals made by ITAS, High Risk From, Days at high risk, Recommended Court Outcome,\n",
    "    Date of Legal Review Request 1, Date Legal Rejects 1, Reason For Rejection 1, Legal Risk Rejection 1,\n",
    "    Date of Legal Review Request 2, Date Legal Rejects 2, Reason For Rejection 2, Legal Risk Rejection 2,\n",
    "    Date of Legel Review Request 3, LCR Request No, Times Lawyers Allocated (>1 reallocated case),\n",
    "    Legal Approval Date, Legal Risk, Date Sent To CA, CA Acceptance Type, Lawyer, Allocated to Solicitor Date,\n",
    "    Legal Team, PG Signatory, Court Outcome, Date Of Order, PGS Addendum Date, Flagged Date, Flagged Type,\n",
    "    Flag Lawyer, Day 40 Review Date, Day 40 Review Completion Date, Day 70 Review Date, Day 70 Review Completion Date,\n",
    "    Day 100 Review Date, Day 100 Review Completion Date, Further Review Date, No. Of Further Reviews, Concern Received From\n",
    "\n",
    "Outputs:\n",
    "  - investigator_daily.csv with columns:\n",
    "        date, staff_id, team, role, FTE, is_new_starter, weeks_since_start,\n",
    "        wip, time_since_last_pickup, mentoring_flag, trainee_flag,\n",
    "        backlog_available, term_flag, season, dow, bank_holiday,\n",
    "        event_newcase, event_legal, event_court\n",
    "  - backlog_series.csv with columns: date, backlog_available\n",
    "  - event_log.csv (one row per event with case_id + event type + owner on that day)\n",
    "\n",
    "Usage (CLI):\n",
    "    python build_investigator_daily_from_raw.py --in /path/to/raw.csv --outdir ./out\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse, sys, re, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------- Utilities -----------------------\n",
    "\n",
    "NULL_STRINGS = {\n",
    "    \"\", \"na\", \"n/a\", \"none\", \"null\", \"-\", \"--\", \"unknown\",\n",
    "    \"not completed\", \"not complete\", \"tbc\", \"n\\\\a\"\n",
    "}\n",
    "\n",
    "DATE_COLS_RAW = [\n",
    "    \"Date Received in OPG\",\n",
    "    \"Date Received in Investigations\",\n",
    "    \"Date allocated to team\",\n",
    "    \"Date allocated to current investigator\",\n",
    "    \"Anticipated completion date\",\n",
    "    \"PG Sign off date\",\n",
    "    \"Closure Date\",\n",
    "    \"High Risk From\",\n",
    "    \"Date of Legal Review Request 1\",\n",
    "    \"Date Legal Rejects 1\",\n",
    "    \"Date of Legal Review Request 2\",\n",
    "    \"Date Legal Rejects 2\",\n",
    "    \"Date of Legel Review Request 3\",\n",
    "    \"Legal Approval Date\",\n",
    "    \"Date Sent To CA\",\n",
    "    \"Allocated to Solicitor Date\",\n",
    "    \"Date Of Order\",\n",
    "    \"Flagged Date\",\n",
    "    \"Day 40 Review Date\",\n",
    "    \"Day 40 Review Completion Date\",\n",
    "    \"Day 70 Review Date\",\n",
    "    \"Day 70 Review Completion Date\",\n",
    "    \"Day 100 Review Date\",\n",
    "    \"Day 100 Review Completion Date\",\n",
    "    \"Further Review Date\",\n",
    "    \"Anticipated completion date\"\n",
    "]\n",
    "\n",
    "def normalise_col(c: str) -> str:\n",
    "    \"\"\"Normalise column name for robust matching (lowercase, strip, collapse spaces).\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(c).strip().lower())\n",
    "\n",
    "def parse_date_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse messy UK-format date strings to pandas datetime (UTC-naive).\"\"\"\n",
    "    def _parse_one(x):\n",
    "        if pd.isna(x): return pd.NaT\n",
    "        xs = str(x).strip().lower()\n",
    "        if xs in NULL_STRINGS: return pd.NaT\n",
    "        # Remove ordinal suffixes (1st, 2nd, 3rd, 4th)\n",
    "        xs = re.sub(r'(\\d{1,2})(st|nd|rd|th)', r'\\1', xs)\n",
    "        # Common typo fix\n",
    "        xs = xs.replace(\"legel\", \"legal\")\n",
    "        # Try pandas to_datetime with dayfirst\n",
    "        try:\n",
    "            return pd.to_datetime(xs, dayfirst=True, errors=\"raise\")\n",
    "        except Exception:\n",
    "            # Try split by space/slash/dash\n",
    "            try:\n",
    "                return pd.to_datetime(xs, infer_datetime_format=True, dayfirst=True, errors=\"coerce\")\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "    return s.apply(_parse_one)\n",
    "\n",
    "def hash_id(text: str) -> str:\n",
    "    \"\"\"Anonymise staff names deterministically but reversibly if you keep a lookup elsewhere.\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return \"\"\n",
    "    return \"S\" + hashlib.sha1(str(text).encode(\"utf-8\")).hexdigest()[:8]\n",
    "\n",
    "def month_to_season(m: int) -> str:\n",
    "    return {12:\"winter\",1:\"winter\",2:\"winter\",\n",
    "            3:\"spring\",4:\"spring\",5:\"spring\",\n",
    "            6:\"summer\",7:\"summer\",8:\"summer\",\n",
    "            9:\"autumn\",10:\"autumn\",11:\"autumn\"}[int(m)]\n",
    "\n",
    "def is_term_month(m: int) -> int:\n",
    "    # Crude proxy: term in Jan–Jul, Sep–Dec; off-term in Aug\n",
    "    return 0 if int(m) == 8 else 1\n",
    "\n",
    "# ------------------------ Core ---------------------------\n",
    "\n",
    "def load_raw(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV/XLSX, keep strings, and trim whitespace.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    if path.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(path, dtype=str)\n",
    "    else:\n",
    "        df = pd.read_csv(path, dtype=str, sep=None, engine=\"python\")\n",
    "    # Strip whitespace\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    # Build a normalised column map\n",
    "    colmap = {normalise_col(c): c for c in df.columns}\n",
    "    return df, colmap\n",
    "\n",
    "def col(df: pd.DataFrame, colmap: Dict[str,str], name: str) -> pd.Series:\n",
    "    \"\"\"Get a column by tolerant/normalised name; return empty Series if missing.\"\"\"\n",
    "    key = normalise_col(name)\n",
    "    if key in colmap:\n",
    "        return df[colmap[key]]\n",
    "    # Try some fallbacks for typos\n",
    "    for k,v in colmap.items():\n",
    "        if key in k or k in key:\n",
    "            return df[v]\n",
    "    return pd.Series([np.nan]*len(df))\n",
    "\n",
    "def engineer(df: pd.DataFrame, colmap: Dict[str,str]) -> pd.DataFrame:\n",
    "    \"\"\"Create typed fields needed for events and intervals.\"\"\"\n",
    "    out = pd.DataFrame({\n",
    "        \"case_id\": col(df, colmap, \"ID\"),\n",
    "        \"investigator\": col(df, colmap, \"Investigator\"),\n",
    "        \"team\": col(df, colmap, \"Team\"),\n",
    "        \"fte\": pd.to_numeric(col(df, colmap, \"Investigator FTE\"), errors=\"coerce\"),\n",
    "    })\n",
    "    # Dates\n",
    "    out[\"dt_received_inv\"]   = parse_date_series(col(df, colmap, \"Date Received in Investigations\"))\n",
    "    out[\"dt_alloc_invest\"]   = parse_date_series(col(df, colmap, \"Date allocated to current investigator\"))\n",
    "    out[\"dt_alloc_team\"]     = parse_date_series(col(df, colmap, \"Date allocated to team\"))\n",
    "    out[\"dt_pg_signoff\"]     = parse_date_series(col(df, colmap, \"PG Sign off date\"))\n",
    "    out[\"dt_close\"]          = parse_date_series(col(df, colmap, \"Closure Date\"))\n",
    "    out[\"dt_legal_req_1\"]    = parse_date_series(col(df, colmap, \"Date of Legal Review Request 1\"))\n",
    "    out[\"dt_legal_rej_1\"]    = parse_date_series(col(df, colmap, \"Date Legal Rejects 1\"))\n",
    "    out[\"dt_legal_req_2\"]    = parse_date_series(col(df, colmap, \"Date of Legal Review Request 2\"))\n",
    "    out[\"dt_legal_rej_2\"]    = parse_date_series(col(df, colmap, \"Date Legal Rejects 2\"))\n",
    "    out[\"dt_legal_req_3\"]    = parse_date_series(col(df, colmap, \"Date of Legel Review Request 3\"))\n",
    "    out[\"dt_legal_approval\"] = parse_date_series(col(df, colmap, \"Legal Approval Date\"))\n",
    "    out[\"dt_date_of_order\"]  = parse_date_series(col(df, colmap, \"Date Of Order\"))\n",
    "    out[\"dt_flagged\"]        = parse_date_series(col(df, colmap, \"Flagged Date\"))\n",
    "    # Fill FTE default\n",
    "    out[\"fte\"] = out[\"fte\"].fillna(1.0)\n",
    "    # Staff ID (hashed) to avoid PII leakage\n",
    "    out[\"staff_id\"] = out[\"investigator\"].apply(hash_id)\n",
    "    # Role not present in data -> set blank\n",
    "    out[\"role\"] = \"\"\n",
    "    return out\n",
    "\n",
    "def date_horizon(typed: pd.DataFrame, pad_days: int = 14) -> Tuple[pd.Timestamp,pd.Timestamp]:\n",
    "    \"\"\"Pick a sensible horizon from min(received/alloc) to max(close/signoff/order)+pad.\"\"\"\n",
    "    start = pd.concat([typed[\"dt_received_inv\"], typed[\"dt_alloc_invest\"], typed[\"dt_alloc_team\"]]).min()\n",
    "    end   = pd.concat([typed[\"dt_close\"], typed[\"dt_pg_signoff\"], typed[\"dt_date_of_order\"]]).max()\n",
    "    if pd.isna(start):\n",
    "        start = pd.Timestamp.today().normalize() - pd.Timedelta(days=30)\n",
    "    if pd.isna(end):\n",
    "        end = pd.Timestamp.today().normalize()\n",
    "    end = end + pd.Timedelta(days=pad_days)\n",
    "    return start.normalize(), end.normalize()\n",
    "\n",
    "def build_event_log(typed: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Explode date columns into a tidy event log per case/staff/date.\"\"\"\n",
    "    records = []\n",
    "    for _, r in typed.iterrows():\n",
    "        sid = r[\"staff_id\"]; team = r[\"team\"]; fte = r[\"fte\"]; cid = r[\"case_id\"]\n",
    "        def add(dt, etype, meta=None):\n",
    "            if pd.isna(dt): return\n",
    "            records.append({\n",
    "                \"date\": dt.normalize(),\n",
    "                \"staff_id\": sid,\n",
    "                \"team\": team,\n",
    "                \"fte\": fte,\n",
    "                \"case_id\": cid,\n",
    "                \"event\": etype,\n",
    "                \"meta\": meta or \"\"\n",
    "            })\n",
    "        add(r[\"dt_alloc_invest\"], \"newcase\")\n",
    "        # Legal markers\n",
    "        add(r[\"dt_legal_req_1\"], \"legal_request\")\n",
    "        add(r[\"dt_legal_req_2\"], \"legal_request\")\n",
    "        add(r[\"dt_legal_req_3\"], \"legal_request\")\n",
    "        add(r[\"dt_legal_approval\"], \"legal_approval\")\n",
    "        # Court marker\n",
    "        add(r[\"dt_date_of_order\"], \"court_order\")\n",
    "    ev = pd.DataFrame.from_records(records)\n",
    "    if ev.empty:\n",
    "        ev = pd.DataFrame(columns=[\"date\",\"staff_id\",\"team\",\"fte\",\"case_id\",\"event\",\"meta\"])\n",
    "    return ev\n",
    "\n",
    "def build_wip_series(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Line-sweep to compute daily WIP per staff: +1 at allocation, -1 at end+1.\"\"\"\n",
    "    # Define end date per case: closure > signoff > horizon end\n",
    "    end_dt = typed[\"dt_close\"].fillna(typed[\"dt_pg_signoff\"]).fillna(end)\n",
    "    starts = typed[[\"staff_id\",\"team\",\"dt_alloc_invest\"]].dropna()\n",
    "    intervals = pd.DataFrame({\n",
    "        \"staff_id\": typed[\"staff_id\"],\n",
    "        \"team\": typed[\"team\"],\n",
    "        \"start\": typed[\"dt_alloc_invest\"],\n",
    "        \"end\": end_dt\n",
    "    }).dropna()\n",
    "    # Build deltas\n",
    "    deltas = []\n",
    "    for _, r in intervals.iterrows():\n",
    "        s = r[\"start\"].normalize(); e = r[\"end\"].normalize()\n",
    "        if s > end or e < start: \n",
    "            continue\n",
    "        s = max(s, start); e = min(e, end)\n",
    "        deltas.append((r[\"staff_id\"], r[\"team\"], s,  1))\n",
    "        deltas.append((r[\"staff_id\"], r[\"team\"], e + pd.Timedelta(days=1), -1))\n",
    "    if not deltas:\n",
    "        idx = pd.date_range(start, end, freq=\"D\")\n",
    "        return pd.DataFrame({\"date\": idx, \"staff_id\": [], \"team\": [], \"wip\": []}).head(0)\n",
    "    deltas = pd.DataFrame(deltas, columns=[\"staff_id\",\"team\",\"date\",\"delta\"])\n",
    "    # Cum-sum per staff across dates\n",
    "    all_dates = pd.DataFrame({\"date\": pd.date_range(start, end, freq=\"D\")})\n",
    "    out_rows = []\n",
    "    for (sid, team), g in deltas.groupby([\"staff_id\",\"team\"]):\n",
    "        gg = g.groupby(\"date\", as_index=False)[\"delta\"].sum()\n",
    "        grid = all_dates.merge(gg, on=\"date\", how=\"left\").fillna({\"delta\":0})\n",
    "        grid[\"wip\"] = grid[\"delta\"].cumsum()\n",
    "        grid[\"staff_id\"] = sid\n",
    "        grid[\"team\"] = team\n",
    "        out_rows.append(grid[[\"date\",\"staff_id\",\"team\",\"wip\"]])\n",
    "    wip = pd.concat(out_rows, ignore_index=True) if out_rows else pd.DataFrame(columns=[\"date\",\"staff_id\",\"team\",\"wip\"])\n",
    "    return wip\n",
    "\n",
    "def build_backlog_series(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Approximate central backlog = cumulative accepted (received in investigations) minus allocations to investigators.\"\"\"\n",
    "    accepted = typed[[\"dt_received_inv\"]].dropna().assign(date=lambda d: d[\"dt_received_inv\"].dt.normalize())[\"date\"].value_counts().sort_index()\n",
    "    allocated = typed[[\"dt_alloc_invest\"]].dropna().assign(date=lambda d: d[\"dt_alloc_invest\"].dt.normalize())[\"date\"].value_counts().sort_index()\n",
    "    idx = pd.date_range(start, end, freq=\"D\")\n",
    "    acc = accepted.reindex(idx, fill_value=0).cumsum()\n",
    "    allo = allocated.reindex(idx, fill_value=0).cumsum()\n",
    "    backlog = (acc - allo).rename(\"backlog_available\").to_frame()\n",
    "    backlog.index.name = \"date\"\n",
    "    backlog = backlog.reset_index()\n",
    "    return backlog\n",
    "\n",
    "def build_daily_panel(typed: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Combine WIP, event log, and simple calendar features into the Stage‑2 daily panel.\"\"\"\n",
    "    ev = build_event_log(typed)\n",
    "    wip = build_wip_series(typed, start, end)\n",
    "    backlog = build_backlog_series(typed, start, end)\n",
    "\n",
    "    # Base grid: all staff x all dates appearing in either WIP or events\n",
    "    staff = typed[[\"staff_id\",\"team\",\"role\",\"fte\"]].drop_duplicates()\n",
    "    dates = pd.DataFrame({\"date\": pd.date_range(start, end, freq=\"D\")})\n",
    "    grid = dates.assign(key=1).merge(staff.assign(key=1), on=\"key\").drop(columns=\"key\")\n",
    "\n",
    "    # Merge WIP\n",
    "    grid = grid.merge(wip, on=[\"date\",\"staff_id\",\"team\"], how=\"left\").fillna({\"wip\":0})\n",
    "\n",
    "    # Events -> pivot to daily flags\n",
    "    if not ev.empty:\n",
    "        ev_flags = (ev\n",
    "            .assign(flag=lambda d: 1)\n",
    "            .pivot_table(index=[\"date\",\"staff_id\"],\n",
    "                         columns=\"event\",\n",
    "                         values=\"flag\",\n",
    "                         aggfunc=\"max\")\n",
    "            .reset_index()\n",
    "            .rename_axis(None, axis=1)\n",
    "            )\n",
    "        grid = grid.merge(ev_flags, on=[\"date\",\"staff_id\"], how=\"left\")\n",
    "    for c in [\"newcase\",\"legal_request\",\"legal_approval\",\"court_order\"]:\n",
    "        if c not in grid:\n",
    "            grid[c] = 0\n",
    "        else:\n",
    "            grid[c] = grid[c].fillna(0).astype(int)\n",
    "\n",
    "    # time_since_last_pickup per staff (based on newcase flag)\n",
    "    grid = grid.sort_values([\"staff_id\",\"date\"])\n",
    "    grid[\"time_since_last_pickup\"] = grid.groupby(\"staff_id\")[\"newcase\"].apply(\n",
    "        lambda x: x.groupby((x==1).cumsum()).cumcount()\n",
    "    )\n",
    "    # For first run-in where no pickup has happened yet, set to large sentinel (e.g., 99)\n",
    "    mask_no_pickups = grid.groupby(\"staff_id\")[\"newcase\"].transform(\"sum\") == 0\n",
    "    grid.loc[mask_no_pickups, \"time_since_last_pickup\"] = 99\n",
    "\n",
    "    # Simple calendar\n",
    "    grid[\"dow\"] = grid[\"date\"].dt.day_name().str[:3]\n",
    "    grid[\"season\"] = grid[\"date\"].dt.month.map(month_to_season)\n",
    "    grid[\"term_flag\"] = grid[\"date\"].dt.month.map(is_term_month).astype(int)\n",
    "    grid[\"bank_holiday\"] = 0  # can be filled later from a calendar\n",
    "\n",
    "    # New starter heuristic: first 28 days since first allocation in data window\n",
    "    first_alloc = (typed.dropna(subset=[\"dt_alloc_invest\"])\n",
    "                         .groupby(\"staff_id\")[\"dt_alloc_invest\"].min()\n",
    "                         .rename(\"first_alloc\"))\n",
    "    grid = grid.merge(first_alloc, on=\"staff_id\", how=\"left\")\n",
    "    grid[\"weeks_since_start\"] = ((grid[\"date\"] - grid[\"first_alloc\"]).dt.days // 7).fillna(0).clip(lower=0).astype(int)\n",
    "    grid[\"is_new_starter\"] = (grid[\"weeks_since_start\"] < 4).astype(int)\n",
    "\n",
    "    # Mentoring/trainee flags not present -> default 0 (can be merged later from HR)\n",
    "    grid[\"mentoring_flag\"] = 0\n",
    "    grid[\"trainee_flag\"] = 0\n",
    "\n",
    "    # Backlog (same for all staff on a given day)\n",
    "    grid = grid.merge(backlog, on=\"date\", how=\"left\").fillna({\"backlog_available\":0}).sort_values([\"staff_id\",\"date\"])\n",
    "\n",
    "    # Rename event flags to model columns\n",
    "    grid[\"event_newcase\"] = grid[\"newcase\"].astype(int)\n",
    "    grid[\"event_legal\"]   = ((grid[\"legal_request\"] + grid[\"legal_approval\"]) > 0).astype(int)\n",
    "    grid[\"event_court\"]   = grid[\"court_order\"].astype(int)\n",
    "    grid = grid.drop(columns=[\"newcase\",\"legal_request\",\"legal_approval\",\"court_order\",\"first_alloc\"])\n",
    "\n",
    "    # Order columns\n",
    "    cols = [\"date\",\"staff_id\",\"team\",\"role\",\"fte\",\n",
    "            \"is_new_starter\",\"weeks_since_start\",\n",
    "            \"wip\",\"time_since_last_pickup\",\n",
    "            \"mentoring_flag\",\"trainee_flag\",\n",
    "            \"backlog_available\",\"term_flag\",\"season\",\"dow\",\"bank_holiday\",\n",
    "            \"event_newcase\",\"event_legal\",\"event_court\"]\n",
    "    return grid[cols].sort_values([\"staff_id\",\"date\"])\n",
    "\n",
    "# ------------------------- CLI --------------------------\n",
    "\n",
    "def main(argv=None):\n",
    "    ap = argparse.ArgumentParser(description=\"Build per‑investigator daily panel from OPG investigations extract.\")\n",
    "    ap.add_argument(\"--in\", dest=\"in_path\", required=True, help=\"Path to raw CSV/XLSX\")\n",
    "    ap.add_argument(\"--outdir\", dest=\"outdir\", default=\"./out\", help=\"Output directory\")\n",
    "    args = ap.parse_args(argv)\n",
    "\n",
    "    in_path = Path(args.in_path)\n",
    "    outdir = Path(args.outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df, colmap = load_raw(in_path)\n",
    "    typed = engineer(df, colmap)\n",
    "    start, end = date_horizon(typed, pad_days=14)\n",
    "\n",
    "    # Build outputs\n",
    "    daily = build_daily_panel(typed, start, end)\n",
    "    backlog = build_backlog_series(typed, start, end)\n",
    "    events = build_event_log(typed)\n",
    "\n",
    "    # Write\n",
    "    daily.to_csv(outdir / \"investigator_daily.csv\", index=False)\n",
    "    backlog.to_csv(outdir / \"backlog_series.csv\", index=False)\n",
    "    events.to_csv(outdir / \"event_log.csv\", index=False)\n",
    "\n",
    "    # Brief report\n",
    "    print(f\"Rows in daily panel: {len(daily):,}\")\n",
    "    print(f\"Date range: {daily['date'].min().date()} → {daily['date'].max().date()}\")\n",
    "    print(f\"Investigators: {daily['staff_id'].nunique()}\")\n",
    "    print(f\"Total new case events: {daily['event_newcase'].sum()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the script to disk\n",
    "script_path.write_text(script_code, encoding=\"utf-8\")\n",
    "\n",
    "# Create a tiny README snippet for convenience\n",
    "readme = Path(\"/mnt/data/README_build_daily.md\")\n",
    "readme.write_text(textwrap.dedent(\"\"\"\n",
    "# Build Investigator Daily Panel (from OPG raw extract)\n",
    "\n",
    "**Script:** `build_investigator_daily_from_raw.py`\n",
    "\n",
    "## Usage\n",
    "```bash\n",
    "python build_investigator_daily_from_raw.py --in /path/to/raw.csv --outdir /path/to/out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
