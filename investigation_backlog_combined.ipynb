{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "investigation_backlog_combined\n",
    "================================\n",
    "\n",
    "module that merges:\n",
    "\n",
    "* Build_Investigator_Daily_from_Raw_14_11_25.ipynb\n",
    "* eda.ipynb\n",
    "\n",
    "and adds a high-level `InvestigationBacklogProject` class as a single\n",
    "entry point for running the full pipeline and EDA.\n",
    "\n",
    "\n",
    "The original bottom-of-notebook demo cells are preserved under an\n",
    "``if __name__ == '__main__':`` guard so they will not run on import.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "#!python -m venv .venv && . .venv/bin/activate\n",
    "\n",
    "# Import libraries/modules for use below\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# Configure paths\n",
    "# Path to the raw investigation data\n",
    "RAW_PATH = Path(\"data/raw/raw.csv\")\n",
    "# Path to the output/processed investigation data\n",
    "OUT_DIR = Path(\"data/out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Print if the path exists\n",
    "print(RAW_PATH.exists(), OUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# DATA PRE-PROCESSING\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Define a set of string patterns that represent missing or null values.\n",
    "# These strings will be treated as equivalent to NaN during cleaning.\n",
    "NULL_STRINGS = {\n",
    "    \"\",\n",
    "    \"na\",\n",
    "    \"n/a\",\n",
    "    \"none\",\n",
    "    \"null\",\n",
    "    \"-\",\n",
    "    \"--\",\n",
    "    \"unknown\",\n",
    "    \"not completed\",\n",
    "    \"not complete\",\n",
    "    \"tbc\",\n",
    "    \"n\\\\a\",\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper: normalise_col()\n",
    "# -------------------------------------------------------------\n",
    "def normalise_col(c: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a column name for consistency.\n",
    "\n",
    "    This function cleans up and standardises column names by:\n",
    "    - convert to string, lower-case\n",
    "    - Removing leading/trailing whitespace\n",
    "    - Replacing multiple spaces with a single space\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c : str\n",
    "        The original column name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A cleaned and standardized version of the column name.\n",
    "    \"\"\"\n",
    "    # Convert to string, remove extra spaces, and make lowercase.\n",
    "    return re.sub(r\"\\s+\", \" \", str(c).strip().lower())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper: parse_date_series()\n",
    "# -------------------------------------------------------------\n",
    "def parse_date_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse and clean a pandas Series of date strings.\n",
    "\n",
    "    This function robustly parse a pandas Series into datetimes:\n",
    "    - Handles various date formats\n",
    "    - Converts known null strings to NaT\n",
    "    - Removes ordinal suffixes (e.g., '1st', '2nd', '3rd')\n",
    "    - Fixes known typos\n",
    "    - Uses robust pandas date parsing with fallback strategies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : pd.Series\n",
    "        A pandas Series containing raw date values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series of datetime64[ns] values with cleaned and parsed dates.\n",
    "    \"\"\"\n",
    "\n",
    "    def _p(x):\n",
    "        \"\"\"Internal helper to parse a single date entry.\"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        # Return NaT if missing\n",
    "        if pd.isna(x):\n",
    "            return pd.NaT\n",
    "\n",
    "        # Convert to lowercase string\n",
    "        xs = str(x).strip().lower()\n",
    "\n",
    "        # Return NaT if in known null string set\n",
    "        if xs in NULL_STRINGS:\n",
    "            return pd.NaT\n",
    "\n",
    "        # Clean up common errors and ordinal suffixes\n",
    "        xs = re.sub(r\"(\\d{1,2})(st|nd|rd|th)\", r\"\\1\", xs).replace(\"legel\", \"legal\")\n",
    "\n",
    "        # Try strict parsing, then flexible fallback\n",
    "        try:\n",
    "            return pd.to_datetime(xs, dayfirst=True, errors=\"raise\")\n",
    "        except Exception:\n",
    "            return pd.to_datetime(\n",
    "                xs, infer_datetime_format=True, dayfirst=True, errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "    # Apply the parser to each element of the Series\n",
    "    return s.apply(_p)\n",
    "\n",
    "    if s is None:\n",
    "        return pd.Series(pd.NaT, index=pd.RangeIndex(0))\n",
    "\n",
    "    # If numeric-like (possible Excel serials), try converting via pandas\n",
    "    s_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    has_numeric = s_num.notna().any()\n",
    "\n",
    "    # First pass: assume strings with day-first ambiguity handled later\n",
    "    dt1 = pd.to_datetime(\n",
    "        s, errors=\"coerce\", utc=False, dayfirst=True, infer_datetime_format=True\n",
    "    )\n",
    "\n",
    "    if has_numeric:\n",
    "        # Where dt1 is NaT but we have a number, try fromordinal-like conversion via pandas\n",
    "        # pandas handles Excel serials when unit='D' origin='1899-12-30'\n",
    "        serial_dt = pd.to_datetime(\n",
    "            s_num, unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\"\n",
    "        )\n",
    "        dt1 = dt1.fillna(serial_dt)\n",
    "\n",
    "    # Final pass (month-first) for any remaining NaT strings\n",
    "    mask_nat = dt1.isna() & s.notna()\n",
    "    if mask_nat.any():\n",
    "        dt2 = pd.to_datetime(\n",
    "            s.where(mask_nat),\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=False,\n",
    "            infer_datetime_format=True,\n",
    "        )\n",
    "        dt1 = dt1.fillna(dt2)\n",
    "\n",
    "    # Normalise to midnight\n",
    "    return dt1.dt.normalize()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper: hash_id()\n",
    "# -------------------------------------------------------------\n",
    "def hash_id(t: str, prefix: str = \"S\", length: int = 12) -> str:\n",
    "    \"\"\"\n",
    "    Generate a short, anonymized hash-based identifier.\n",
    "\n",
    "    Creates a pseudonymized ID for text entries using SHA1 hashing.\n",
    "    Empty or missing values return an empty string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : str\n",
    "        The input text value (e.g., name, case number).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        An anonymised hash string prefixed with 'S', e.g., 'S1a2b3c4d'.\n",
    "    \"\"\"\n",
    "    # # Return empty string for null or blank input\n",
    "    # if pd.isna(t) or str(t).strip() == '':\n",
    "    #     return ''\n",
    "\n",
    "    # # Create SHA1 hash and take first 8 characters for compact ID\n",
    "    # return 'S' + hashlib.sha1(str(t).encode('utf-8')).hexdigest()[:8]\n",
    "\n",
    "    if pd.isna(t) or str(t).strip() == \"\":\n",
    "        return \"\"\n",
    "    h = hashlib.sha256(str(t).strip().lower().encode(\"utf-8\")).hexdigest()\n",
    "    return f\"{prefix}_{h[:length]}\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper: month_to_season()\n",
    "# -------------------------------------------------------------\n",
    "def month_to_season(m: int) -> str:\n",
    "    \"\"\"\n",
    "    Convert a numeric month into a season name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Month number (1–12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The season corresponding to the month ('winter', 'spring', 'summer', or 'autumn').\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> month_to_season(4)\n",
    "    'spring'\n",
    "    >>> month_to_season(10)\n",
    "    'autumn'\n",
    "    \"\"\"\n",
    "    # Map month numbers to their respective seasons\n",
    "    return {\n",
    "        12: \"winter\",\n",
    "        1: \"winter\",\n",
    "        2: \"winter\",\n",
    "        3: \"spring\",\n",
    "        4: \"spring\",\n",
    "        5: \"spring\",\n",
    "        6: \"summer\",\n",
    "        7: \"summer\",\n",
    "        8: \"summer\",\n",
    "        9: \"autumn\",\n",
    "        10: \"autumn\",\n",
    "        11: \"autumn\",\n",
    "    }[int(m)]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helper: is_term_month()\n",
    "# -------------------------------------------------------------\n",
    "def is_term_month(m: int) -> int:\n",
    "    \"\"\"\n",
    "    Identify whether a month is a 'termination month'.\n",
    "\n",
    "    In the current logic, August (month 8) is excluded and returns 0.\n",
    "    All other months return 1, representing active/valid months.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Month number (1–12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if the month is August, else 1.\n",
    "    \"\"\"\n",
    "    # Return binary flag based on month value\n",
    "    return 0 if int(m) == 8 else 1\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# DATA LOADING AND FEATURE ENGINEERING\n",
    "# -------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: load_raw()\n",
    "# -------------------------------------------------------------\n",
    "def load_raw(p: Path, force_encoding: str | None = None):\n",
    "    \"\"\"\n",
    "    Load a CSV or Excel file into a pandas DataFrame with robust encoding handling.\n",
    "\n",
    "    This function attempts to open and read raw data files safely, even when\n",
    "    character encodings vary or are unknown. It tries multiple encodings until\n",
    "    one succeeds, trims cell whitespace, drops empty rows/columns, and returns a\n",
    "    column-name map to support fuzzy lookups.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : Path\n",
    "        Path to the input file.\n",
    "    force_encoding : str, optional\n",
    "        If provided, forces the use of a specific encoding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (df, colmap)\n",
    "        df : pd.DataFrame\n",
    "            Cleaned dataframe containing the raw data.\n",
    "        colmap : dict\n",
    "            Mapping of normalized column names (lowercased, trimmed) to original column headers.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the file path does not exist.\n",
    "    RuntimeError\n",
    "        If all encoding attempts fail.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check file existence\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "    # --- Read the file (Excel has no encoding issue) ---\n",
    "    if p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(p, dtype=str)\n",
    "    else:\n",
    "        tried, df, last_err = [], None, None\n",
    "        encodings_to_try = (\n",
    "            [force_encoding]\n",
    "            if force_encoding\n",
    "            else [\n",
    "                \"utf-8-sig\",\n",
    "                \"utf-8\",\n",
    "                \"cp1252\",\n",
    "                \"latin1\",\n",
    "                \"iso-8859-1\",\n",
    "                \"utf-16\",\n",
    "                \"utf-16le\",\n",
    "                \"utf-16be\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Try to read using multiple encodings\n",
    "        for enc in encodings_to_try:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p,\n",
    "                    dtype=str,\n",
    "                    sep=None,\n",
    "                    engine=\"python\",\n",
    "                    encoding=enc,\n",
    "                    encoding_errors=\"strict\",\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                tried.append(enc)\n",
    "                last_err = e\n",
    "        if df is None:\n",
    "            # Fallback with replacement to avoid hard fail\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p,\n",
    "                    dtype=str,\n",
    "                    sep=None,\n",
    "                    engine=\"python\",\n",
    "                    encoding=\"cp1252\",\n",
    "                    encoding_errors=\"replace\",\n",
    "                )\n",
    "                print(\n",
    "                    f\"[load_raw] WARNING: used cp1252 with replacement after failed encodings: {tried}\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Failed to read CSV. Tried encodings {tried}. Last error: {last_err}\"\n",
    "                ) from e\n",
    "\n",
    "    # --- Clean up cell text, drop fully-empty rows/columns ---\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    # Drop columns that are entirely blank/NaN\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    # Drop rows that are entirely blank/NaN\n",
    "    df = df.dropna(axis=0, how=\"all\")\n",
    "\n",
    "    # --- Build mapping of normalised column names → original names ---\n",
    "    raw_cols = list(df.columns)\n",
    "    normalised = [normalise_col(c) for c in raw_cols]\n",
    "\n",
    "    # Handle collisions (two columns normalise to the same key)\n",
    "    colmap = {}\n",
    "    seen = {}\n",
    "    for orig, norm in zip(raw_cols, normalised):\n",
    "        if norm in seen:\n",
    "            # append a numeric suffix to make the key unique\n",
    "            seen[norm] += 1\n",
    "            key = f\"{norm}__{seen[norm]}\"\n",
    "        else:\n",
    "            seen[norm] = 0\n",
    "            key = norm\n",
    "        colmap[key] = orig\n",
    "\n",
    "    return df, colmap\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: col()\n",
    "# -------------------------------------------------------------\n",
    "def col(df: pd.DataFrame, colmap: dict, name: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Retrieve a column from a DataFrame by fuzzy name matching.\n",
    "\n",
    "    This function normalises the requested column name and searches the column map\n",
    "    for an exact or partial match. Returns a Series of NaNs if not found.\n",
    "    - First tries exact match on a normalised key.\n",
    "    - Then tries partial match (either direction).\n",
    "    - Finally, falls back to an empty Series (NaNs) of correct length.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The source DataFrame.\n",
    "    colmap : dict\n",
    "        Mapping of normalised column names to original names (from load_raw()).\n",
    "    name : str\n",
    "        Column name to look up.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        The column data if found, otherwise a Series of NaN values.\n",
    "    \"\"\"\n",
    "\n",
    "    k = normalise_col(name)\n",
    "\n",
    "    # 1) Exact match\n",
    "    if k in colmap:\n",
    "        return df[colmap[k]]\n",
    "\n",
    "    # 2) Partial match (prefix/substring in either direction)\n",
    "    for kk, v in colmap.items():\n",
    "        if k in kk or kk in k:\n",
    "            return df[v]\n",
    "\n",
    "    # 3) If load_raw had to suffix collided keys, try any key that starts with k\n",
    "    for kk, v in colmap.items():\n",
    "        if kk.startswith(k):\n",
    "            return df[v]\n",
    "\n",
    "    # 4) Default: return empty column (NaNs) so downstream code doesn't crash\n",
    "    return pd.Series([np.nan] * len(df), index=df.index)\n",
    "    # # Exact match first\n",
    "    # if k in colmap:\n",
    "    #     return df[colmap[k]]\n",
    "\n",
    "    # # Partial match fallback\n",
    "    # for kk, v in colmap.items():\n",
    "    #     if k in kk or kk in k:\n",
    "    #         return df[v]\n",
    "\n",
    "    # # Default: return empty column of NaNs\n",
    "    # return pd.Series([np.nan] * len(df))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: engineer()\n",
    "# -------------------------------------------------------------\n",
    "def engineer(\n",
    "    df: pd.DataFrame,\n",
    "    colmap: dict,\n",
    "    only_reallocated: bool = False,  # NEW: filter toggle\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer standardised and typed columns from raw investigation data.\n",
    "\n",
    "    This function extracts and converts the key variables such as case IDs, investigators,\n",
    "    FTEs, and multiple date columns from the raw file using reusable helper functions.\n",
    "      - selects and cleans core identifiers (case, staff, team, role, FTE)\n",
    "      - parses all relevant milestone dates\n",
    "      - brings in extra attributes (reallocated flag, weighting, types/status)\n",
    "      - computes anonymised staff IDs\n",
    "      - optionally filters to only reallocated cases via `only_reallocated`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw dataframe from load_raw().\n",
    "    colmap : dict\n",
    "        Column name mapping from load_raw().\n",
    "    only_reallocated : bool, default True\n",
    "        If True, return only cases where 'Reallocated Case' is truthy\n",
    "        (accepts yes/y/true/1, case-insensitive). If False, return all cases.\n",
    "\n",
    "    pd.DataFrame\n",
    "        Cleaned, typed dataset ready for downstream modelling.\n",
    "        Includes:\n",
    "          - `is_reallocated` (bool) derived from 'Reallocated Case'\n",
    "          - all date columns as datetime (normalised)\n",
    "          - numeric fields coerced where applicable (fte, weighting, days_to_pg_signoff)\n",
    "    \"\"\"\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"case_id\": col(df, colmap, \"ID\"),\n",
    "            \"investigator\": col(df, colmap, \"Investigator\"),\n",
    "            \"team\": col(df, colmap, \"Team\"),\n",
    "            \"fte\": pd.to_numeric(col(df, colmap, \"Investigator FTE\"), errors=\"coerce\"),\n",
    "            \"reallocated_case\": col(df, colmap, \"Reallocated Case\"),\n",
    "            \"weighting\": pd.to_numeric(col(df, colmap, \"Weighting\"), errors=\"coerce\"),\n",
    "            \"case_type\": col(df, colmap, \"Case Type\"),\n",
    "            \"concern_type\": col(df, colmap, \"Concern Type\"),\n",
    "            \"status\": col(df, colmap, \"Status\"),\n",
    "            \"days_to_pg_signoff\": pd.to_numeric(\n",
    "                col(df, colmap, \"Days to PG sign off\"), errors=\"coerce\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Parse and standardise relevant date columns\n",
    "    out[\"dt_received_inv\"] = parse_date_series(\n",
    "        col(df, colmap, \"Date Received in Investigations\")\n",
    "    )\n",
    "    out[\"dt_alloc_invest\"] = parse_date_series(\n",
    "        col(df, colmap, \"Date allocated to current investigator\")\n",
    "    )\n",
    "    out[\"dt_alloc_team\"] = parse_date_series(col(df, colmap, \"Date allocated to team\"))\n",
    "    out[\"dt_pg_signoff\"] = parse_date_series(col(df, colmap, \"PG Sign off date\"))\n",
    "    out[\"dt_close\"] = parse_date_series(col(df, colmap, \"Closure Date\"))\n",
    "    out[\"dt_legal_req_1\"] = parse_date_series(\n",
    "        col(df, colmap, \"Date of Legal Review Request 1\")\n",
    "    )\n",
    "    out[\"dt_legal_rej_1\"] = parse_date_series(col(df, colmap, \"Date Legal Rejects 1\"))\n",
    "    out[\"dt_legal_req_2\"] = parse_date_series(\n",
    "        col(df, colmap, \"Date of Legal Review Request 2\")\n",
    "    )\n",
    "    out[\"dt_legal_rej_2\"] = parse_date_series(col(df, colmap, \"Date Legal Rejects 2\"))\n",
    "    out[\"dt_legal_req_3\"] = parse_date_series(\n",
    "        col(df, colmap, \"Date of Legel Review Request 3\")\n",
    "    )\n",
    "    out[\"dt_legal_approval\"] = parse_date_series(col(df, colmap, \"Legal Approval Date\"))\n",
    "    out[\"dt_date_of_order\"] = parse_date_series(col(df, colmap, \"Date Of Order\"))\n",
    "    out[\"dt_flagged\"] = parse_date_series(col(df, colmap, \"Flagged Date\"))\n",
    "    out[\"dt_sent_to_ca\"] = parse_date_series(col(df, colmap, \"Date Sent To CA\"))\n",
    "\n",
    "    # Fill missing FTEs with 1.0, hash investigator names for anonymization, and add placeholders\n",
    "    # Defaults, anonymisation, and placeholders\n",
    "    out[\"fte\"] = out[\"fte\"].fillna(1.0)  # assume FT when missing\n",
    "    out[\"staff_id\"] = out[\"investigator\"].apply(hash_id)  # anonymise\n",
    "    # If a 'role' column existed in raw, keep it; else initialise blank\n",
    "    role_series = (\n",
    "        col(df, colmap, \"Role\")\n",
    "        if \"role\" in [k.split(\"__\")[0] for k in colmap.keys()]\n",
    "        else pd.Series([\"\"] * len(out))\n",
    "    )\n",
    "    out[\"role\"] = role_series.fillna(\"\") if isinstance(role_series, pd.Series) else \"\"\n",
    "\n",
    "    # Compute days_to_pg_signoff if wholly missing but dates exist\n",
    "    if (\n",
    "        out[\"days_to_pg_signoff\"].isna().all()\n",
    "        and (\"dt_pg_signoff\" in out)\n",
    "        and (\"dt_alloc_invest\" in out)\n",
    "    ):\n",
    "        diff = (out[\"dt_pg_signoff\"] - out[\"dt_alloc_invest\"]).dt.days\n",
    "        out[\"days_to_pg_signoff\"] = pd.to_numeric(diff, errors=\"coerce\")\n",
    "\n",
    "    # --- NEW: derive a clean boolean, then optionally filter ---\n",
    "    reall_str = out[\"reallocated_case\"].astype(str).str.strip().str.lower()\n",
    "    out[\"is_reallocated\"] = reall_str.isin({\"yes\", \"y\", \"true\", \"1\"})\n",
    "\n",
    "    if only_reallocated:\n",
    "        out = out.loc[out[\"is_reallocated\"]].reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# DATA MANIPULATION AND PROCESSING\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: date_horizon()\n",
    "# -------------------------------------------------------------\n",
    "def date_horizon(\n",
    "    typed: pd.DataFrame, pad_days: int = 14, fallback_to_all_dates: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Primary rule:\n",
    "      - start := earliest non-null value in 'dt_received_inv'\n",
    "      - end   := latest non-null value in 'dt_pg_signoff'\n",
    "\n",
    "    Optional fallback:\n",
    "      If either start or end cannot be determined (column missing or all NaT)\n",
    "      *and* fallback_to_all_dates is True, compute:\n",
    "        - start := min across ALL columns starting with 'dt_'\n",
    "        - end   := max across ALL columns starting with 'dt_'\n",
    "\n",
    "    Finally, apply `pad_days` to the end date. If still missing after fallback,\n",
    "    default to a 30-day lookback for start and today for end (+ padding).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Feature-engineered dataset with standardized date columns.\n",
    "    pad_days : int, default=14\n",
    "        Number of days to extend the end horizon.\n",
    "        pad_days adds a few days to the end date as a buffer.\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        Whether to fall back to scanning all `dt_` columns when the primary\n",
    "        columns are unavailable or empty.\n",
    "        This allows scanning all dt_… columns if the main two are missing/empty.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of pd.Timestamp\n",
    "        (start, end) normalised date range.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Falls back to recent 30 days if dt_received_inv or dt_pg_signoff\n",
    "    are missing or contain no valid dates.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from datetime import datetime\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05'), pd.NaT],\n",
    "    ...     'dt_alloc_invest': [pd.NaT, pd.Timestamp('2025-01-10')],\n",
    "    ...     'dt_alloc_team': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.NaT, pd.Timestamp('2025-02-01')],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> s, e = date_horizon(df, pad_days=7)\n",
    "    >>> isinstance(s, pd.Timestamp) and isinstance(e, pd.Timestamp)\n",
    "    True\n",
    "    >>> (e - s).days >= (pd.Timestamp('2025-02-01') - pd.Timestamp('2025-01-05')).days\n",
    "    True\n",
    "    \"\"\"\n",
    "    # start = pd.concat([typed['dt_received_inv'], typed['dt_alloc_invest'], typed['dt_alloc_team']]).min()\n",
    "    # end = pd.concat([typed['dt_close'], typed['dt_pg_signoff'], typed['dt_date_of_order']]).max()\n",
    "\n",
    "    # --- Primary computation from specified columns ---\n",
    "    start = pd.NaT\n",
    "    end = pd.NaT\n",
    "\n",
    "    # Initialise start and end as “not a time” (missing).\n",
    "    if \"dt_received_inv\" in typed:\n",
    "        start = typed[\"dt_received_inv\"].dropna().min()\n",
    "\n",
    "    # If the “received” column exists, take the earliest non-missing date as start.\n",
    "    if \"dt_pg_signoff\" in typed:\n",
    "        end = typed[\"dt_pg_signoff\"].dropna().max()\n",
    "\n",
    "    # If the “PG sign-off” column exists, take the latest non-missing date as end.\n",
    "    # --- Optional fallback over all dt_ columns ---\n",
    "    if (pd.isna(start) or pd.isna(end)) and fallback_to_all_dates:\n",
    "        dt_cols = [c for c in typed.columns if c.startswith(\"dt_\")]\n",
    "        if dt_cols:\n",
    "            all_dates = pd.concat(\n",
    "                [typed[c] for c in dt_cols], ignore_index=True\n",
    "            ).dropna()\n",
    "            if pd.isna(start) and not all_dates.empty:\n",
    "                start = all_dates.min()\n",
    "            if pd.isna(end) and not all_dates.empty:\n",
    "                end = all_dates.max()\n",
    "\n",
    "    # --- Final graceful defaults if still missing ---\n",
    "    # If we still don’t have a start/end, default to a 30-day lookback ending at today.\n",
    "    today = pd.Timestamp.today().normalize()\n",
    "    if pd.isna(start):\n",
    "        start = today - pd.Timedelta(days=30)\n",
    "    if pd.isna(end):\n",
    "        end = today\n",
    "\n",
    "    # Add pad_days to end (the buffer) and normalize both dates to midnight (clean calendar dates).\n",
    "    # --- Apply padding to end and normalise ---\n",
    "    end = (end + pd.Timedelta(days=pad_days)).normalize()\n",
    "    return start.normalize(), end\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_event_log()\n",
    "# -------------------------------------------------------------\n",
    "def build_event_log(\n",
    "    typed: pd.DataFrame, pad_days: int = 14, fallback_to_all_dates: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct a staff-day event log from feature-engineered investigation data.\n",
    "\n",
    "    Each row represents a dated event for a specific case and staff member.\n",
    "    For example, “Investigator S1 picked up case C1 on 2025-01-10.”\n",
    "\n",
    "    For each case, this function creates dated event records (e.g., new case pickup,\n",
    "    legal requests/approvals, court orders) at the staff-day level.\n",
    "\n",
    "    Events emitted (if their date exists):\n",
    "      received         -> dt_received_inv\n",
    "      alloc_team       -> dt_alloc_team\n",
    "      newcase          -> dt_alloc_invest\n",
    "      sent_to_ca       -> dt_sent_to_ca\n",
    "      legal_request    -> dt_legal_req_1, dt_legal_req_2, dt_legal_req_3\n",
    "      legal_reject     -> dt_legal_rej_1, dt_legal_rej_2\n",
    "      legal_approval   -> dt_legal_approval\n",
    "      pg_signoff       -> dt_pg_signoff\n",
    "      court_order      -> dt_date_of_order\n",
    "      closed           -> dt_close\n",
    "      flagged          -> dt_flagged\n",
    "\n",
    "      The output is restricted to the date horizon determined by date_horizon()\n",
    "      using dt_received_inv for start and dt_pg_signoff for end (with padding).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Output of engineer(); typically already filtered to reallocated cases.\n",
    "        Expected columns include identifiers, staffing info, and the dt_* fields.\n",
    "    pad_days : int, default=14\n",
    "        Extra days added to end horizon via date_horizon().\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        If start/end cannot be derived from the primary columns, allow\n",
    "        date_horizon() to fallback across all dt_* columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['date','staff_id','team','fte','case_id','event','meta']\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Includes lightweight, structured meta (JSON) with weighting, case_type, concern_type, status, and days_to_pg_signoff.\n",
    "    to keep contextual attributes about that case alongside each event (for later analysis or auditing), such as:\n",
    "    Case weighting (e.g., 2.5 for complexity or workload)\n",
    "    Case type (Financial / Welfare / etc.)\n",
    "    Concern type (Neglect / Abuse / etc.)\n",
    "    Current status (Open / Closed / etc.)\n",
    "    Days to PG sign-off (performance metric)\n",
    "\n",
    "    Instead of duplicating these as separate columns for every event — which would make the event log wide, repetitive,\n",
    "    and harder to serialize — we store them compactly in a single column named meta.\n",
    "    Each meta cell is a JSON string encoding those extra attributes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> from datetime import datetime\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id': ['S1'],\n",
    "    ...     'team': ['A'],\n",
    "    ...     'fte': [1.0],\n",
    "    ...     'case_id': ['C1'],\n",
    "    ...     'weighting': [2.5],\n",
    "    ...     'case_type': ['Financial'],\n",
    "    ...     'concern_type': ['Neglect'],\n",
    "    ...     'status': ['Open'],\n",
    "    ...     'days_to_pg_signoff': [15],\n",
    "    ...     # Key timeline dates\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05')],\n",
    "    ...     'dt_alloc_team': [pd.Timestamp('2025-01-08')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-10')],\n",
    "    ...     'dt_sent_to_ca': [pd.Timestamp('2025-01-12')],\n",
    "    ...     'dt_legal_req_1': [pd.Timestamp('2025-01-14')],\n",
    "    ...     'dt_legal_req_2': [pd.NaT],\n",
    "    ...     'dt_legal_req_3': [pd.NaT],\n",
    "    ...     'dt_legal_rej_1': [pd.NaT],\n",
    "    ...     'dt_legal_rej_2': [pd.NaT],\n",
    "    ...     'dt_legal_approval': [pd.Timestamp('2025-01-20')],\n",
    "    ...     'dt_pg_signoff': [pd.Timestamp('2025-01-25')],\n",
    "    ...     'dt_date_of_order': [pd.NaT],\n",
    "    ...     'dt_close': [pd.Timestamp('2025-02-01')],\n",
    "    ...     'dt_flagged': [pd.NaT],\n",
    "    ... })\n",
    "    >>> ev = build_event_log(typed)\n",
    "    >>> sorted(ev['event'].unique().tolist())\n",
    "    ['alloc_team', 'closed', 'legal_approval', 'legal_request',\n",
    "     'newcase', 'pg_signoff', 'received', 'sent_to_ca']\n",
    "    >>> set(ev.columns) >= {'date','staff_id','team','fte','case_id','event','meta'}\n",
    "    True\n",
    "    >>> # Each meta cell contains structured JSON metadata:\n",
    "    >>> import json\n",
    "    >>> json.loads(ev.loc[0, 'meta'])\n",
    "    {'weighting': 2.5,\n",
    "     'case_type': 'Financial',\n",
    "     'concern_type': 'Neglect',\n",
    "     'status': 'Open',\n",
    "     'days_to_pg_signoff': 15.0}\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd, json\n",
    "    >>> # Two cases, two investigators, showcasing more event types\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id': ['S1', 'S2'],\n",
    "    ...     'team': ['A', 'B'],\n",
    "    ...     'fte': [1.0, 0.8],\n",
    "    ...     'case_id': ['C1', 'C2'],\n",
    "    ...     'weighting': [2.5, 1.0],\n",
    "    ...     'case_type': ['Financial', 'Welfare'],\n",
    "    ...     'concern_type': ['Neglect', 'Abuse'],\n",
    "    ...     'status': ['Open', 'Open'],\n",
    "    ...     'days_to_pg_signoff': [15, pd.NA],\n",
    "    ...     # Timeline dates (C1 has a full path incl. pg_signoff; C2 shows rejects, no signoff)\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-05'), pd.Timestamp('2025-01-07')],\n",
    "    ...     'dt_alloc_team': [pd.Timestamp('2025-01-08'), pd.Timestamp('2025-01-09')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-10'), pd.Timestamp('2025-01-11')],\n",
    "    ...     'dt_sent_to_ca': [pd.Timestamp('2025-01-12'), pd.NaT],\n",
    "    ...     'dt_legal_req_1': [pd.Timestamp('2025-01-14'), pd.Timestamp('2025-01-15')],\n",
    "    ...     'dt_legal_req_2': [pd.NaT, pd.Timestamp('2025-01-18')],\n",
    "    ...     'dt_legal_req_3': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_legal_rej_1': [pd.NaT, pd.Timestamp('2025-01-17')],\n",
    "    ...     'dt_legal_rej_2': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_legal_approval': [pd.Timestamp('2025-01-20'), pd.NaT],\n",
    "    ...     'dt_pg_signoff': [pd.Timestamp('2025-01-25'), pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.Timestamp('2025-02-01'), pd.NaT],\n",
    "    ...     'dt_flagged': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> ev = build_event_log(typed)  # uses date_horizon(start=dt_received_inv, end=dt_pg_signoff+pad)\n",
    "    >>> # Unique event types emitted\n",
    "    >>> sorted(ev['event'].unique().tolist())\n",
    "    ['alloc_team', 'closed', 'legal_approval', 'legal_reject', 'legal_request',\n",
    "     'newcase', 'pg_signoff', 'received', 'sent_to_ca']\n",
    "    >>> # Schema check\n",
    "    >>> set(ev.columns) >= {'date','staff_id','team','fte','case_id','event','meta'}\n",
    "    True\n",
    "    >>> # Per-case event counts (C1 has a full pathway, C2 has requests + a reject)\n",
    "    >>> ev.groupby('case_id')['event'].count().to_dict()  # doctest: +ELLIPSIS\n",
    "    {'C1': 8, 'C2': 6}\n",
    "    >>> # meta is JSON with contextual fields\n",
    "    >>> m = json.loads(ev.loc[ev['case_id'].eq('C2')].iloc[0]['meta'])\n",
    "    >>> set(m.keys()) == {'weighting','case_type','concern_type','status','days_to_pg_signoff'}\n",
    "    True\n",
    "    >>> m['case_type'], m['concern_type'], m['weighting']\n",
    "    ('Welfare', 'Abuse', 1.0)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import json\n",
    "\n",
    "    # Ensure expected minimal columns exist\n",
    "    base_cols = [\"staff_id\", \"team\", \"fte\", \"case_id\"]\n",
    "    for c in base_cols:\n",
    "        if c not in typed.columns:\n",
    "            raise KeyError(\n",
    "                f\"build_event_log: required column '{c}' missing from 'typed'.\"\n",
    "            )\n",
    "\n",
    "    # Compute the date horizon\n",
    "    start, end = date_horizon(\n",
    "        typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates\n",
    "    )\n",
    "\n",
    "    # Helper to safely read a column if present\n",
    "    def getcol(name: str):\n",
    "        return (\n",
    "            typed[name]\n",
    "            if name in typed.columns\n",
    "            else pd.Series([pd.NaT] * len(typed), index=typed.index)\n",
    "        )\n",
    "\n",
    "    # Pre-pull columns used in meta (safe if absent)\n",
    "    weighting = (\n",
    "        typed[\"weighting\"]\n",
    "        if \"weighting\" in typed.columns\n",
    "        else pd.Series([pd.NA] * len(typed), index=typed.index)\n",
    "    )\n",
    "    case_type = (\n",
    "        typed[\"case_type\"]\n",
    "        if \"case_type\" in typed.columns\n",
    "        else pd.Series([pd.NA] * len(typed), index=typed.index)\n",
    "    )\n",
    "    concern_type = (\n",
    "        typed[\"concern_type\"]\n",
    "        if \"concern_type\" in typed.columns\n",
    "        else pd.Series([pd.NA] * len(typed), index=typed.index)\n",
    "    )\n",
    "    status = (\n",
    "        typed[\"status\"]\n",
    "        if \"status\" in typed.columns\n",
    "        else pd.Series([pd.NA] * len(typed), index=typed.index)\n",
    "    )\n",
    "    days_to_pg = (\n",
    "        typed[\"days_to_pg_signoff\"]\n",
    "        if \"days_to_pg_signoff\" in typed.columns\n",
    "        else pd.Series([pd.NA] * len(typed), index=typed.index)\n",
    "    )\n",
    "\n",
    "    # Map of event names to the corresponding date columns to scan (one or many)\n",
    "    event_map = {\n",
    "        \"received\": [\"dt_received_inv\"],\n",
    "        \"alloc_team\": [\"dt_alloc_team\"],\n",
    "        \"newcase\": [\"dt_alloc_invest\"],\n",
    "        \"sent_to_ca\": [\"dt_sent_to_ca\"],\n",
    "        \"legal_request\": [\"dt_legal_req_1\", \"dt_legal_req_2\", \"dt_legal_req_3\"],\n",
    "        \"legal_reject\": [\"dt_legal_rej_1\", \"dt_legal_rej_2\"],\n",
    "        \"legal_approval\": [\"dt_legal_approval\"],\n",
    "        \"pg_signoff\": [\"dt_pg_signoff\"],\n",
    "        \"court_order\": [\"dt_date_of_order\"],\n",
    "        \"closed\": [\"dt_close\"],\n",
    "        \"flagged\": [\"dt_flagged\"],\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    # Iterate row-wise to emit events per case\n",
    "    for i, r in typed.iterrows():\n",
    "        sid, team, fte, cid = r[\"staff_id\"], r[\"team\"], r[\"fte\"], r[\"case_id\"]\n",
    "\n",
    "        # Build the meta payload once per row\n",
    "        meta_dict = {\n",
    "            \"weighting\": None if pd.isna(weighting.iloc[i]) else weighting.iloc[i],\n",
    "            \"case_type\": None if pd.isna(case_type.iloc[i]) else str(case_type.iloc[i]),\n",
    "            \"concern_type\": (\n",
    "                None if pd.isna(concern_type.iloc[i]) else str(concern_type.iloc[i])\n",
    "            ),\n",
    "            \"status\": None if pd.isna(status.iloc[i]) else str(status.iloc[i]),\n",
    "            \"days_to_pg_signoff\": (\n",
    "                None if pd.isna(days_to_pg.iloc[i]) else float(days_to_pg.iloc[i])\n",
    "            ),\n",
    "        }\n",
    "        meta_json = json.dumps(meta_dict, ensure_ascii=False)\n",
    "\n",
    "        # Emit events for each configured date column\n",
    "        for etype, cols in event_map.items():\n",
    "            for c in cols:\n",
    "                if c in typed.columns:\n",
    "                    dt = r[c]\n",
    "                    if pd.notna(dt):\n",
    "                        dtn = pd.to_datetime(dt).normalize()\n",
    "                        # Keep only within the computed horizon\n",
    "                        if start <= dtn <= end:\n",
    "                            records.append(\n",
    "                                {\n",
    "                                    \"date\": dtn,\n",
    "                                    \"staff_id\": sid,\n",
    "                                    \"team\": team,\n",
    "                                    \"fte\": fte,\n",
    "                                    \"case_id\": cid,\n",
    "                                    \"event\": etype,\n",
    "                                    \"meta\": meta_json,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "    ev = pd.DataFrame.from_records(records)\n",
    "\n",
    "    if ev.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"date\", \"staff_id\", \"team\", \"fte\", \"case_id\", \"event\", \"meta\"]\n",
    "        )\n",
    "\n",
    "    # Deduplicate identical events (same staff/case/date/type)\n",
    "    ev = (\n",
    "        ev.drop_duplicates(subset=[\"date\", \"staff_id\", \"case_id\", \"event\"])\n",
    "        .sort_values([\"date\", \"staff_id\", \"case_id\", \"event\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Ensure dtypes are tidy\n",
    "    ev[\"date\"] = pd.to_datetime(ev[\"date\"]).dt.normalize()\n",
    "    ev[\"fte\"] = pd.to_numeric(ev[\"fte\"], errors=\"coerce\")\n",
    "\n",
    "    return ev\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# TIME SERIES ANALYSIS\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_wip_series()\n",
    "# -------------------------------------------------------------\n",
    "def build_wip_series(\n",
    "    typed: pd.DataFrame,\n",
    "    start: pd.Timestamp | None = None,\n",
    "    end: pd.Timestamp | None = None,\n",
    "    pad_days: int = 14,\n",
    "    fallback_to_all_dates: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a Work-In-Progress (WIP) daily series per staff member.\n",
    "\n",
    "    A case is considered WIP from dt_alloc_invest (inclusive) to the earliest of:\n",
    "      - dt_close\n",
    "      - dt_pg_signoff\n",
    "      - provided/computed `end` horizon\n",
    "\n",
    "    If `start`/`end` are not provided, they are derived via `date_horizon()` with the\n",
    "    rule: start from dt_received_inv, end from dt_pg_signoff (+ pad_days).\n",
    "\n",
    "    Inputs and defaults:\n",
    "    typed: engineered table (one row per case).\n",
    "    start, end: optional date limits for the report.\n",
    "    If start or end are missing, it calls date_horizon() to derive them from the data using the rule (received → pg_signoff + padding).\n",
    "    Then it normalises them to whole dates (midnight).\n",
    "\n",
    "    Output includes:\n",
    "      - `wip`       : number of active cases (count-based)\n",
    "      - `wip_load`  : workload proxy, defined as weighting / fte (fallbacks to 1.0 if absent)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Expected columns:\n",
    "          identifiers: ['staff_id','team','case_id'] (case_id optional for debugging)\n",
    "          dates: ['dt_alloc_invest','dt_close','dt_pg_signoff'] (+ others for date_horizon)\n",
    "          optional: ['weighting','fte']\n",
    "    start : pd.Timestamp | None\n",
    "        Start of the reporting horizon (normalised to date). If None, computed via date_horizon().\n",
    "    end : pd.Timestamp | None\n",
    "        End of the reporting horizon (normalised to date). If None, computed via date_horizon().\n",
    "    pad_days : int, default=14\n",
    "        Only used if start/end are not supplied; passed to date_horizon().\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        Passed to date_horizon().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: ['date','staff_id','team','wip','wip_load']\n",
    "        - One row per (date, staff_id, team).\n",
    "        - `wip` is guaranteed non-negative.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> # Two cases for S1; second case has PG sign-off. Includes weighting & fte for wip_load.\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'staff_id': ['S1','S1'],\n",
    "    ...     'team': ['A','A'],\n",
    "    ...     'case_id': ['C1','C2'],\n",
    "    ...     'fte': [1.0, 0.5],\n",
    "    ...     'weighting': [2.0, 1.0],\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-01')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-05')],\n",
    "    ...     'dt_close': [pd.Timestamp('2025-01-03'), pd.NaT],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.Timestamp('2025-01-07')],\n",
    "    ... })\n",
    "    >>> # Explicit horizon\n",
    "    >>> wip = build_wip_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-10'))\n",
    "    >>> set(wip.columns) == {'date','staff_id','team','wip','wip_load'}\n",
    "    True\n",
    "    >>> wip['wip'].ge(0).all()\n",
    "    True\n",
    "    >>> # On 2025-01-06, both cases are WIP -> wip >= 1\n",
    "    >>> int(wip.loc[wip['date'].eq(pd.Timestamp('2025-01-06')), 'wip'].max()) >= 1\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Compute horizon (if needed) ---\n",
    "    # If you don’t pass start/end, the function figures them out using your project rule:\n",
    "    # start = earliest dt_received_inv\n",
    "    # end = latest dt_pg_signoff plus a small padding window\n",
    "    if start is None or end is None:\n",
    "        s, e = date_horizon(\n",
    "            typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates\n",
    "        )\n",
    "        if start is None:\n",
    "            start = s\n",
    "        if end is None:\n",
    "            end = e\n",
    "\n",
    "    # Normalise\n",
    "    start = pd.to_datetime(start).normalize()\n",
    "    end = pd.to_datetime(end).normalize()\n",
    "\n",
    "    # --- Guard: required columns for interval construction ---\n",
    "    # Verifies key columns exist: staff_id, team, dt_alloc_invest.\n",
    "    # If any are missing, it raises a helpful error explaining what’s needed.\n",
    "    for c in [\"staff_id\", \"team\", \"dt_alloc_invest\"]:\n",
    "        if c not in typed.columns:\n",
    "            raise KeyError(\n",
    "                f\"build_wip_series: required column '{c}' missing from 'typed'.\"\n",
    "            )\n",
    "\n",
    "    # --- Prepare per-case start/end ---\n",
    "    # Start of work\n",
    "    s_col = pd.to_datetime(typed[\"dt_alloc_invest\"], errors=\"coerce\")\n",
    "\n",
    "    # Earliest of dt_close and dt_pg_signoff per row; then fallback to provided/computed end\n",
    "    # End of work = the earliest of dt_close and dt_pg_signoff.\n",
    "    # If both are missing, the end defaults to the overall report end date (so open cases remain WIP).\n",
    "    close_candidates = pd.concat(\n",
    "        [\n",
    "            (\n",
    "                pd.to_datetime(typed[\"dt_close\"], errors=\"coerce\")\n",
    "                if \"dt_close\" in typed\n",
    "                else pd.Series(pd.NaT, index=typed.index)\n",
    "            ),\n",
    "            (\n",
    "                pd.to_datetime(typed[\"dt_pg_signoff\"], errors=\"coerce\")\n",
    "                if \"dt_pg_signoff\" in typed\n",
    "                else pd.Series(pd.NaT, index=typed.index)\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    row_end = close_candidates.min(axis=1)  # earliest available milestone\n",
    "    row_end = row_end.fillna(end)\n",
    "\n",
    "    # Case load for wip_load: weighting / fte (with robust fallbacks)\n",
    "    # If weighting is missing, it uses 1.0 (assume average complexity).\n",
    "    if \"weighting\" in typed.columns:\n",
    "        w_series = pd.to_numeric(typed[\"weighting\"], errors=\"coerce\").fillna(1.0)\n",
    "    else:\n",
    "        w_series = pd.Series(1.0, index=typed.index)\n",
    "    # If fte is missing or zero, it uses 1.0 (avoid division by zero and keep a sane baseline).\n",
    "    if \"fte\" in typed.columns:\n",
    "        fte_series = (\n",
    "            pd.to_numeric(typed[\"fte\"], errors=\"coerce\").replace(0, pd.NA).fillna(1.0)\n",
    "        )\n",
    "    else:\n",
    "        fte_series = pd.Series(1.0, index=typed.index)\n",
    "\n",
    "    # Load per case = weighting ÷ fte\n",
    "    load = (w_series / fte_series).astype(float)\n",
    "\n",
    "    # Creates a small table with one row per case showing:\n",
    "    # staff_id, team, start (allocation), end (close/signoff/report end), and load.\n",
    "    intervals = pd.DataFrame(\n",
    "        {\n",
    "            \"staff_id\": typed[\"staff_id\"],\n",
    "            \"team\": typed[\"team\"],\n",
    "            \"start\": s_col,\n",
    "            \"end\": row_end,\n",
    "            \"load\": load,\n",
    "        }\n",
    "    ).dropna(subset=[\"start\", \"end\"])\n",
    "\n",
    "    # --- Build delta encoding (inclusive start, inclusive end) ---\n",
    "    # Delta encoding (efficient daily accumulation)\n",
    "    # Creates a full daily calendar and applies the cumulative sum of deltas.\n",
    "    deltas = []\n",
    "    horizon_start, horizon_end = start, end\n",
    "    for _, r in intervals.iterrows():\n",
    "        s = pd.to_datetime(r[\"start\"]).normalize()\n",
    "        e = pd.to_datetime(r[\"end\"]).normalize()\n",
    "\n",
    "        # Skip if outside horizon\n",
    "        if s > horizon_end or e < horizon_start:\n",
    "            continue\n",
    "\n",
    "        s = max(s, horizon_start)\n",
    "        e = min(e, horizon_end)\n",
    "\n",
    "        # For each case interval: Add a +1 (and +load) on the start date.\n",
    "        # Add a −1 (and −load) on the day after the end date.\n",
    "        # +1 case and +load at start; -1 and -load at day after end\n",
    "        deltas.append((r[\"staff_id\"], r[\"team\"], s, 1.0, r[\"load\"]))\n",
    "        deltas.append(\n",
    "            (r[\"staff_id\"], r[\"team\"], e + pd.Timedelta(days=1), -1.0, -r[\"load\"])\n",
    "        )\n",
    "    # This means when we later cumulatively sum these daily changes, we get the number of active cases (and total load) each day.\n",
    "    if not deltas:\n",
    "        return pd.DataFrame(columns=[\"date\", \"staff_id\", \"team\", \"wip\", \"wip_load\"])\n",
    "\n",
    "    deltas = pd.DataFrame(\n",
    "        deltas, columns=[\"staff_id\", \"team\", \"date\", \"d_cases\", \"d_load\"]\n",
    "    )\n",
    "    # Builds a continuous list of dates from start to end\n",
    "    all_dates = pd.DataFrame(\n",
    "        {\"date\": pd.date_range(horizon_start, horizon_end, freq=\"D\")}\n",
    "    )\n",
    "\n",
    "    # --- Accumulate per staff/team over the horizon ---\n",
    "    # For each staff × team group:\n",
    "    # Merges the deltas onto the daily grid.\n",
    "    # Cumulative sums to get wip (counts) and wip_load (load).\n",
    "    # Clips at zero to avoid negative values if data has gaps.\n",
    "    out_rows = []\n",
    "    for (sid, team), g in deltas.groupby([\"staff_id\", \"team\"], sort=False):\n",
    "        gg = g.groupby(\"date\", as_index=False)[[\"d_cases\", \"d_load\"]].sum()\n",
    "        grid = all_dates.merge(gg, on=\"date\", how=\"left\").fillna(\n",
    "            {\"d_cases\": 0.0, \"d_load\": 0.0}\n",
    "        )\n",
    "        # clip(lower=0) ensures small data glitches can’t produce negatives.\n",
    "        grid[\"wip\"] = grid[\"d_cases\"].cumsum().clip(lower=0)  # case count\n",
    "        grid[\"wip_load\"] = grid[\"d_load\"].cumsum().clip(lower=0.0)  # workload proxy\n",
    "        grid[\"staff_id\"] = sid\n",
    "        grid[\"team\"] = team\n",
    "        out_rows.append(grid[[\"date\", \"staff_id\", \"team\", \"wip\", \"wip_load\"]])\n",
    "\n",
    "    out = (\n",
    "        pd.concat(out_rows, ignore_index=True)\n",
    "        if out_rows\n",
    "        else pd.DataFrame(columns=[\"date\", \"staff_id\", \"team\", \"wip\", \"wip_load\"])\n",
    "    )\n",
    "\n",
    "    # Ensure dtypes / normalisation\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"]).dt.normalize()\n",
    "    out[\"wip\"] = pd.to_numeric(out[\"wip\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "    out[\"wip_load\"] = (\n",
    "        pd.to_numeric(out[\"wip_load\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# TIME SERIES ANALYSIS\n",
    "# -------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_backlog_series()\n",
    "# -------------------------------------------------------------\n",
    "def build_backlog_series(\n",
    "    typed: pd.DataFrame,\n",
    "    start: pd.Timestamp | None = None,\n",
    "    end: pd.Timestamp | None = None,\n",
    "    pad_days: int = 14,\n",
    "    fallback_to_all_dates: bool = True,\n",
    "    clip_zero: bool = True,\n",
    "    compute_weighted: bool = False,\n",
    "    exclude_weekends: bool = False,\n",
    "    holidays: list | pd.Series | None = None,\n",
    "    freq: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a daily backlog series where:\n",
    "        backlog = cumulative received − cumulative allocated.\n",
    "\n",
    "    Definitions\n",
    "    -----------\n",
    "    - Received: cases entering Investigations (dt_received_inv)=(case enters Investigations queue).\n",
    "    - Allocated: cases allocated to an investigator (dt_alloc_invest)=(case leaves queue to an investigator).\n",
    "    - Backlog available: cases received but not yet allocated.\n",
    "\n",
    "    Horizon\n",
    "    -------\n",
    "    If `start`/`end` are not provided, they are derived via `date_horizon()`:\n",
    "      start := earliest dt_received_inv,\n",
    "      end := latest dt_pg_signoff (+ pad_days),\n",
    "      with optional fallback to all dt_* columns if primary dates are missing.\n",
    "\n",
    "    Options\n",
    "    -------\n",
    "    - clip_zero:        Prevent negative backlog (recommended).\n",
    "    - compute_weighted: Also compute weighted backlog using 'weighting' if present.\n",
    "    - exclude_weekends: Remove Saturdays/Sundays from the time axis.\n",
    "    - holidays:         Iterable of dates to exclude (e.g., UK bank holidays).\n",
    "    - freq:             Optional resampling frequency (e.g., 'W-MON', 'W-FRI', 'MS').\n",
    "                        For cumulative series, we take the last value per period.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    typed : pd.DataFrame\n",
    "        Expected columns:\n",
    "          - dates: ['dt_received_inv','dt_alloc_invest']  (others allowed but not required)\n",
    "          - optional: ['weighting'] if compute_weighted=True\n",
    "        Note: this frame is already filtered to reallocated cases per your earlier requirement.\n",
    "    start, end : pd.Timestamp | None\n",
    "        Reporting horizon (inclusive). If None, computed via date_horizon().\n",
    "    pad_days : int, default=14\n",
    "        Only used when deriving start/end via date_horizon().\n",
    "    fallback_to_all_dates : bool, default=True\n",
    "        Passed to date_horizon().\n",
    "    clip_zero : bool, default=True\n",
    "        If True, backlog cannot go below 0 (defensive; improves interpretability).\n",
    "    compute_weighted : bool, default=False\n",
    "        If True and 'weighting' is present, also compute backlog_weighted\n",
    "        using the same logic but summing weights instead of counts.\n",
    "    exclude_weekends : bool, default=False\n",
    "        If True drop Saturdays/Sundays from the series\n",
    "    holidays : bool, default=False\n",
    "        If True drop a custom list/series of dates (e.g., UK bank holidays)\n",
    "    freq : str | None\n",
    "        optional resampling (e.g., 'W-MON', 'W-FRI', 'MS' for month-start).\n",
    "        For cumulative series, we take the last value per period.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with at least following Columns (daily):\n",
    "          - date\n",
    "          - received_cum      : cumulative count of received\n",
    "          - allocated_cum     : cumulative count of allocated\n",
    "          - backlog_available : received_cum - allocated_cum (clipped at 0 if clip_zero)\n",
    "          - (optional: and, if compute_weighted) received_weighted_cum, allocated_weighted_cum, backlog_weighted\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ... })\n",
    "    >>> backlog = build_backlog_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05'))\n",
    "    >>> list(backlog.columns)\n",
    "    ['date', 'received_cum', 'allocated_cum', 'backlog_available']\n",
    "    >>> backlog.iloc[-1]['backlog_available']  # 2 received, 1 allocated -> 1\n",
    "    1.0\n",
    "\n",
    "    >>> # Weighted example (if 'weighting' present)\n",
    "    >>> typed2 = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ...     'weighting': [2.0, 0.5],\n",
    "    ... })\n",
    "    >>> backlog_w = build_backlog_series(typed2, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05'), compute_weighted=True)\n",
    "    >>> {'backlog_available', 'backlog_weighted'}.issubset(backlog_w.columns)\n",
    "    True\n",
    "\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.NaT],\n",
    "    ... })\n",
    "    >>> # Daily (default calendar)\n",
    "    >>> build_backlog_series(typed, pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05')).tail(1)[['backlog_available']].iloc[0,0]\n",
    "    1.0\n",
    "\n",
    "    >>> # Business days only (excludes weekends)\n",
    "    >>> business = build_backlog_series(\n",
    "    ...     typed,\n",
    "    ...     pd.Timestamp('2025-01-01'),\n",
    "    ...     pd.Timestamp('2025-01-10'),\n",
    "    ...     exclude_weekends=True\n",
    "    ... )\n",
    "\n",
    "    >>> # With holidays excluded and weekly roll-up (end-of-week values)\n",
    "    >>> holidays = [pd.Timestamp('2025-01-06')]\n",
    "    >>> weekly = build_backlog_series(\n",
    "    ...     typed,\n",
    "    ...     pd.Timestamp('2025-01-01'),\n",
    "    ...     pd.Timestamp('2025-01-31'),\n",
    "    ...     exclude_weekends=True,\n",
    "    ...     holidays=holidays,\n",
    "    ...     freq='W-FRI'\n",
    "    ... )\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Derive horizon if needed  ---\n",
    "    # If we didn’t pass start/end, we derive them with date_horizon()\n",
    "    if start is None or end is None:\n",
    "        s, e = date_horizon(\n",
    "            typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates\n",
    "        )\n",
    "        if start is None:\n",
    "            start = s\n",
    "        if end is None:\n",
    "            end = e\n",
    "    # Normalise them to dates (no times).\n",
    "    start = pd.to_datetime(start).normalize()\n",
    "    end = pd.to_datetime(end).normalize()\n",
    "\n",
    "    # --- Extract and normalise event dates ---\n",
    "    rec_dates = (\n",
    "        pd.to_datetime(\n",
    "            typed.get(\"dt_received_inv\", pd.Series([], dtype=\"datetime64[ns]\")),\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        .dropna()\n",
    "        .dt.normalize()\n",
    "    )\n",
    "    alloc_dates = (\n",
    "        pd.to_datetime(\n",
    "            typed.get(\"dt_alloc_invest\", pd.Series([], dtype=\"datetime64[ns]\")),\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        .dropna()\n",
    "        .dt.normalize()\n",
    "    )\n",
    "\n",
    "    # --- Daily counts (received / allocated) ---\n",
    "    # Daily counts → cumulative totals\n",
    "    # Count how many received and allocated events happen per day.\n",
    "    received_daily = rec_dates.value_counts().sort_index()\n",
    "    allocated_daily = alloc_dates.value_counts().sort_index()\n",
    "\n",
    "    # --- Build full daily index over the horizon ---\n",
    "    idx = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "    # Optional calendar filtering (weekends and/or holidays)\n",
    "    if exclude_weekends:\n",
    "        idx = idx[idx.weekday < 5]  # 0=Mon ... 4=Fri\n",
    "    if holidays is not None and len(pd.Index(holidays)) > 0:\n",
    "        hol = pd.to_datetime(pd.Index(holidays)).normalize()\n",
    "        idx = idx.difference(hol)\n",
    "\n",
    "    # Helper to reindex to possibly filtered calendar and cumulate\n",
    "    def cumulate(series_counts: pd.Series, index: pd.DatetimeIndex) -> pd.Series:\n",
    "        # We need the *full* daily cumsum first, then realign to filtered index\n",
    "        full_range = pd.date_range(start, end, freq=\"D\")\n",
    "        full_cum = (\n",
    "            series_counts.reindex(full_range, fill_value=0).cumsum().astype(float)\n",
    "        )\n",
    "        # If calendar is filtered, take values at the kept dates\n",
    "        return full_cum.reindex(index, method=\"ffill\").fillna(0.0)\n",
    "\n",
    "    # --- Cumulate counts over the horizon (missing days = 0) ---\n",
    "    # Reindex missing days as zeros and cumulatively sum to get “total so far”.\n",
    "    received_cum = received_daily.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "    allocated_cum = allocated_daily.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "\n",
    "    # Backlog is the gap between total received and total allocated.\n",
    "    backlog = received_cum - allocated_cum\n",
    "    # Optionally clip at 0 (defensive, avoids negative values if historical allocations\n",
    "    #  predate the first received in the window).\n",
    "    if clip_zero:\n",
    "        backlog = backlog.clip(lower=0.0)\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": idx,\n",
    "            \"received_cum\": received_cum.values,\n",
    "            \"allocated_cum\": allocated_cum.values,\n",
    "            \"backlog_available\": backlog.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # --- Optional weighted backlog ---\n",
    "    # Sum weights per day at receipt and at allocation, then cumulate and subtract.\n",
    "    # Same structure as counts, but with weights instead of 1s.\n",
    "    if compute_weighted:\n",
    "        # If weighting missing, assume 1.0 for rows with the date present, else 0\n",
    "        weights = pd.to_numeric(\n",
    "            typed.get(\"weighting\", pd.Series([1.0] * len(typed))), errors=\"coerce\"\n",
    "        ).fillna(1.0)\n",
    "\n",
    "        # Map weights to dates for received and allocated events\n",
    "        def weighted_daily(dates: pd.Series, weight_series: pd.Series) -> pd.Series:\n",
    "            if len(dates) == 0:\n",
    "                return pd.Series(dtype=float)\n",
    "            tmp = pd.DataFrame({\"date\": dates.reset_index(drop=True)})\n",
    "            # Align weights to the same original row positions as 'dates'\n",
    "            tmp[\"weight\"] = weight_series.loc[dates.index].values\n",
    "            return tmp.groupby(\"date\")[\"weight\"].sum().sort_index()\n",
    "\n",
    "        # Build per-date weight sums for received and allocated\n",
    "        rec_w_daily = weighted_daily(rec_dates, weights)\n",
    "        alloc_w_daily = weighted_daily(alloc_dates, weights)\n",
    "\n",
    "        # reindex\n",
    "        rec_w_cum = cumulate(rec_w_daily, idx)\n",
    "        alloc_w_cum = cumulate(alloc_w_daily, idx)\n",
    "\n",
    "        # # Build per-date weight sums for received and allocated\n",
    "        # rec_weights = (\n",
    "        #     pd.DataFrame({'date': rec_dates.reset_index(drop=True)})\n",
    "        #     .assign(weight=weights.loc[rec_dates.index].values if len(rec_dates) else [])\n",
    "        #     .groupby('date')['weight'].sum()\n",
    "        #     if len(rec_dates) else pd.Series(dtype=float)\n",
    "        # )\n",
    "\n",
    "        # alloc_weights = (\n",
    "        #     pd.DataFrame({'date': alloc_dates.reset_index(drop=True)})\n",
    "        #     .assign(weight=weights.loc[alloc_dates.index].values if len(alloc_dates) else [])\n",
    "        #     .groupby('date')['weight'].sum()\n",
    "        #     if len(alloc_dates) else pd.Series(dtype=float)\n",
    "        # )\n",
    "\n",
    "        # rec_w_cum = rec_weights.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "        # alloc_w_cum = alloc_weights.reindex(idx, fill_value=0).cumsum().astype(float)\n",
    "\n",
    "        backlog_w = rec_w_cum - alloc_w_cum\n",
    "        if clip_zero:\n",
    "            backlog_w = backlog_w.clip(lower=0.0)\n",
    "\n",
    "        out[\"received_weighted_cum\"] = rec_w_cum.values\n",
    "        out[\"allocated_weighted_cum\"] = alloc_w_cum.values\n",
    "        out[\"backlog_weighted\"] = backlog_w.values\n",
    "\n",
    "    # --- Optional resampling (weekly/monthly views)\n",
    "    if freq is not None:\n",
    "        # Set index for resampling, then take \"last\" per period for cumulative metrics.\n",
    "        out = out.set_index(\"date\").sort_index()\n",
    "        agg_map = {\n",
    "            \"received_cum\": \"last\",\n",
    "            \"allocated_cum\": \"last\",\n",
    "            \"backlog_available\": \"last\",\n",
    "        }\n",
    "        if compute_weighted:\n",
    "            agg_map.update(\n",
    "                {\n",
    "                    \"received_weighted_cum\": \"last\",\n",
    "                    \"allocated_weighted_cum\": \"last\",\n",
    "                    \"backlog_weighted\": \"last\",\n",
    "                }\n",
    "            )\n",
    "        out = out.resample(freq).agg(agg_map).dropna(how=\"all\").reset_index()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: build_daily_panel()\n",
    "# -------------------------------------------------------------\n",
    "def build_daily_panel(\n",
    "    typed: pd.DataFrame,\n",
    "    start: pd.Timestamp | None = None,\n",
    "    end: pd.Timestamp | None = None,\n",
    "    *,\n",
    "    pad_days: int = 14,\n",
    "    fallback_to_all_dates: bool = True,\n",
    "    # Pass-through options to backlog & WIP builders\n",
    "    backlog_kwargs: dict | None = None,\n",
    "    wip_kwargs: dict | None = None,\n",
    "    # Panel calendar options (also forwarded into backlog unless overridden there)\n",
    "    exclude_weekends: bool = False,\n",
    "    holidays: list | pd.Series | None = None,\n",
    "    backlog_freq: str | None = None,  # e.g. 'W-FRI', 'W-MON', 'MS'\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a fully-featured daily staff panel for modelling and analytics.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    This function combines outputs from:\n",
    "      - build_event_log()     → daily operational events (e.g., newcase, legal, sign-off)\n",
    "      - build_wip_series()    → daily work-in-progress (active cases, workloads)\n",
    "      - build_backlog_series()→ daily system backlog (received minus allocated)\n",
    "    into one unified dataset at the **staff × date** level.\n",
    "\n",
    "    Calendar controls\n",
    "    -----------------\n",
    "    exclude_weekends : if True, panel dates will exclude Saturdays/Sundays\n",
    "    holidays         : iterable of dates marked as bank holidays in the panel;\n",
    "                       passed to backlog as exclusions too (unless overridden).\n",
    "    backlog_freq     : resampling frequency for backlog only (e.g., 'W-FRI', 'MS').\n",
    "                       Daily panel remains daily (or business-day if exclude_weekends=True).\n",
    "\n",
    "    Horizon:\n",
    "    If `start` and `end` are not provided, the function automatically determines\n",
    "    the date range using `date_horizon()` based on your project’s rule:\n",
    "      start := earliest dt_received_inv\n",
    "      end   := latest `dt_pg_signoff` (+ padding of `pad_days`)\n",
    "    Set fallback_to_all_dates=True to allow scanning all dt_* if primaries are missing.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Event flags derived from build_event_log(): newcase, alloc_team, sent_to_ca,\n",
    "      legal_request, legal_reject, legal_approval, pg_signoff, court_order, closed, flagged.\n",
    "    - Compact flags provided: event_newcase, event_legal, event_court, event_pg_signoff,\n",
    "      event_sent_to_ca, event_flagged.\n",
    "    - WIP uses dt_alloc_invest → earliest(dt_close, dt_pg_signoff, end).\n",
    "\n",
    "       typed : pd.DataFrame\n",
    "        Feature-engineered dataframe from `engineer()`, typically filtered\n",
    "        to reallocated cases.\n",
    "        Must include:\n",
    "          - Identifiers: `case_id`, `staff_id`, `team`, `role`, `fte`\n",
    "          - Core dates:  `dt_received_inv`, `dt_alloc_invest`, `dt_pg_signoff`,\n",
    "                         `dt_close` (and optionally legal & court milestones)\n",
    "        Optional columns (used if present):\n",
    "          - `weighting`, `status`, `case_type`, `concern_type`,\n",
    "            `days_to_pg_signoff`, etc.\n",
    "\n",
    "    start, end : pd.Timestamp | None, default None\n",
    "        Reporting horizon. If not given, derived automatically from `date_horizon()`.\n",
    "\n",
    "    pad_days : int, default 14\n",
    "        Number of days to extend the end horizon when deriving automatically.\n",
    "\n",
    "    fallback_to_all_dates : bool, default True\n",
    "        When true, allows `date_horizon()` to use all dt_* columns if the primary\n",
    "        (received / PG sign-off) columns are missing or incomplete.\n",
    "\n",
    "    backlog_kwargs : dict | None\n",
    "        Extra keyword arguments forwarded to `build_backlog_series()`.\n",
    "        Examples:\n",
    "            {'compute_weighted': True, 'clip_zero': True,\n",
    "             'exclude_weekends': False, 'holidays': holidays,\n",
    "             'freq': 'W-FRI'}\n",
    "\n",
    "    wip_kwargs : dict | None\n",
    "        Extra keyword arguments forwarded to `build_wip_series()`.\n",
    "        Example:\n",
    "            {'pad_days': 14, 'fallback_to_all_dates': True}\n",
    "\n",
    "    exclude_weekends : bool, default False\n",
    "        If True, weekends (Saturday/Sunday) are excluded from the daily panel\n",
    "        and from the backlog calculation.\n",
    "\n",
    "    holidays : list | pd.Series | None, default None\n",
    "        List or Series of public holidays to exclude from the panel timeline\n",
    "        and mark with `bank_holiday = 1`.\n",
    "\n",
    "    backlog_freq : str | None, default None\n",
    "        Optional resampling frequency for backlog only.\n",
    "        Examples: 'W-FRI' (weekly, Friday close), 'MS' (month-start).\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    Returns\n",
    "    -----------------------------------------------------------------------\n",
    "    tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "        (daily, backlog, events)\n",
    "\n",
    "        **daily** : pd.DataFrame\n",
    "        One row per (date × staff × team), containing:\n",
    "          - Workload:  `wip`, `wip_load`\n",
    "          - Backlog context: `backlog_available`\n",
    "          - Event flags: `event_newcase`, `event_legal`, `event_court`,\n",
    "                         `event_pg_signoff`, `event_sent_to_ca`, `event_flagged`\n",
    "          - Calendar features: `dow`, `season`, `term_flag`, `bank_holiday`\n",
    "          - Tenure features: `weeks_since_start`, `is_new_starter`\n",
    "          - Temporal context: `time_since_last_pickup`\n",
    "\n",
    "        **backlog** : pd.DataFrame\n",
    "        System-level backlog series built by `build_backlog_series()` with optional\n",
    "        business-day or weekly/monthly resampling.\n",
    "\n",
    "        **events** : pd.DataFrame\n",
    "        Event log built by `build_event_log()`, containing granular dated events\n",
    "        per staff, case, and team.\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    Examples\n",
    "    -----------------------------------------------------------------------\n",
    "    >>> import pandas as pd\n",
    "    >>> typed = pd.DataFrame({\n",
    "    ...     'case_id': ['C1','C2'],\n",
    "    ...     'investigator': ['Alice','Bob'],\n",
    "    ...     'team': ['T1','T1'],\n",
    "    ...     'role': ['Investigator','Investigator'],\n",
    "    ...     'fte': [1.0, 0.8],\n",
    "    ...     'staff_id': ['S1','S2'],\n",
    "    ...     'dt_received_inv': [pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-02')],\n",
    "    ...     'dt_alloc_invest': [pd.Timestamp('2025-01-02'), pd.Timestamp('2025-01-03')],\n",
    "    ...     'dt_pg_signoff': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_close': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_legal_req_1': [pd.NaT, pd.Timestamp('2025-01-04')],\n",
    "    ...     'dt_legal_approval': [pd.NaT, pd.NaT],\n",
    "    ...     'dt_date_of_order': [pd.NaT, pd.NaT],\n",
    "    ... })\n",
    "    >>> start, end = pd.Timestamp('2025-01-01'), pd.Timestamp('2025-01-05')\n",
    "    >>> daily, backlog, events = build_daily_panel(\n",
    "    ...     typed,\n",
    "    ...     start=start,\n",
    "    ...     end=end,\n",
    "    ...     exclude_weekends=True,\n",
    "    ...     holidays=[pd.Timestamp('2025-01-03')],\n",
    "    ...     backlog_freq='W-FRI',\n",
    "    ...     backlog_kwargs={'compute_weighted': True}\n",
    "    ... )\n",
    "    >>> # Daily panel has one row per staff per day\n",
    "    >>> set({'date','staff_id','team','fte','wip','event_newcase'}).issubset(daily.columns)\n",
    "    True\n",
    "    >>> # Backlog matches the number of working days\n",
    "    >>> len(backlog) <= (end - start).days + 1\n",
    "    True\n",
    "    >>> # Event log contains expected event types\n",
    "    >>> {'newcase','legal_request'}.issubset(set(events['event'].unique())) if not events.empty else True\n",
    "    True\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    backlog_kwargs = {} if backlog_kwargs is None else dict(backlog_kwargs)\n",
    "    wip_kwargs = {} if wip_kwargs is None else dict(wip_kwargs)\n",
    "\n",
    "    # 1 Determine horizon (uses your updated rule) ---\n",
    "    if start is None or end is None:\n",
    "        s, e = date_horizon(\n",
    "            typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates\n",
    "        )\n",
    "        start = s if start is None else start\n",
    "        end = e if end is None else end\n",
    "    start = pd.to_datetime(start).normalize()\n",
    "    end = pd.to_datetime(end).normalize()\n",
    "\n",
    "    # 2 Build the three core artefacts from the pipeline (events, WIP, backlog)\n",
    "    events = build_event_log(\n",
    "        typed, pad_days=pad_days, fallback_to_all_dates=fallback_to_all_dates\n",
    "    )\n",
    "\n",
    "    # WIP stays daily across full horizon; the panel may later filter dates\n",
    "    wip = build_wip_series(typed, start=start, end=end, **wip_kwargs)\n",
    "\n",
    "    # Ensure panel-level calendar options are forwarded to backlog unless explicitly set\n",
    "    backlog_defaults = {\n",
    "        \"pad_days\": pad_days,\n",
    "        \"fallback_to_all_dates\": fallback_to_all_dates,\n",
    "        \"exclude_weekends\": exclude_weekends,\n",
    "        \"holidays\": holidays,\n",
    "        \"freq\": backlog_freq,\n",
    "    }\n",
    "    for k, v in backlog_defaults.items():\n",
    "        backlog_kwargs.setdefault(k, v)\n",
    "\n",
    "    backlog = build_backlog_series(typed, start=start, end=end, **backlog_kwargs)\n",
    "\n",
    "    # 3) Panel date index (daily or business-day)\n",
    "    date_index = pd.date_range(start, end, freq=\"D\")\n",
    "    if exclude_weekends:\n",
    "        date_index = date_index[date_index.weekday < 5]\n",
    "    if holidays is not None and len(pd.Index(holidays)) > 0:\n",
    "        hol = pd.to_datetime(pd.Index(holidays)).normalize()\n",
    "        date_index = date_index.difference(hol)\n",
    "    dates = pd.DataFrame({\"date\": date_index})\n",
    "\n",
    "    # 4) Build staff-date grid (all combinations)\n",
    "    staff = typed[[\"staff_id\", \"team\"]].drop_duplicates()\n",
    "    grid = (\n",
    "        staff.assign(_k=1)\n",
    "        .merge(pd.DataFrame({\"date\": date_index}).assign(_k=1), on=\"_k\", how=\"outer\")\n",
    "        .drop(columns=[\"_k\"])\n",
    "    )\n",
    "\n",
    "    # 5) Merge WIP data (wip & wip_load). If grid has filtered dates, merge naturally subsets.\n",
    "    grid = grid.merge(wip, on=[\"date\", \"staff_id\", \"team\"], how=\"left\")\n",
    "    for c, default in [(\"wip\", 0.0), (\"wip_load\", 0.0)]:\n",
    "        grid[c] = (\n",
    "            pd.to_numeric(grid.get(c, default), errors=\"coerce\")\n",
    "            .fillna(default)\n",
    "            .astype(float)\n",
    "        )\n",
    "\n",
    "    # 6) Pivot events → daily flags per staff\n",
    "    if not events.empty:\n",
    "        ev_flags = (\n",
    "            events.assign(flag=1)\n",
    "            .pivot_table(\n",
    "                index=[\"date\", \"staff_id\"],\n",
    "                columns=\"event\",\n",
    "                values=\"flag\",\n",
    "                aggfunc=\"max\",\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Merge at staff-day; team may differ if staff moved teams, but WIP merge above anchors team\n",
    "        grid = grid.merge(ev_flags, on=[\"date\", \"staff_id\"], how=\"left\")\n",
    "\n",
    "    # Ensure a stable set of event columns exists\n",
    "    event_cols = [\n",
    "        \"newcase\",\n",
    "        \"alloc_team\",\n",
    "        \"sent_to_ca\",\n",
    "        \"legal_request\",\n",
    "        \"legal_reject\",\n",
    "        \"legal_approval\",\n",
    "        \"pg_signoff\",\n",
    "        \"court_order\",\n",
    "        \"closed\",\n",
    "        \"flagged\",\n",
    "    ]\n",
    "    for c in event_cols:\n",
    "        grid[c] = grid.get(c, 0)\n",
    "        grid[c] = grid[c].fillna(0).astype(int)\n",
    "\n",
    "    # Compact event groupings useful for modelling\n",
    "    grid[\"event_newcase\"] = grid[\"newcase\"].astype(int)\n",
    "    grid[\"event_legal\"] = (\n",
    "        (grid[\"legal_request\"] + grid[\"legal_approval\"] + grid[\"legal_reject\"]) > 0\n",
    "    ).astype(int)\n",
    "    grid[\"event_court\"] = grid[\"court_order\"].astype(int)\n",
    "    grid[\"event_pg_signoff\"] = grid[\"pg_signoff\"].astype(int)\n",
    "    grid[\"event_sent_to_ca\"] = grid[\"sent_to_ca\"].astype(int)\n",
    "    grid[\"event_flagged\"] = grid[\"flagged\"].astype(int)\n",
    "\n",
    "    # 7) Days since last pickup (per staff)\n",
    "    grid = grid.sort_values([\"staff_id\", \"date\"])\n",
    "\n",
    "    def _days_since_last_pickup(series: pd.Series) -> pd.Series:\n",
    "        out, last = [], None\n",
    "        for i, v in enumerate(series):\n",
    "            if v == 1:\n",
    "                last = i\n",
    "                out.append(0)\n",
    "            else:\n",
    "                out.append(i - last if last is not None else pd.NA)\n",
    "        return pd.Series(out, index=series.index)\n",
    "\n",
    "    grid[\"time_since_last_pickup\"] = (\n",
    "        grid.groupby(\"staff_id\", group_keys=False)[\"event_newcase\"]\n",
    "        .apply(_days_since_last_pickup)\n",
    "        .fillna(99)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # 8) Calendar features\n",
    "    grid[\"dow\"] = grid[\"date\"].dt.day_name().str[:3]\n",
    "    grid[\"season\"] = grid[\"date\"].dt.month.map(month_to_season)\n",
    "    grid[\"term_flag\"] = grid[\"date\"].dt.month.map(is_term_month).astype(int)\n",
    "    # Bank holiday flag (1 if the date is in holidays)\n",
    "    if holidays is not None and len(pd.Index(holidays)) > 0:\n",
    "        hol = pd.to_datetime(pd.Index(holidays)).normalize()\n",
    "        grid[\"bank_holiday\"] = grid[\"date\"].isin(hol).astype(int)\n",
    "    else:\n",
    "        grid[\"bank_holiday\"] = 0\n",
    "\n",
    "    # 9) New starter (tenure) features (weeks since first allocation per staff)\n",
    "    first_alloc = (\n",
    "        typed.dropna(subset=[\"dt_alloc_invest\"])\n",
    "        .groupby(\"staff_id\")[\"dt_alloc_invest\"]\n",
    "        .min()\n",
    "        .rename(\"first_alloc\")\n",
    "    )\n",
    "    grid = grid.merge(first_alloc, on=\"staff_id\", how=\"left\")\n",
    "    grid[\"weeks_since_start\"] = (\n",
    "        ((grid[\"date\"] - grid[\"first_alloc\"]).dt.days // 7)\n",
    "        .fillna(0)\n",
    "        .clip(lower=0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    grid[\"is_new_starter\"] = (grid[\"weeks_since_start\"] < 4).astype(int)\n",
    "    grid = grid.drop(columns=[\"first_alloc\"])\n",
    "\n",
    "    # 10) Merge backlog (always by 'date'; backlog may be resampled)\n",
    "    # If backlog was resampled (e.g., weekly), forward-fill to panel dates.\n",
    "    if \"date\" in backlog.columns and backlog[\"date\"].is_monotonic_increasing:\n",
    "        back = backlog.set_index(\"date\").sort_index()\n",
    "        # Keep only the core columns we need (avoid accidental merges)\n",
    "        keep_cols = [\n",
    "            c\n",
    "            for c in back.columns\n",
    "            if c\n",
    "            in {\n",
    "                \"received_cum\",\n",
    "                \"allocated_cum\",\n",
    "                \"backlog_available\",\n",
    "                \"received_weighted_cum\",\n",
    "                \"allocated_weighted_cum\",\n",
    "                \"backlog_weighted\",\n",
    "            }\n",
    "        ]\n",
    "        back = back[keep_cols]\n",
    "        back = back.reindex(date_index, method=\"ffill\")  # align to panel calendar\n",
    "        back = back.reset_index().rename(columns={\"index\": \"date\"})\n",
    "    else:\n",
    "        back = backlog.copy()\n",
    "\n",
    "    grid = grid.merge(back, on=\"date\", how=\"left\")\n",
    "    grid[\"backlog_available\"] = pd.to_numeric(\n",
    "        grid.get(\"backlog_available\", 0.0), errors=\"coerce\"\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    # 11) Final tidy columns & order\n",
    "    cols = [\n",
    "        \"date\",\n",
    "        \"staff_id\",\n",
    "        \"team\",\n",
    "        \"role\",\n",
    "        \"fte\",\n",
    "        \"wip\",\n",
    "        \"wip_load\",\n",
    "        \"time_since_last_pickup\",\n",
    "        \"weeks_since_start\",\n",
    "        \"is_new_starter\",\n",
    "        \"backlog_available\",\n",
    "        \"term_flag\",\n",
    "        \"season\",\n",
    "        \"dow\",\n",
    "        \"bank_holiday\",\n",
    "        \"event_newcase\",\n",
    "        \"event_legal\",\n",
    "        \"event_court\",\n",
    "        \"event_pg_signoff\",\n",
    "        \"event_sent_to_ca\",\n",
    "        \"event_flagged\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in grid.columns]  # be tolerant\n",
    "    daily = grid[cols].sort_values([\"staff_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    return daily, backlog, events\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: summarise_daily_panel()\n",
    "# -------------------------------------------------------------\n",
    "def summarise_daily_panel(\n",
    "    daily: pd.DataFrame,\n",
    "    by: list[str] = (\"date\", \"team\"),\n",
    "    *,\n",
    "    freq: str | None = None,\n",
    "    # How to aggregate each metric; sensible defaults provided\n",
    "    agg_map: dict | None = None,\n",
    "    # If resampling, how to aggregate cumulative-style fields\n",
    "    resample_cum_last: tuple[str, ...] = (\"backlog_available\",),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise the daily staff panel by date/team (or any grouping).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    daily : pd.DataFrame\n",
    "        Output of build_daily_panel()[0], with columns like:\n",
    "          ['date','staff_id','team','wip','wip_load','backlog_available',\n",
    "           'event_newcase','event_legal','event_court','event_pg_signoff',\n",
    "           'event_sent_to_ca','event_flagged','time_since_last_pickup', ...]\n",
    "    by : list[str], default ('date','team')\n",
    "        Grouping columns. Must include 'date' if you want a time series.\n",
    "        Examples: ('date',), ('date','team'), ('date','team','role')\n",
    "    freq : str | None, default None\n",
    "        Optional resampling frequency over time *after* grouping.\n",
    "        Examples: 'W-FRI', 'MS'. If None, returns daily resolution.\n",
    "    agg_map : dict | None, default None\n",
    "        Custom aggregation map. If None, a sensible default is used:\n",
    "          - Sum counts/loads/events\n",
    "          - Mean backlog_available\n",
    "          - Median time_since_last_pickup\n",
    "          - Distinct staff_count\n",
    "    resample_cum_last : tuple[str,...], default ('backlog_available',)\n",
    "        For resampling, fields treated as *cumulative/stateful* and aggregated\n",
    "        via 'last' per period (e.g., backlog_available).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per group (and per period if resampled). Includes:\n",
    "          - wip_sum, wip_load_sum\n",
    "          - backlog_available_mean (and backlog_available_last if resampled)\n",
    "          - events counts: newcase, legal, court, pg_signoff, sent_to_ca, flagged\n",
    "          - staff_count (distinct staff_id)\n",
    "          - time_since_last_pickup_median\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # team-level daily\n",
    "    >>> team_daily = summarise_daily_panel(daily, by=['date','team'])\n",
    "    >>> # team-level weekly (Friday)\n",
    "    >>> team_weekly = summarise_daily_panel(daily, by=['date','team'], freq='W-FRI')\n",
    "    \"\"\"\n",
    "    if \"date\" not in by:\n",
    "        raise ValueError(\n",
    "            \"`by` must include 'date' to preserve time order (or set freq=None for a non-time summary).\"\n",
    "        )\n",
    "\n",
    "    # Default aggregation plan\n",
    "    default_agg = {\n",
    "        \"wip\": \"sum\",\n",
    "        \"wip_load\": \"sum\",\n",
    "        \"backlog_available\": \"mean\",  # daily mean backlog across staff on that date\n",
    "        \"event_newcase\": \"sum\",\n",
    "        \"event_legal\": \"sum\",\n",
    "        \"event_court\": \"sum\",\n",
    "        \"event_pg_signoff\": \"sum\",\n",
    "        \"event_sent_to_ca\": \"sum\",\n",
    "        \"event_flagged\": \"sum\",\n",
    "        \"time_since_last_pickup\": \"median\",\n",
    "        \"staff_id\": pd.Series.nunique,  # distinct headcount working that day\n",
    "    }\n",
    "    if agg_map is not None:\n",
    "        default_agg.update(agg_map)\n",
    "\n",
    "    # Group and aggregate on the daily grid\n",
    "    grouped = (\n",
    "        daily.groupby(list(by), dropna=False)\n",
    "        .agg(default_agg)\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"wip\": \"wip_sum\",\n",
    "                \"wip_load\": \"wip_load_sum\",\n",
    "                \"backlog_available\": \"backlog_available_mean\",\n",
    "                \"event_newcase\": \"event_newcase_sum\",\n",
    "                \"event_legal\": \"event_legal_sum\",\n",
    "                \"event_court\": \"event_court_sum\",\n",
    "                \"event_pg_signoff\": \"event_pg_signoff_sum\",\n",
    "                \"event_sent_to_ca\": \"event_sent_to_ca_sum\",\n",
    "                \"event_flagged\": \"event_flagged_sum\",\n",
    "                \"time_since_last_pickup\": \"time_since_last_pickup_median\",\n",
    "                \"staff_id\": \"staff_count\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    if freq is None:\n",
    "        # Return daily/grouped summary as-is\n",
    "        return grouped.sort_values(by).reset_index(drop=True)\n",
    "\n",
    "    # Resampling: we need a DatetimeIndex aligned on 'date'\n",
    "    out = []\n",
    "    other_keys = [k for k in by if k != \"date\"]\n",
    "    for keys, sub in grouped.groupby(other_keys, dropna=False):\n",
    "        # Ensure consistent frame and index\n",
    "        sub = sub.sort_values(\"date\").set_index(\"date\")\n",
    "\n",
    "        # For numeric fields, decide resampling rule:\n",
    "        # - For cumulative/state-like fields -> last\n",
    "        # - For flow-like fields (counts) -> sum\n",
    "        numeric_cols = sub.select_dtypes(include=\"number\").columns.tolist()\n",
    "\n",
    "        # Prepare aggregation map for resample\n",
    "        resample_agg = {}\n",
    "        for col in numeric_cols:\n",
    "            if col in resample_cum_last:\n",
    "                resample_agg[col] = \"last\"\n",
    "            else:\n",
    "                resample_agg[col] = \"sum\"\n",
    "\n",
    "        sub_res = sub.resample(freq).agg(resample_agg)\n",
    "\n",
    "        # Keep grouping keys\n",
    "        if not isinstance(keys, tuple):\n",
    "            keys = (keys,)\n",
    "        for k, v in zip(other_keys, keys):\n",
    "            sub_res[k] = v\n",
    "\n",
    "        out.append(sub_res.reset_index())\n",
    "\n",
    "    resampled = pd.concat(out, ignore_index=True) if out else grouped\n",
    "    return resampled.sort_values(\n",
    "        by if freq is None else ([\"date\"] + other_keys)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# daily, backlog, events = build_daily_panel(\n",
    "#     typed,\n",
    "#     # optional: let it auto-derive start/end via date_horizon()\n",
    "#     exclude_weekends=True,\n",
    "#     holidays=[pd.Timestamp('2025-05-05'), pd.Timestamp('2025-08-25')],  # UK BHs (example)\n",
    "#     backlog_freq='W-FRI',  # weekly backlog, last value each Friday\n",
    "#     backlog_kwargs={'compute_weighted': True, 'clip_zero': True},  # weighted backlog too\n",
    "#     wip_kwargs={'pad_days': 14, 'fallback_to_all_dates': True}\n",
    "# )\n",
    "\n",
    "\n",
    "# # 1) Team-level daily\n",
    "# team_daily = summarise_daily_panel(daily, by=['date','team'])\n",
    "\n",
    "# # 2) Team-level weekly (Friday), treating backlog as a level (last-of-week)\n",
    "# team_weekly = summarise_daily_panel(\n",
    "#     daily,\n",
    "#     by=['date','team'],\n",
    "#     freq='W-FRI',\n",
    "#     resample_cum_last=('backlog_available',)  # keep as 'last' per week\n",
    "# )\n",
    "\n",
    "# # 3) Overall totals per day (collapse teams)\n",
    "# org_daily = summarise_daily_panel(daily, by=['date'])\n",
    "\n",
    "# # 4) Custom aggregation rules (e.g., use max backlog across staff instead of mean)\n",
    "# custom = summarise_daily_panel(\n",
    "#     daily,\n",
    "#     by=['date','team'],\n",
    "#     agg_map={'backlog_available': 'max'}\n",
    "# )\n",
    "# # Save events DataFrame to CSV\n",
    "# custom.to_csv(OUT_DIR / \"Custom_Summary.csv\", index=False)\n",
    "# print(custom)\n",
    "\n",
    "\n",
    "# --- Interval Analysis: new code (non-invasive) ---\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional, Dict, Any\n",
    "\n",
    "y = 4  # Number of years for analysis to start with\n",
    "\n",
    "# Meteorological seasons\n",
    "_SEASON_MAP = {\n",
    "    12: \"winter\",\n",
    "    1: \"winter\",\n",
    "    2: \"winter\",\n",
    "    3: \"spring\",\n",
    "    4: \"spring\",\n",
    "    5: \"spring\",\n",
    "    6: \"summer\",\n",
    "    7: \"summer\",\n",
    "    8: \"summer\",\n",
    "    9: \"autumn\",\n",
    "    10: \"autumn\",\n",
    "    11: \"autumn\",\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "\n",
    "# tiny config (which months count as term; how many weeks someone is a “new starter”).\n",
    "class IntervalFlags:\n",
    "    term_months: Iterable[int] = (1, 4, 7, 10)\n",
    "    new_starter_weeks: int = 12\n",
    "\n",
    "\n",
    "def _to_date(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.normalize()\n",
    "\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, cols: Iterable[str]) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    return df\n",
    "\n",
    "\n",
    "def _bool(x) -> pd.Series:\n",
    "    return pd.Series(x, dtype=\"boolean\")\n",
    "\n",
    "\n",
    "class IntervalAnalysis:\n",
    "    \"\"\"Extension utilities for case-level time interval analysis (read-only, additive).\"\"\"\n",
    "\n",
    "    REQUIRED_COLUMNS = [\n",
    "        \"date\",\n",
    "        \"staff_id\",\n",
    "        \"team\",\n",
    "        \"case_id\",\n",
    "        \"case_type\",\n",
    "        \"concern_type\",\n",
    "        \"status\",\n",
    "        \"dt_alloc_invest\",\n",
    "        \"dt_pg_signoff\",\n",
    "        \"dt_received_inv\",\n",
    "        \"dt_alloc_team\",\n",
    "        \"dt_close\",\n",
    "        \"dt_sent_to_ca\",\n",
    "        \"days_to_pg_signoff\",\n",
    "        \"fte\",\n",
    "        \"weighting\",\n",
    "        \"wip\",\n",
    "        \"wip_load\",\n",
    "        \"time_since_last_pickup\",\n",
    "        \"weeks_since_start\",\n",
    "        \"is_new_starter\",\n",
    "        \"backlog_available\",\n",
    "        \"term_flag\",\n",
    "        \"season\",\n",
    "        \"dow\",\n",
    "        \"bank_holiday\",\n",
    "        \"event_newcase\",\n",
    "        \"event_legal\",\n",
    "        \"event_court\",\n",
    "        \"event_pg_signoff\",\n",
    "        \"event_sent_to_ca\",\n",
    "        \"event_flagged\",\n",
    "        \"backlog\",  # <-- numeric backlog count per date\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def build_interval_frame(\n",
    "        raw: pd.DataFrame,\n",
    "        *,\n",
    "        backlog_series: Optional[\n",
    "            pd.DataFrame\n",
    "        ] = None,  # expects cols ['date','backlog'] if provided\n",
    "        bank_holidays: Optional[Iterable[pd.Timestamp | str]] = None,\n",
    "        flags: IntervalFlags = IntervalFlags(),\n",
    "        default_date_from: str | pd.Timestamp | None = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Construct a dataframe matching the requested schema, with a numeric 'backlog' column.\n",
    "        This DOES NOT modify existing notebook functions; it can be used alongside them.\n",
    "        If a backlog series is not provided, it is computed per observed 'date' as:\n",
    "            backlog(date) = #cases with (dt_received_inv <= date) and (dt_close isna or dt_close > date)\n",
    "        \"\"\"\n",
    "        df = raw.copy()\n",
    "\n",
    "        date_cols = [\n",
    "            \"date\",\n",
    "            \"dt_alloc_invest\",\n",
    "            \"dt_pg_signoff\",\n",
    "            \"dt_received_inv\",\n",
    "            \"dt_alloc_team\",\n",
    "            \"dt_close\",\n",
    "            \"dt_sent_to_ca\",\n",
    "        ]\n",
    "        df = _ensure_columns(\n",
    "            df,\n",
    "            date_cols\n",
    "            + [\n",
    "                \"fte\",\n",
    "                \"weighting\",\n",
    "                \"status\",\n",
    "                \"case_type\",\n",
    "                \"concern_type\",\n",
    "                \"team\",\n",
    "                \"staff_id\",\n",
    "                \"case_id\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        for c in date_cols:\n",
    "            df[c] = _to_date(df[c])\n",
    "\n",
    "        # Observation date default\n",
    "        if \"date\" not in raw.columns or df[\"date\"].isna().all():\n",
    "            df[\"date\"] = df[\"dt_alloc_invest\"]\n",
    "        fallback = df[\"dt_received_inv\"].where(df[\"date\"].isna(), df[\"date\"])\n",
    "        df[\"date\"] = df[\"date\"].fillna(df[\"dt_alloc_invest\"]).fillna(fallback)\n",
    "\n",
    "        # Numeric defaults\n",
    "        df[\"fte\"] = pd.to_numeric(df[\"fte\"], errors=\"coerce\").fillna(1.0)\n",
    "        df[\"weighting\"] = pd.to_numeric(df[\"weighting\"], errors=\"coerce\").fillna(1.0)\n",
    "\n",
    "        # Primary interval(s)\n",
    "        df[\"days_to_pg_signoff\"] = (\n",
    "            ((df[\"dt_pg_signoff\"] - df[\"dt_alloc_invest\"]).dt.days)\n",
    "            .astype(\"float\")\n",
    "            .replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "        )\n",
    "\n",
    "        # WIP flag\n",
    "        df[\"wip\"] = _bool(\n",
    "            (df[\"dt_alloc_invest\"].notna())\n",
    "            & (df[\"date\"].notna())\n",
    "            & (df[\"date\"] >= df[\"dt_alloc_invest\"])\n",
    "            & (df[\"dt_close\"].isna() | (df[\"date\"] < df[\"dt_close\"]))\n",
    "        )\n",
    "        df[\"wip_load\"] = (\n",
    "            df[\"fte\"] * df[\"weighting\"] * df[\"wip\"].fillna(False).astype(float)\n",
    "        ).astype(float)\n",
    "\n",
    "        # Inter-pickup (gap between allocations) per staff\n",
    "        if df[\"staff_id\"].notna().any():\n",
    "            df = df.sort_values([\"staff_id\", \"dt_alloc_invest\"])\n",
    "            df[\"time_since_last_pickup\"] = (\n",
    "                df.groupby(\"staff_id\")[\"dt_alloc_invest\"].diff().dt.days.astype(\"float\")\n",
    "            )\n",
    "        else:\n",
    "            df[\"time_since_last_pickup\"] = np.nan\n",
    "\n",
    "        # Weeks since start\n",
    "        start_date = (\n",
    "            _to_date(pd.Series(pd.Timestamp(default_date_from))).iloc[0]\n",
    "            if default_date_from\n",
    "            else df[\"date\"].min()\n",
    "        )\n",
    "        df[\"weeks_since_start\"] = ((df[\"date\"] - start_date).dt.days / 7.0).astype(\n",
    "            float\n",
    "        )\n",
    "\n",
    "        # New starter flag (weeks from first allocation)\n",
    "        if df[\"staff_id\"].notna().any():\n",
    "            first_alloc = df.groupby(\"staff_id\")[\"dt_alloc_invest\"].transform(\"min\")\n",
    "            weeks_from_first = ((df[\"date\"] - first_alloc).dt.days / 7.0).astype(float)\n",
    "            df[\"is_new_starter\"] = _bool(\n",
    "                weeks_from_first <= float(flags.new_starter_weeks)\n",
    "            )\n",
    "        else:\n",
    "            df[\"is_new_starter\"] = _bool(False)\n",
    "\n",
    "        # Backlog availability flag\n",
    "        df[\"backlog_available\"] = _bool(\n",
    "            (df[\"dt_received_inv\"].notna())\n",
    "            & (df[\"date\"].notna())\n",
    "            & (df[\"date\"] >= df[\"dt_received_inv\"])\n",
    "            & (df[\"dt_close\"].isna() | (df[\"date\"] < df[\"dt_close\"]))\n",
    "        )\n",
    "\n",
    "        # Term/seasonality\n",
    "        df[\"term_flag\"] = _bool(\n",
    "            df[\"date\"].dt.month.isin(set(int(m) for m in flags.term_months))\n",
    "        )\n",
    "        df[\"season\"] = df[\"date\"].dt.month.map(_SEASON_MAP).astype(\"string\")\n",
    "        df[\"dow\"] = df[\"date\"].dt.day_name().astype(\"string\")\n",
    "\n",
    "        # Bank holiday\n",
    "        if bank_holidays is None:\n",
    "            df[\"bank_holiday\"] = _bool(False)\n",
    "        else:\n",
    "            bh = (\n",
    "                pd.to_datetime(pd.Series(list(bank_holidays)), errors=\"coerce\")\n",
    "                .dt.normalize()\n",
    "                .dropna()\n",
    "                .unique()\n",
    "            )\n",
    "            df[\"bank_holiday\"] = _bool(df[\"date\"].isin(bh))\n",
    "\n",
    "        # Event flags\n",
    "        status_text = (\n",
    "            df[\"status\"].astype(\"string\").str.lower().fillna(\"\")\n",
    "            + \" \"\n",
    "            + df[\"concern_type\"].astype(\"string\").str.lower().fillna(\"\")\n",
    "            + \" \"\n",
    "            + df[\"case_type\"].astype(\"string\").str.lower().fillna(\"\")\n",
    "        )\n",
    "        df[\"event_newcase\"] = _bool(df[\"date\"].eq(df[\"dt_received_inv\"]))\n",
    "        df[\"event_pg_signoff\"] = _bool(df[\"date\"].eq(df[\"dt_pg_signoff\"]))\n",
    "        df[\"event_sent_to_ca\"] = _bool(df[\"date\"].eq(df[\"dt_sent_to_ca\"]))\n",
    "        df[\"event_legal\"] = _bool(\n",
    "            status_text.str.contains(r\"\\\\blegal\\\\b|solicitor|attorney|advice\")\n",
    "        )\n",
    "        df[\"event_court\"] = _bool(\n",
    "            status_text.str.contains(r\"\\\\bcourt\\\\b|hearing|tribunal\")\n",
    "        )\n",
    "        df[\"event_flagged\"] = _bool(\n",
    "            status_text.str.contains(r\"\\\\bflag|priority|escalat\")\n",
    "        )\n",
    "\n",
    "        # --- Backlog numeric column ---\n",
    "        if backlog_series is not None and {\"date\", \"backlog\"}.issubset(\n",
    "            set(map(str.lower, backlog_series.columns.str.lower()))\n",
    "        ):\n",
    "            # Standardise columns and merge on date\n",
    "            bs = backlog_series.copy()\n",
    "            # normalise headers\n",
    "            cols_lower = {c: c.lower() for c in bs.columns}\n",
    "            bs.rename(columns={c: c.lower() for c in bs.columns}, inplace=True)\n",
    "            # ensure types\n",
    "            bs[\"date\"] = _to_date(bs[\"date\"])\n",
    "            bs[\"backlog\"] = pd.to_numeric(bs[\"backlog\"], errors=\"coerce\")\n",
    "            df = df.merge(\n",
    "                bs[[\"date\", \"backlog\"]].drop_duplicates(\"date\"), on=\"date\", how=\"left\"\n",
    "            )\n",
    "        else:\n",
    "            # Compute per observed 'date' (count of outstanding cases)\n",
    "            # backlog(date) = sum( dt_received_inv <= date and (dt_close isna or dt_close > date) )\n",
    "            # We'll compute on the set of dates that appear in df['date'].\n",
    "            dates = df[\"date\"].dropna().sort_values().unique()\n",
    "            # Pre-calc arrays for vectorised comparison\n",
    "            recv = df[\"dt_received_inv\"].values\n",
    "            close = df[\"dt_close\"].values\n",
    "            # For memory safety on very large data, fall back to a groupby boolean sum.\n",
    "            # Here we try a straightforward loop over unique dates.\n",
    "            bmap = {}\n",
    "            for d in dates:\n",
    "                # mask = (recv <= d) & (np.isnan(close) | (close > d))\n",
    "                mask = (recv <= d) & (pd.isna(close) | (close > d))\n",
    "                bmap[d] = int(mask.sum())\n",
    "            df[\"backlog\"] = df[\"date\"].map(bmap).astype(\"float\")\n",
    "\n",
    "        # Ensure all required columns exist & order\n",
    "        df = _ensure_columns(df, IntervalAnalysis.REQUIRED_COLUMNS)\n",
    "        df = df[IntervalAnalysis.REQUIRED_COLUMNS].copy()\n",
    "\n",
    "        # Dtypes\n",
    "        for c in [\n",
    "            \"wip\",\n",
    "            \"is_new_starter\",\n",
    "            \"backlog_available\",\n",
    "            \"term_flag\",\n",
    "            \"bank_holiday\",\n",
    "            \"event_newcase\",\n",
    "            \"event_legal\",\n",
    "            \"event_court\",\n",
    "            \"event_pg_signoff\",\n",
    "            \"event_sent_to_ca\",\n",
    "            \"event_flagged\",\n",
    "        ]:\n",
    "            df[c] = df[c].astype(\"boolean\")\n",
    "        for c in [\n",
    "            \"days_to_pg_signoff\",\n",
    "            \"fte\",\n",
    "            \"weighting\",\n",
    "            \"wip_load\",\n",
    "            \"time_since_last_pickup\",\n",
    "            \"weeks_since_start\",\n",
    "            \"backlog\",\n",
    "        ]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        for c in [\n",
    "            \"staff_id\",\n",
    "            \"team\",\n",
    "            \"case_id\",\n",
    "            \"case_type\",\n",
    "            \"concern_type\",\n",
    "            \"status\",\n",
    "            \"season\",\n",
    "            \"dow\",\n",
    "        ]:\n",
    "            df[c] = df[c].astype(\"string\")\n",
    "        for c in [\n",
    "            \"date\",\n",
    "            \"dt_alloc_invest\",\n",
    "            \"dt_pg_signoff\",\n",
    "            \"dt_received_inv\",\n",
    "            \"dt_alloc_team\",\n",
    "            \"dt_close\",\n",
    "            \"dt_sent_to_ca\",\n",
    "        ]:\n",
    "            df[c] = _to_date(df[c])\n",
    "\n",
    "        return df\n",
    "\n",
    "    # ---- Analysis helpers (year focus) ----\n",
    "    @staticmethod\n",
    "    def filter_year(\n",
    "        df: pd.DataFrame, y: int = y, anchor: Optional[pd.Timestamp] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        if anchor is None:\n",
    "            anchor = pd.Timestamp.today().normalize()\n",
    "        start = anchor - pd.Timedelta(days=y * 365)\n",
    "        return df[df[\"date\"].between(start, anchor, inclusive=\"both\")].copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def interval_columns_available(df: pd.DataFrame) -> Dict[str, pd.Series]:\n",
    "        out = {}\n",
    "        if \"days_to_pg_signoff\" in df.columns:\n",
    "            out[\"days_to_pg_signoff\"] = df[\"days_to_pg_signoff\"]\n",
    "        if {\"dt_close\", \"dt_alloc_invest\"}.issubset(df.columns):\n",
    "            out[\"days_alloc_to_close\"] = (\n",
    "                df[\"dt_close\"] - df[\"dt_alloc_invest\"]\n",
    "            ).dt.days.astype(\"float\")\n",
    "        if {\"dt_sent_to_ca\", \"dt_alloc_invest\"}.issubset(df.columns):\n",
    "            out[\"days_alloc_to_sent_to_ca\"] = (\n",
    "                df[\"dt_sent_to_ca\"] - df[\"dt_alloc_invest\"]\n",
    "            ).dt.days.astype(\"float\")\n",
    "        if \"time_since_last_pickup\" in df.columns:\n",
    "            out[\"inter_pickup_days\"] = df[\"time_since_last_pickup\"]\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def distribution_summary(s: pd.Series) -> Dict[str, Any]:\n",
    "        s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "        if s.empty:\n",
    "            return {\"count\": 0}\n",
    "        q = s.quantile([0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        return {\n",
    "            \"count\": int(s.size),\n",
    "            \"mean\": float(s.mean()),\n",
    "            \"std\": float(s.std(ddof=1)) if s.size > 1 else 0.0,\n",
    "            \"min\": float(s.min()),\n",
    "            \"p10\": float(q.loc[0.1]),\n",
    "            \"p25\": float(q.loc[0.25]),\n",
    "            \"p50\": float(q.loc[0.5]),\n",
    "            \"p75\": float(q.loc[0.75]),\n",
    "            \"p90\": float(q.loc[0.9]),\n",
    "            \"max\": float(s.max()),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def analyse_interval_distributions(\n",
    "        df: pd.DataFrame,\n",
    "        *,\n",
    "        anchor: Optional[pd.Timestamp] = None,\n",
    "        by: Optional[list[str]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        dfl = IntervalAnalysis.filter_year(df, y=y, anchor=anchor)\n",
    "        metrics = IntervalAnalysis.interval_columns_available(dfl)\n",
    "\n",
    "        if not by:\n",
    "            return {\n",
    "                name: IntervalAnalysis.distribution_summary(series)\n",
    "                for name, series in metrics.items()\n",
    "            }\n",
    "\n",
    "        # --- FIX: make group keys safe for dropna=False ---\n",
    "        import pandas as pd\n",
    "\n",
    "        safe_keys = []\n",
    "        for c in by:\n",
    "            s = dfl[c].astype(\"object\")  # avoid pandas \"string\" NA semantics\n",
    "            s = s.where(pd.notna(s), \"__NA__\")  # sentinel for missing category\n",
    "            safe_keys.append(s)\n",
    "\n",
    "        grouped = dfl.groupby(safe_keys, dropna=False)\n",
    "        # -----------------------------------------------\n",
    "\n",
    "        out = {}\n",
    "        for name, series in metrics.items():\n",
    "            blocks = {}\n",
    "            for gkey, idx in grouped.groups.items():\n",
    "                # normalise key to tuple and map sentinel back to None for readability\n",
    "                gkey = gkey if isinstance(gkey, tuple) else (gkey,)\n",
    "                gkey = tuple(None if x == \"__NA__\" else x for x in gkey)\n",
    "\n",
    "                subset = series.loc[idx]  # subset the precomputed Series by index\n",
    "                blocks[gkey] = IntervalAnalysis.distribution_summary(subset)\n",
    "            out[name] = blocks\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def monthly_trend(\n",
    "        df: pd.DataFrame,\n",
    "        metric: str = \"days_to_pg_signoff\",\n",
    "        *,\n",
    "        anchor: Optional[pd.Timestamp] = None,\n",
    "        agg: str = \"median\",\n",
    "        by: Optional[list[str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        dfl = IntervalAnalysis.filter_year(df, y=y, anchor=anchor)\n",
    "        if metric not in dfl.columns:\n",
    "            raise KeyError(f\"Metric '{metric}' not in dataframe.\")\n",
    "        dfl = dfl.copy()\n",
    "        dfl[\"yyyymm\"] = dfl[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "        def _aggfunc(x):\n",
    "            if isinstance(agg, str) and agg.startswith(\"p\") and agg[1:].isdigit():\n",
    "                q = int(agg[1:]) / 100.0\n",
    "                return x.quantile(q)\n",
    "            return getattr(x, agg)() if hasattr(x, agg) else x.median()\n",
    "\n",
    "        if by:\n",
    "            grp = (\n",
    "                dfl.groupby(by + [\"yyyymm\"])[metric]\n",
    "                .apply(_aggfunc)\n",
    "                .reset_index(name=metric)\n",
    "            )\n",
    "            grp = grp.sort_values(by + [\"yyyymm\"]).assign(\n",
    "                mom_delta=lambda g: g.groupby(by)[metric].diff()\n",
    "            )\n",
    "        else:\n",
    "            grp = (\n",
    "                dfl.groupby([\"yyyymm\"])[metric].apply(_aggfunc).reset_index(name=metric)\n",
    "            )\n",
    "            grp = grp.sort_values([\"yyyymm\"]).assign(\n",
    "                mom_delta=lambda g: g[metric].diff()\n",
    "            )\n",
    "        return grp\n",
    "\n",
    "    @staticmethod\n",
    "    def volatility_score(\n",
    "        df: pd.DataFrame,\n",
    "        metric: str = \"days_to_pg_signoff\",\n",
    "        *,\n",
    "        anchor: Optional[pd.Timestamp] = None,\n",
    "        freq: str = \"W\",\n",
    "        by: Optional[list[str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        dfl = IntervalAnalysis.filter_year(df, y=y, anchor=anchor)\n",
    "        if metric not in dfl.columns:\n",
    "            raise KeyError(f\"Metric '{metric}' not in dataframe.\")\n",
    "        dfl = dfl.set_index(\"date\")\n",
    "\n",
    "        if by:\n",
    "            pieces = []\n",
    "            for keys, g in dfl.groupby(by, dropna=False):\n",
    "                bucket = g[metric].resample(freq).median()\n",
    "                vol = bucket.std()\n",
    "                row = dict(zip(by, keys if isinstance(keys, tuple) else (keys,)))\n",
    "                row.update({\"metric\": metric, \"freq\": freq, \"volatility\": vol})\n",
    "                pieces.append(row)\n",
    "            return pd.DataFrame(pieces)\n",
    "        else:\n",
    "            bucket = dfl[metric].resample(freq).median()\n",
    "            return pd.DataFrame(\n",
    "                {\"metric\": [metric], \"freq\": [freq], \"volatility\": [bucket.std()]}\n",
    "            )\n",
    "\n",
    "\n",
    "# --- Usage (examples) ---\n",
    "# NOTE: Examples are commented out to avoid altering notebooks' execution flow.\n",
    "# You can un-comment and run after your usual pipeline steps.\n",
    "#\n",
    "# engineered = engineer(raw_df, colmap)                     # existing function\n",
    "# daily = build_daily_panel(engineered)                     # existing function\n",
    "# backlog_series = build_backlog_series(engineered)         # existing function (date, backlog)\n",
    "#\n",
    "# df_interval = IntervalAnalysis.build_interval_frame(\n",
    "#     engineered, backlog_series=backlog_series, bank_holidays=None\n",
    "# )\n",
    "# summaries = IntervalAnalysis.analyse_interval_distributions(df_interval, by=[\"team\"])\n",
    "# trend = IntervalAnalysis.monthly_trend(df_interval, metric=\"days_to_pg_signoff\", agg=\"median\", by=[\"team\"])\n",
    "# vol = IntervalAnalysis.volatility_score(df_interval, metric=\"inter_pickup_days\", freq=\"W\", by=[\"staff_id\"])\n",
    "\n",
    "\n",
    "# let volatility_score accept aliases / computed metrics\n",
    "def _volatility_score_safe(\n",
    "    df, metric: str = \"days_to_pg_signoff\", *, anchor=None, freq: str = \"W\", by=None\n",
    "):\n",
    "    dfl = IntervalAnalysis.filter_year(df, y=y, anchor=anchor)\n",
    "\n",
    "    # Allow aliases or compute-on-the-fly metrics\n",
    "    if metric not in dfl.columns:\n",
    "        if metric == \"inter_pickup_days\" and \"time_since_last_pickup\" in dfl.columns:\n",
    "            dfl = dfl.assign(inter_pickup_days=dfl[\"time_since_last_pickup\"])\n",
    "        elif metric == \"days_alloc_to_close\" and {\n",
    "            \"dt_close\",\n",
    "            \"dt_alloc_invest\",\n",
    "        }.issubset(dfl.columns):\n",
    "            dfl = dfl.assign(\n",
    "                days_alloc_to_close=(\n",
    "                    dfl[\"dt_close\"] - dfl[\"dt_alloc_invest\"]\n",
    "                ).dt.days.astype(\"float\")\n",
    "            )\n",
    "        elif metric == \"days_alloc_to_sent_to_ca\" and {\n",
    "            \"dt_sent_to_ca\",\n",
    "            \"dt_alloc_invest\",\n",
    "        }.issubset(dfl.columns):\n",
    "            dfl = dfl.assign(\n",
    "                days_alloc_to_sent_to_ca=(\n",
    "                    dfl[\"dt_sent_to_ca\"] - dfl[\"dt_alloc_invest\"]\n",
    "                ).dt.days.astype(\"float\")\n",
    "            )\n",
    "        else:\n",
    "            raise KeyError(f\"Metric '{metric}' not in dataframe and cannot be derived.\")\n",
    "    dfl = dfl.set_index(\"date\")\n",
    "\n",
    "    if by:\n",
    "        pieces = []\n",
    "        for keys, g in dfl.groupby(by, dropna=False):\n",
    "            bucket = g[metric].resample(freq).median()\n",
    "            vol = bucket.std()\n",
    "            row = dict(zip(by, keys if isinstance(keys, tuple) else (keys,)))\n",
    "            row.update({\"metric\": metric, \"freq\": freq, \"volatility\": vol})\n",
    "            pieces.append(row)\n",
    "        return pd.DataFrame(pieces)\n",
    "    else:\n",
    "        bucket = dfl[metric].resample(freq).median()\n",
    "        return pd.DataFrame(\n",
    "            {\"metric\": [metric], \"freq\": [freq], \"volatility\": [bucket.std()]}\n",
    "        )\n",
    "\n",
    "\n",
    "# Apply monkey patch\n",
    "IntervalAnalysis.volatility_score = staticmethod(_volatility_score_safe)\n",
    "print(\"Patched IntervalAnalysis.volatility_score to support metric aliases.\")\n",
    "\n",
    "\n",
    "# eda_opg.py\n",
    "# Advanced, object-oriented EDA utilities tailored to OPG Investigation Backlog data.\n",
    "# We estimate time to PG sign-off with a Kaplan–Meier curve so we can use both completed and still-open cases without bias. From the survival curve we read median and tail quantiles (P80/P90). Those feed capacity planning, SLAs, and discrete-event simulation. For example, High-risk cases show a longer P90, so adding experienced reviewers there reduces the tail and the visible backlog. We verify group differences with a log-rank test, and we export quantiles by case type as inputs to the microsimulation.\n",
    "\n",
    "# For each investigation case we care about “How long from when OPG receives the concern until PG signs it off?”. Many cases are still open on the day you analyse the data. Those open cases are right-censored: we know they’ve already taken at least X days, but we don’t yet know the final total. If you simply drop open cases or pretend they finished today, you’ll bias results (usually underestimating true times).\n",
    "\n",
    "from dataclasses import dataclass  # dataclass for a clear, typed configuration object\n",
    "from typing import (\n",
    "    List,\n",
    "    Tuple,\n",
    ")  # precise type hints for maintainability and IDE help\n",
    "import warnings  # to warn (not crash) when optional deps are missing\n",
    "\n",
    "\n",
    "# Optional scientific/statistical packages.\n",
    "try:\n",
    "    from lifelines import (\n",
    "        KaplanMeierFitter,\n",
    "    )  # survival analysis (censoring-aware) - non-parametric stats\n",
    "\n",
    "    _HAS_LIFELINES = True  # flag for availability\n",
    "except Exception:\n",
    "    _HAS_LIFELINES = False  # if not installed, we degrade gracefully\n",
    "\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency  # for Cramér’s V (categorical association)\n",
    "\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm  # for VIF (variance inflation factor) to remove multicolinearity\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "    _HAS_STATSMODELS = True\n",
    "except Exception:\n",
    "    _HAS_STATSMODELS = False\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Configuration for the EDA run\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EDAConfig:\n",
    "    \"\"\"\n",
    "    Configuration object declaring column names and options explicitly.\n",
    "    Make structure explicit to avoid 'magic strings' spread in code.\n",
    "    \"\"\"\n",
    "\n",
    "    id_col: str  # unique case identifier column\n",
    "    date_received: str  # date case received by OPG\n",
    "    date_allocated: str  # date case allocated to investigator (may be missing)\n",
    "    date_signed_off: str  # date case signed off (may be missing)\n",
    "    target_col: Optional[str] = None  # optional target (e.g. 'legal_review' 0/1)\n",
    "    numeric_cols: Optional[List[str]] = None  # numeric feature columns\n",
    "    categorical_cols: Optional[List[str]] = None  # categorical feature columns\n",
    "    time_index_col: Optional[str] = (\n",
    "        None  # column to use as time index for resampling (e.g. 'date_received')\n",
    "    )\n",
    "    team_col: Optional[str] = None  # team field for KPI grouping\n",
    "    risk_col: Optional[str] = None  # risk band\n",
    "    case_type_col: Optional[str] = None  # case type\n",
    "    region_col: Optional[str] = None  # region (optional)\n",
    "    # Defaults for time-series resampling and lag analysis\n",
    "    resample_rule: str = \"D\"  # daily by default\n",
    "    lag_list: Tuple[int, ...] = (1, 7, 14)  # lags to compute correlations at\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Main EDA class\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "class OPGInvestigationEDA:\n",
    "    \"\"\"\n",
    "    An object-oriented EDA toolkit for OPG investigations backlog problems.\n",
    "    Provides validated, reproducible, unit-testable methods for exploratory analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, config: EDAConfig) -> None:\n",
    "        \"\"\"\n",
    "        Store the data and config, and immediately derive standard fields (durations + censor flags).\n",
    "        \"\"\"\n",
    "        self.df = df.copy()  # do not mutate the caller's DataFrame\n",
    "        self.cfg = config  # keep typed configuration\n",
    "        self._derive_standard_fields()  # add days_to_alloc + censor flags up-front\n",
    "\n",
    "    # ---------------------------\n",
    "    # Core derivations and checks\n",
    "    # ---------------------------\n",
    "\n",
    "    def _derive_standard_fields(self) -> None:\n",
    "        \"\"\"\n",
    "        Derive commonly used interval variables and censor flags.\n",
    "        These are used across many EDA tasks in backlog analysis.\n",
    "        \"\"\"\n",
    "        # Ensure the three date columns exist and are datetime\n",
    "        for col in [\n",
    "            self.cfg.date_received,\n",
    "            self.cfg.date_allocated,\n",
    "            self.cfg.date_signed_off,\n",
    "        ]:\n",
    "            if col not in self.df.columns:\n",
    "                raise KeyError(\n",
    "                    f\"Expected date column missing: {col}\"\n",
    "                )  # fail early with a clear message\n",
    "            self.df[col] = pd.to_datetime(\n",
    "                self.df[col], errors=\"coerce\"\n",
    "            )  # coerce invalid strings to NaT\n",
    "\n",
    "        # Derive time-to-sign-off (days). For NaT (not signed_off yet), result is NaN.\n",
    "        self.df[\"days_to_signoff\"] = (\n",
    "            self.df[self.cfg.date_signed_off] - self.df[self.cfg.date_received]\n",
    "        ).dt.days\n",
    "\n",
    "        # Negative durations indicate data issues (signed_off before received). Set to NaN (to be investigated).\n",
    "        self.df.loc[self.df[\"days_to_signoff\"] < 0, \"days_to_signoff\"] = np.nan\n",
    "\n",
    "        # Censor flag for signed_off event: 1 if signed_off exists, else 0.\n",
    "        self.df[\"event_signed_off\"] = (\n",
    "            self.df[self.cfg.date_signed_off].notna().astype(int)\n",
    "        )\n",
    "\n",
    "        # Derive time-to-allocate(days) similarly; not always used, but often requested.\n",
    "        self.df[\"days_to_allocate\"] = (\n",
    "            self.df[self.cfg.date_allocated] - self.df[self.cfg.date_received]\n",
    "        ).dt.days\n",
    "        self.df.loc[self.df[\"days_to_allocate\"] < 0, \"days_to_allocate\"] = np.nan\n",
    "\n",
    "        # Censor flag for allocate: 1 if allocated date exists.\n",
    "        self.df[\"event_allocated\"] = (\n",
    "            self.df[self.cfg.date_allocated].notna().astype(int)\n",
    "        )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 0) Quick structural summary\n",
    "    # ---------------------------\n",
    "\n",
    "    def quick_overview(self) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "        Provide a compact dict of shape, dtypes, missingness, duplicate id counts, and time coverage.\n",
    "        \"\"\"\n",
    "        out: Dict[str, object] = {}  # container for multiple small facts\n",
    "        out[\"shape\"] = self.df.shape  # (rows, cols)\n",
    "        # .astype(int) → Converts True → 1 and False → 0.\n",
    "        out[\"dtypes\"] = self.df.dtypes.astype(\n",
    "            str\n",
    "        ).to_dict()  # map column -> dtype string\n",
    "        # mean(): When called on a boolean DataFrame (isna()), True is treated as 1 and False as 0. By default, mean() operates column-wise (axis=0), so it computes the fraction of missing values in each column.\n",
    "        # .sort_values(ascending=False): Sorts the resulting Series in descending order, so columns with the highest percentage of missing values appear first.\n",
    "        out[\"missing_pct\"] = (\n",
    "            self.df.isna().mean().sort_values(ascending=False)\n",
    "        )  # missingness percentage per column\n",
    "\n",
    "        # Duplicate ID counts (only if id column is provided)\n",
    "        if self.cfg.id_col in self.df.columns:\n",
    "            out[\"duplicate_ids\"] = int(\n",
    "                self.df.duplicated(subset=[self.cfg.id_col]).sum()\n",
    "            )\n",
    "        else:\n",
    "            out[\"duplicate_ids\"] = None\n",
    "\n",
    "        # Time coverage for the 3 key date columns\n",
    "        coverage = {}\n",
    "        for col in [\n",
    "            self.cfg.date_received,\n",
    "            self.cfg.date_allocated,\n",
    "            self.cfg.date_signed_off,\n",
    "        ]:\n",
    "            coverage[col] = (self.df[col].min(), self.df[col].max())\n",
    "        out[\"date_ranges\"] = coverage\n",
    "\n",
    "        # Class balance if target present (e.g. legal_review)\n",
    "        if self.cfg.target_col and self.cfg.target_col in self.df:\n",
    "            # Count occurrences of each class, including NaN\n",
    "            out[\"target_counts\"] = (\n",
    "                self.df[self.cfg.target_col].value_counts(dropna=False).to_dict()\n",
    "            )\n",
    "            # returns percentage-like values (fractions of total count). Useful when you want distribution instead of absolute counts. You can multiply by 100 to get percentages:\n",
    "            out[\"target_share\"] = (\n",
    "                self.df[self.cfg.target_col]\n",
    "                .value_counts(normalize=True)\n",
    "                .round(4)\n",
    "                .to_dict()\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1) Missing data profiling\n",
    "    # ---------------------------\n",
    "\n",
    "    def missingness_matrix(self, cols: Optional[List[str]] = None) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Return missingness fraction by column, optionally restricted to a subset list.\n",
    "        \"\"\"\n",
    "        cols = cols or list(self.df.columns)  # if no list provided, use all columns\n",
    "        return (\n",
    "            self.df[cols].isna().mean().sort_values(ascending=False)\n",
    "        )  # fraction missing per column\n",
    "\n",
    "    def missing_vs_target(self, feature: str) -> Optional[pd.Series]:\n",
    "        \"\"\"\n",
    "        Check whether missingness in a feature relates to the target (if any).\n",
    "        Returns target-wise mean missingness (proportions) or None if no target.\n",
    "        \"\"\"\n",
    "        if not self.cfg.target_col or self.cfg.target_col not in self.df:\n",
    "            return None  # cannot compare without a target\n",
    "        return (  # compute the mean proportion of missing values for a given column (feature) grouped by the target column\n",
    "            # assign(_miss=...) → Adds a temporary column _miss to the DataFrame without modifying the original. containing those 0/1 values.\n",
    "            # .astype(int) → Converts True → 1 and False → 0.\n",
    "            self.df.assign(\n",
    "                _miss=self.df[feature].isna().astype(int)\n",
    "            )  # 1 if missing else 0\n",
    "            # Calculates the mean of _miss in each group → proportion of missing values.\n",
    "            .groupby(self.cfg.target_col)[\"_miss\"]\n",
    "            .mean()  # average missingness by target\n",
    "            .sort_index()  # Sorts the result by the group labels.\n",
    "        )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2) Distribution & outliers\n",
    "    # ---------------------------\n",
    "\n",
    "    def iqr_outliers(self, col: str) -> Dict[str, object]:\n",
    "        \"\"\"\n",
    "        Classic IQR rule to flag outliers for a numeric column (robust to skew).\n",
    "        \"\"\"\n",
    "        # By default, dropna() works row-wise (axis=0) and drops rows where at least one value is missing.\n",
    "        # If you only want to drop rows where all values are missing, you can use:self.df[cols].dropna(how='all')\n",
    "        # If you want to drop rows with NaN only in specific columns, you can pass subset=cols.\n",
    "        ser = self.df[\n",
    "            col\n",
    "        ].dropna()  # ignore NaN and drops rows where at least one value is missing.\n",
    "        q1, q3 = ser.quantile([0.25, 0.75])  # first and third quartiles\n",
    "        iqr = q3 - q1  # interquartile range\n",
    "        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr  # Tukey's rule bounds\n",
    "        mask = (self.df[col] < lo) | (self.df[col] > hi)  # boolean mask for outliers\n",
    "        return {\n",
    "            \"q1\": float(q1),\n",
    "            \"q3\": float(q3),\n",
    "            \"iqr\": float(iqr),\n",
    "            \"lower_bound\": float(lo),\n",
    "            \"upper_bound\": float(hi),\n",
    "            \"n_outliers\": int(mask.sum()),\n",
    "            \"outlier_rows\": self.df.loc[mask, [self.cfg.id_col, col]].head(\n",
    "                10\n",
    "            ),  # sample some IDs to inspect\n",
    "        }\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3) Categorical summaries\n",
    "    # ---------------------------\n",
    "\n",
    "    def group_summary(\n",
    "        self, by: List[str], metrics: Dict[str, Tuple[str, str]]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        General grouped summary: pass a list of group columns and metric spec dict:\n",
    "        metrics = {\"n\": (\"id\", \"count\"), \"legal_rate\": (\"legal_review\", \"mean\")}\n",
    "\n",
    "        Returns sorted grouped table.\n",
    "        \"\"\"\n",
    "        # Build an agg dict in the signature pandas expects\n",
    "        # building a dynamic aggregation specification for a Pandas groupby using pd.NamedAgg.\n",
    "        agg_spec = {\n",
    "            k: pd.NamedAgg(column=v[0], aggfunc=v[1]) for k, v in metrics.items()\n",
    "        }\n",
    "        # agg(**agg_spec): Expands the dictionary into keyword arguments for named aggregation.\n",
    "        out = (\n",
    "            self.df.groupby(by).agg(**agg_spec).reset_index()\n",
    "        )  # perform the groupby aggregation\n",
    "        return out  # leave sorting to the caller\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4) Correlations & redundancy\n",
    "    # ---------------------------\n",
    "\n",
    "    def numeric_correlations(self, method: str = \"spearman\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return numeric-numeric correlation matrix (Spearman default for robustness to skew and outliers).\n",
    "        \"\"\"\n",
    "        if not self.cfg.numeric_cols:\n",
    "            raise ValueError(\"No numeric_cols configured for correlation.\")\n",
    "        return self.df[self.cfg.numeric_cols].corr(method=method)  # correlation matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def cramers_v(x: pd.Series, y: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Cramér’s V between two categorical variables (bias-corrected).\n",
    "        Requires scipy; returns np.nan if unavailable.\n",
    "        Define a function that takes two pandas Series (two categorical columns)\n",
    "        and returns a single number (the V value).\n",
    "        Cramér’s V is a 0–1 strength-of-association measure between two\n",
    "        categorical variables (0 = no association, 1 = very strong).\n",
    "        It’s like a correlation for categories.\n",
    "        Use it in EDA to spot redundant categorical features\n",
    "        (e.g., case_type vs risk_band) so the model isn’t learning the same information twice.\n",
    "        Cramér’s V (important with small samples or many categories).\n",
    "        Notes the SciPy dependency.\n",
    "        Feature redundancy: If case_type and risk_band have a high V (say ≥ 0.5),\n",
    "        they’re strongly associated. You might:\n",
    "        keep both but be cautious about interpreting coefficients,\n",
    "        or drop/merge one to simplify the model and reduce multicollinearity risk.\n",
    "        Data understanding: High associations can reveal operational patterns\n",
    "        (e.g., some teams disproportionately handle certain case types).\n",
    "        Model stability: Redundant predictors can make models unstable or harder to interpret;\n",
    "        removing redundancy improves robustness and clarity.\n",
    "        Rules of thumb for interpretation (context-dependent):\n",
    "        V < 0.1 ≈ negligible, 0.1–0.3 weak, 0.3–0.5 moderate, > 0.5 strong.\n",
    "        Important: Cramér’s V is an association strength, not a causal measure and not\n",
    "        a significance test by itself (the chi-square test provides a p-value; V provides effect size).\n",
    "        Missing values: pd.crosstab ignores NaNs; decide whether to impute or drop beforehand.\n",
    "        High-cardinality categories: Many levels can complicate interpretation; consider grouping rare levels.\n",
    "        Huge sample sizes: Chi-square p-values will be almost always “significant”;\n",
    "        look at V as an effect size to judge practical significance.\n",
    "        Ordinal categories: If categories are truly ordered (e.g., severity bands),\n",
    "        you may also analyse with Spearman’s correlation on numeric codes (with care).\n",
    "        \"\"\"\n",
    "        if not _HAS_SCIPY:  # if scipy not installed, degrade gracefully, don’t crash.\n",
    "            # Warn the user and return NaN. Thiskeeps CI and quick environments stable even without optional deps.\n",
    "            warnings.warn(\"scipy not available: returning NaN for Cramér’s V\")\n",
    "            return float(\"nan\")\n",
    "        # Build a contingency table of counts for every combination of x category and y category.\n",
    "        # rows = case_type (LPA/Deputyship/Other), cols = risk_band (Low/Medium/High).\n",
    "        tbl = pd.crosstab(x, y)  # contingency table\n",
    "        # Run the chi-square test of independence on that table.\n",
    "        # The Chi-square test looks at the pattern of observations and will tell us if certain combinations of the categories occur more frequently than we would expect by chance, given the total number of times each category occurred. The chi-square statistic tells you how much difference exists between the observed count in each table cell to the counts you would expect if there were no relationship at all in the population. Thus, low p-values (p< .05) indicate a likely difference between the theoretical population and the collected sample. You can conclude that a relationship exists between the categorical variables.\n",
    "        # chi2_contingency returns (chi2 statistic, p-value, dof, expected counts).\n",
    "        # We take index [0]: the statistic.\n",
    "        # Intuition: larger chi-square ⇒ bigger deviation from independence.\n",
    "        chi2 = chi2_contingency(tbl)[0]  # chi-squared test statistic\n",
    "        # Total number of observations in the table, needed to turn chi-square into an effect size.\n",
    "        n = tbl.to_numpy().sum()  # sample size\n",
    "        # Compute phi-squared (χ² divided by sample size). This scales the statistic by how much data we have.\n",
    "        phi2 = chi2 / n  # raw effect size\n",
    "        # Number of row categories (r) and column categories (k). Needed for bias correction.\n",
    "        r, k = tbl.shape  # rows, cols\n",
    "        # Bias correction (Bergsma 2013)\n",
    "        # These three lines implement a bias correction (Bergsma & Wicher, 2013), which adjusts the effect size when sample sizes are small or category counts are high. phi2corr subtracts a small-sample “expected inflation” term from phi2. rcorr and kcorr are corrected counts of categories used in the denominator to keep the scale proper (so V stays within [0, 1] more reliably).\n",
    "        phi2corr = max(0, phi2 - (k - 1) * (r - 1) / (n - 1))\n",
    "        rcorr = r - (r - 1) ** 2 / (n - 1)\n",
    "        kcorr = k - (k - 1) ** 2 / (n - 1)\n",
    "        # max(1e-12, ...) is a safety guard so we never divide by zero (e.g., degenerate tables).The square root puts the metric on the 0–1 scale. Casting to float makes sure you get a plain Python float (handy for printing/logging).\n",
    "        return float(\n",
    "            np.sqrt(phi2corr / max(1e-12, min(kcorr - 1, rcorr - 1)))\n",
    "        )  # guard against div-by-zero\n",
    "\n",
    "    def redundancy_drop_list(\n",
    "        self, cols: Optional[List[str]] = None, thresh: float = 0.90\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identify numeric columns to drop because they are highly correlated with others (abs(r) > thresh).\n",
    "        \"\"\"\n",
    "        cols = cols or (self.cfg.numeric_cols or [])\n",
    "        corr = self.df[cols].corr().abs()  # absolute Pearson by default\n",
    "        upper = corr.where(\n",
    "            np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "        )  # upper triangle without diagonal, while masking the rest with NaN.\n",
    "        return [\n",
    "            c for c in upper.columns if (upper[c] > thresh).any()\n",
    "        ]  # columns with any high corr\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5) Multicollinearity (VIF)\n",
    "    # ---------------------------\n",
    "\n",
    "    def vif_report(self, cols: Optional[List[str]] = None) -> Optional[pd.Series]:\n",
    "        \"\"\"\n",
    "        Multicolinearity detection using Variance Inflation Factor for a set of numeric predictors.\n",
    "        Returns None if statsmodels is not available.\n",
    "        Interpreting VIF: VIF = 1 → No correlation with other variables.\n",
    "        1 < VIF ≤ 5 → Moderate correlation (usually acceptable).\n",
    "        VIF > 5 or 10 → High multicollinearity; consider removing or transforming the variable.\n",
    "        Common Pitfall: If you run your original code without add_constant,\n",
    "        you might get artificially low or high VIF values.\n",
    "        \"\"\"\n",
    "        if not _HAS_STATSMODELS:\n",
    "            warnings.warn(\"statsmodels not available: VIF report skipped.\")\n",
    "            return None\n",
    "        cols = cols or (self.cfg.numeric_cols or [])\n",
    "        # Removes any rows that contain NaN (missing values) in any of those selected columns\n",
    "        # By default, dropna() works row-wise (axis=0) and drops rows where at least one value is missing.\n",
    "        # If you only want to drop rows where all values are missing, you can use:self.df[cols].dropna(how='all')\n",
    "        # If you want to drop rows with NaN only in specific columns, you can pass subset=cols.\n",
    "\n",
    "        # X containing only the independent variables (no target column).\n",
    "        X = self.df[cols].dropna()  # VIF requires no missing values\n",
    "        # import statsmodels.api as sm\n",
    "        # It adds an intercept column (a column of 1s) to your dataset X.\n",
    "        # This is important for regression models in statsmodels,\n",
    "        # because by default they do not automatically include an intercept term.\n",
    "        # from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        # from statsmodels.tools.tools import add_constant\n",
    "        X = sm.add_constant(\n",
    "            X\n",
    "        )  # add constant term, otherwise, VIF values can be misleading\n",
    "        vif = pd.Series(  # variance_inflation_factor is imported from statsmodels.stats.outliers_influence.\n",
    "            [variance_inflation_factor(X.values, i) for i in range(X.shape[1])],\n",
    "            index=X.columns,\n",
    "            name=\"VIF\",\n",
    "        )\n",
    "        return vif.sort_values(ascending=False)  # higher VIF -> more collinear\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6) Class imbalance & leakage\n",
    "    # ---------------------------\n",
    "\n",
    "    def imbalance_summary(self) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Return positive share and counts for a binary target (if present).\n",
    "        Calculate the positive class share from a Pandas DataFrame and\n",
    "        then builds a dictionary with counts for each class (n0, n1)\n",
    "        and the positive share (pos_share).\n",
    "        \"\"\"\n",
    "        if not self.cfg.target_col or self.cfg.target_col not in self.df:\n",
    "            return None\n",
    "        # Counting the frequency of each unique value in a column, including NaN values.\n",
    "        vc = self.df[self.cfg.target_col].value_counts(dropna=False)\n",
    "        # Safely compute positive share\n",
    "        # pos_share = self.df[self.cfg.target_col].mean()  # This is already a float scalar\n",
    "        # Build the result dictionary with safe int casting\n",
    "        # return {\n",
    "        #    \"n0\": int(vc.get(0, 0)),  # Count of class 0\n",
    "        #    \"n1\": int(vc.get(1, 0)),  # Count of class 1\n",
    "        #    \"pos_share\": round(float(pos_share), 4)  # Ensure float before rounding\n",
    "        # }\n",
    "        pos_share = float(self.df[self.cfg.target_col].mean())\n",
    "        return {\n",
    "            \"n0\": int(vc.get(0, 0)),\n",
    "            \"n1\": int(vc.get(1, 0)),\n",
    "            \"pos_share\": round(pos_share, 4),\n",
    "        }\n",
    "\n",
    "    def leakage_scan(self, suspicious_keywords: Iterable[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Heuristically scan columns for likely post-treatment/leakage fields\n",
    "        (e.g., 'post', 'signed', 'decision').\n",
    "        Catch post-treatment / leakage columns (e.g., anything created\n",
    "        after allocation/sign-off) before modeling.\n",
    "        Leakage makes models look unrealistically good.\n",
    "        Pre-allocation models must not use columns like date_pg_signoff,\n",
    "        signed_status, legal_decision—these leak future info. This quick scan is a sanity net.\n",
    "        Example: sus = eda.leakage_scan([\"post\", \"signed\", \"decision\", \"pg_signoff\"])\n",
    "        print(sus)  # e.g. ['date_pg_signoff', 'signed_off_flag', 'legal_decision_code']\n",
    "        \"\"\"\n",
    "        # normalise for case-insensitive check\n",
    "        keys = [\n",
    "            k.lower() for k in suspicious_keywords\n",
    "        ]  # 1) Lower-case the keywords so matching is case-insensitive.\n",
    "\n",
    "        hits = []  # 2) Start an empty list to store suspicious column names.\n",
    "        for c in self.df.columns:  # 3) Look at every column in the DataFrame.\n",
    "            low = c.lower()  # 4) Lower-case the column name (case-insensitive compare).\n",
    "            if any(\n",
    "                k in low for k in keys\n",
    "            ):  # 5) If *any* keyword is a substring of the column name…\n",
    "                hits.append(c)  # 6) …flag it by appending to hits.\n",
    "        return hits  # 7) Return the list of suspicious columns.\n",
    "\n",
    "    # ---------------------------\n",
    "    # 7) Interactions (binned)\n",
    "    # ---------------------------\n",
    "\n",
    "    def binned_interaction_rate(\n",
    "        self, num_col: str, cat_col: str, target: Optional[str] = None, q: int = 5\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute target mean across bins of a numeric column and levels of a categorical column.\n",
    "        Useful to screen interactions (e.g., risk_band × days_to_alloc -> legal_review rate).\n",
    "        quickly see if a numeric feature and a categorical feature interact to change the\n",
    "        target rate (e.g., legal review rate varies by risk_band and days_to_alloc bins).\n",
    "        Example: You can spot patterns like\n",
    "        “High risk + long days_to_alloc → much higher legal-review rate”,\n",
    "        justifying an interaction term or different triage rules.\n",
    "        tab = eda.binned_interaction_rate(\"days_to_alloc\", \"risk_band\", target=\"legal_review\", q=5)\n",
    "        print(tab)  # a table of legal_review rate by risk_band (rows) and days_to_alloc bins (columns)\n",
    "        \"\"\"\n",
    "\n",
    "        target = (\n",
    "            target or self.cfg.target_col\n",
    "        )  # 1) Use provided target or the configured default target.\n",
    "        if not target:\n",
    "            raise ValueError(\n",
    "                \"Target column required for interaction rates.\"\n",
    "            )  # 2) Must know which target to average.\n",
    "        # 3) Keep only rows that have both the numeric and categorical values (target may still be NaN).\n",
    "        tmp = self.df[[num_col, cat_col, target]].dropna(subset=[num_col, cat_col])\n",
    "        # 4) Bin the numeric feature into ~q quantile bins (robust to skew).\n",
    "        tmp[\"__bin__\"] = pd.qcut(\n",
    "            tmp[num_col],\n",
    "            q=min(\n",
    "                q, tmp[num_col].nunique()\n",
    "            ),  # Do not create more bins than we have unique values.\n",
    "            duplicates=\"drop\",  #    If some bins would be identical, 'duplicates=\"drop\"' merges them cleanly.\n",
    "        )\n",
    "        # 5) For each category×bin cell, compute the mean target (e.g., legal review rate).\n",
    "        # Pivot to a matrix: rows=category, cols=bins.\n",
    "        out = tmp.groupby([cat_col, \"__bin__\"])[target].mean().unstack()\n",
    "        return out  # 6) Return the matrix for inspection/plotting.\n",
    "\n",
    "    # ---------------------------\n",
    "    # 8) Time-series EDA\n",
    "    # ---------------------------\n",
    "\n",
    "    def resample_time_series(self, metrics: Dict[str, Tuple[str, str]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Resample to daily/weekly, aggregating metrics.\n",
    "        Example metrics:\n",
    "            {\"backlog\": (\"backlog\", \"last\"), \"inv_mean\": (\"investigators_on_duty\", \"mean\")}\n",
    "        Turn event rows into daily/weekly time series KPIs\n",
    "        (e.g., daily backlog last value, average investigators on duty),\n",
    "        optionally add a 7-day smoother.\n",
    "        Clean daily KPIs (backlog, staffing) to plot trends, check seasonality,\n",
    "        and feed forecasting models (SARIMAX, etc.).\n",
    "        Example:\n",
    "        daily = eda.resample_time_series({\n",
    "        \"backlog\": (\"backlog\", \"last\"),\n",
    "        \"inv_mean\": (\"investigators_on_duty\", \"mean\"),\n",
    "        })\n",
    "        print(daily.tail())\n",
    "        \"\"\"\n",
    "        # 1) Need to know which column is the time index (e.g., 'date_received_opg').\n",
    "        if not self.cfg.time_index_col:\n",
    "            raise ValueError(\"Set time_index_col in EDAConfig to resample.\")\n",
    "\n",
    "        # 2) Create a DateTimeIndex from the configured column, coercing bad values to NaT; sort chronologically.\n",
    "        ts = self.df.set_index(\n",
    "            pd.to_datetime(self.df[self.cfg.time_index_col], errors=\"coerce\")\n",
    "        ).sort_index()\n",
    "\n",
    "        # 3) Build an aggregation spec, e.g., 'backlog' uses ('backlog', 'last'),\n",
    "        # 'inv_mean' uses ('investigators_on_duty', 'mean').\n",
    "        agg_spec = {\n",
    "            k: pd.NamedAgg(column=v[0], aggfunc=v[1]) for k, v in metrics.items()\n",
    "        }\n",
    "\n",
    "        # 4) Resample by the rule ('D' for daily, 'W' for weekly) and aggregate using the spec.\n",
    "        out = ts.resample(self.cfg.resample_rule).agg(\n",
    "            **agg_spec\n",
    "        )  # astericks expand the dic to keywords\n",
    "\n",
    "        if self.cfg.resample_rule.upper() == \"D\":\n",
    "            for k in list(metrics.keys()):\n",
    "                out[f\"{k}_7d\"] = out[k].rolling(7, min_periods=3).mean()\n",
    "        # 5) Convenience: if daily, add a 7-day rolling mean per metric (smoother, handles weekends/spikes).\n",
    "\n",
    "        return out\n",
    "        # 6) Return the resampled KPI frame (indexed by date).\n",
    "\n",
    "    def lag_correlations(\n",
    "        self, s1: pd.Series, s2: pd.Series, lags: Optional[Iterable[int]] = None\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Compute correlation between s1[t] and s2[t - k] for specified lags (default from config).\n",
    "        quick check if changes in one series lead or follow another\n",
    "        (e.g., does staffing today correlate with backlog a week later?).\n",
    "        If corr(backlog, inv_mean shifted by 7) is negative and large in magnitude,\n",
    "        more staff tends to reduce backlog about a week later.\n",
    "        It’s not causality proof, but a strong operational hint for modeling and simulation.\n",
    "        Example:\n",
    "        corrs = eda.lag_correlations(daily[\"backlog\"], daily[\"inv_mean\"], lags=[1,7,14])\n",
    "        print(corrs)\n",
    "        \"\"\"\n",
    "        lags = list(\n",
    "            lags or self.cfg.lag_list\n",
    "        )  # 1) Use provided lags or defaults (e.g., [1, 7, 14]).\n",
    "        out = {}  # 2) Result holder: lag name -> correlation number.\n",
    "        for k in lags:  # 3) For each lag k…\n",
    "            out[f\"lag_{k}\"] = float(\n",
    "                s1.corr(s2.shift(k))\n",
    "            )  # 4) …compute corr between s1[t] and s2 shifted by k (i.e., s2 at t-k).\n",
    "        return pd.Series(out)  # 5) Return as a small Series for easy reading/plotting.\n",
    "\n",
    "    # ---------------------------\n",
    "    # 9) Survival / interval analysis\n",
    "    # ---------------------------\n",
    "\n",
    "    def km_quantiles_by_group(\n",
    "        self,\n",
    "        duration: str,\n",
    "        event: str,\n",
    "        group: str,\n",
    "        probs: Iterable[float] = (0.25, 0.5, 0.75),\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute Kaplan–Meier quantiles by group if lifelines is installed.\n",
    "        Fallback: naive quantiles (ignores censoring) with a warning.\n",
    "        Kaplan–Meier (censor-aware) service-time quantiles (median, P75, P90…)\n",
    "        by group (e.g., by risk_band).\n",
    "        Falls back to naive quantiles if lifelines is missing.\n",
    "        “High-risk median to PG sign-off is 45 days (P90=110) vs\n",
    "        Low-risk median 28 (P90=70).”\n",
    "        That directly informs SLAs, case prioritisation, and DES service-time inputs.\n",
    "        Example:\n",
    "        km_tab = eda.km_quantiles_by_group(\"days_to_signoff\", \"event_signoff\", \"risk_band\", probs=(0.5, 0.8, 0.9))\n",
    "        \"\"\"\n",
    "        res = []  # 1) Collect per-group rows here.\n",
    "        for g, dfg in self.df.groupby(\n",
    "            group\n",
    "        ):  # 2) For each group (e.g., each risk band)…\n",
    "            if _HAS_LIFELINES:\n",
    "                km = KaplanMeierFitter()  # 3) Create a KM estimator.\n",
    "                km.fit(\n",
    "                    durations=dfg[duration], event_observed=dfg[event]\n",
    "                )  # 4) Fit with durations + censor flags.\n",
    "                row = {\"group\": g}  # 5) Start a result row with the group name.\n",
    "                for p in probs:\n",
    "                    row[f\"q{int(p*100)}\"] = float(\n",
    "                        km.quantile(p)\n",
    "                    )  # 6) Read censor-aware quantiles (e.g., q50=median).\n",
    "                res.append(row)  # 7) Save this group’s row.\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    \"lifelines not available; using naive quantiles (censoring ignored).\"\n",
    "                )\n",
    "                row = {\"group\": g}\n",
    "                for p in probs:\n",
    "                    row[f\"q{int(p*100)}\"] = float(\n",
    "                        dfg[duration].quantile(p)\n",
    "                    )  # 8) Fallback: naive quantiles.\n",
    "                res.append(row)\n",
    "        return (\n",
    "            pd.DataFrame(res).sort_values(\"group\").reset_index(drop=True)\n",
    "        )  # 9) Return a tidy table.\n",
    "\n",
    "    # ---------------------------\n",
    "    # 10) KPI tables for stakeholders\n",
    "    # ---------------------------\n",
    "\n",
    "    def monthly_kpis(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Produce a stakeholder-friendly monthly KPI table by team (if team_col set):\n",
    "        - backlog (last of month),\n",
    "        - median days_to_alloc,\n",
    "        - legal review rate.\n",
    "        Produce a stakeholder-ready monthly table per team: backlog,\n",
    "        median time to allocation, and legal-review rate.\n",
    "        It turns raw rows into an executive KPI view over time:\n",
    "        “Team B’s median days to allocation rose in Q3 while\n",
    "        backlog last-of-month climbed; legal review rate stable.”\n",
    "        Perfect for dashboards and monthly packs.\n",
    "        Example:\n",
    "        kpis = eda.monthly_kpis()\n",
    "        print(kpis.head())\n",
    "        return out.sort_values([self.cfg.team_col, \"__month\"])\n",
    "        \"\"\"\n",
    "        if not self.cfg.team_col:\n",
    "            raise ValueError(\"team_col must be configured for monthly KPIs.\")\n",
    "        # 1) Need the team column to group by.\n",
    "\n",
    "        month = self.df[self.cfg.date_received].dt.to_period(\"M\").dt.to_timestamp()\n",
    "        # 2) Convert received date to a month period, then back to a timestamp (first day of that month).\n",
    "\n",
    "        tmp = self.df.assign(__month=month)\n",
    "        # 3) Add a helper column '__month' for grouping.\n",
    "\n",
    "        out = (\n",
    "            tmp.groupby([self.cfg.team_col, \"__month\"])\n",
    "            .agg(\n",
    "                backlog=(\n",
    "                    (\"backlog\", \"last\") if \"backlog\" in tmp.columns else (\"id\", \"count\")\n",
    "                ),\n",
    "                # 4) Backlog: the last value in each month (if available). Otherwise, fallback to a count.\n",
    "                median_alloc=(\"days_to_alloc\", \"median\"),\n",
    "                # 5) Median time-to-allocation (robust to skew).\n",
    "                legal_rate=(\n",
    "                    (self.cfg.target_col, \"mean\")\n",
    "                    if self.cfg.target_col\n",
    "                    else (self.cfg.id_col, \"count\")\n",
    "                ),\n",
    "                # 6) If target exists (e.g., legal_review), take mean (i.e., rate). Else, just count.\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        return out.sort_values([self.cfg.team_col, \"__month\"])\n",
    "        # 7) Return a tidy table sorted by team and month.\n",
    "\n",
    "\n",
    "# demo_eda.py\n",
    "# Small, self-contained demo that exercises key methods on synthetic OPG-like data.\n",
    "\n",
    "from eda_opg import EDAConfig, OPGInvestigationEDA\n",
    "\n",
    "# ----- 1) Create a small synthetic dataset for demonstration -----\n",
    "rng = np.random.default_rng(42)\n",
    "n = 2000\n",
    "\n",
    "# Base dates\n",
    "start = pd.Timestamp(\"2024-01-01\")\n",
    "recv_dates = start + pd.to_timedelta(rng.integers(0, 300, size=n), unit=\"D\")\n",
    "\n",
    "# Allocation occurs for ~85% within 1-30 days; else censored (NaT)\n",
    "alloc_delays = rng.integers(1, 31, size=n)\n",
    "allocated_mask = rng.random(size=n) < 0.85\n",
    "alloc_dates = pd.Series(recv_dates) + pd.to_timedelta(alloc_delays, unit=\"D\")\n",
    "alloc_dates = alloc_dates.where(allocated_mask, pd.NaT)\n",
    "\n",
    "# Sign-off for ~70% within 20-120 days from received; else censored\n",
    "signoff_delays = rng.integers(20, 121, size=n)\n",
    "so_mask = rng.random(size=n) < 0.70\n",
    "signoff_dates = pd.Series(recv_dates) + pd.to_timedelta(signoff_delays, unit=\"D\")\n",
    "signoff_dates = signoff_dates.where(so_mask, pd.NaT)\n",
    "\n",
    "# Categorical fields\n",
    "case_types = rng.choice([\"LPA\", \"Deputyship\", \"Other\"], size=n, p=[0.6, 0.3, 0.1])\n",
    "risk_band = rng.choice([\"Low\", \"Medium\", \"High\"], size=n, p=[0.5, 0.35, 0.15])\n",
    "teams = rng.choice([\"Team A\", \"Team B\", \"Team C\"], size=n, p=[0.4, 0.4, 0.2])\n",
    "region = rng.choice([\"North\", \"Midlands\", \"South\"], size=n)\n",
    "\n",
    "# Daily ops fields\n",
    "investigators_on_duty = rng.integers(8, 20, size=n)  # rough proxy\n",
    "allocations = rng.integers(0, 25, size=n)  # allocated on that day\n",
    "backlog = np.maximum(\n",
    "    0, 500 + rng.normal(0, 60, size=n).astype(int)\n",
    ")  # evolving backlog proxy\n",
    "\n",
    "# Target: legal review ~5%, with higher odds for High risk and longer allocation delay\n",
    "# We'll simulate it based on logits to mimic a real signal\n",
    "base_logit = -3.0 + 0.02 * np.nan_to_num(alloc_dates - recv_dates).astype(\n",
    "    \"timedelta64[D]\"\n",
    ").astype(float)\n",
    "risk_bump = np.select(\n",
    "    [risk_band == \"High\", risk_band == \"Medium\"], [1.2, 0.4], default=0.0\n",
    ")\n",
    "logit = base_logit + risk_bump\n",
    "prob = 1 / (1 + np.exp(-logit))\n",
    "legal_review = (rng.random(size=n) < prob).astype(int)\n",
    "\n",
    "# Assemble DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": np.arange(1, n + 1),\n",
    "        \"date_received_opg\": recv_dates,\n",
    "        \"date_allocated_investigator\": alloc_dates,\n",
    "        \"date_pg_signoff\": signoff_dates,\n",
    "        \"case_type\": case_types,\n",
    "        \"risk_band\": risk_band,\n",
    "        \"team\": teams,\n",
    "        \"region\": region,\n",
    "        \"investigators_on_duty\": investigators_on_duty,\n",
    "        \"allocations\": allocations,\n",
    "        \"backlog\": backlog,\n",
    "        \"legal_review\": legal_review,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ----- 2) Configure columns and instantiate the EDA toolkit -----\n",
    "cfg = EDAConfig(\n",
    "    id_col=\"id\",\n",
    "    date_received=\"date_received_opg\",\n",
    "    date_allocated=\"date_allocated_investigator\",\n",
    "    date_signed_off=\"date_pg_signoff\",\n",
    "    target_col=\"legal_review\",\n",
    "    numeric_cols=[\n",
    "        \"days_to_alloc\",\n",
    "        \"days_to_signoff\",\n",
    "        \"investigators_on_duty\",\n",
    "        \"allocations\",\n",
    "        \"backlog\",\n",
    "    ],\n",
    "    categorical_cols=[\"case_type\", \"risk_band\", \"team\", \"region\"],\n",
    "    time_index_col=\"date_received_opg\",\n",
    "    team_col=\"team\",\n",
    "    risk_col=\"risk_band\",\n",
    "    case_type_col=\"case_type\",\n",
    ")\n",
    "\n",
    "eda = OPGInvestigationEDA(df, cfg)\n",
    "\n",
    "# ----- 3) Run a few core EDA tasks (print or log these in practice) -----\n",
    "print(\"\\n== QUICK OVERVIEW ==\")\n",
    "print(eda.quick_overview())\n",
    "\n",
    "print(\"\\n== MISSINGNESS ==\")\n",
    "print(eda.missingness_matrix().head(10))\n",
    "print(\n",
    "    \"Missing 'days_to_signoff' vs target:\\n\", eda.missing_vs_target(\"days_to_signoff\")\n",
    ")\n",
    "\n",
    "print(\"\\n== OUTLIERS (days_to_signoff) ==\")\n",
    "print(eda.iqr_outliers(\"days_to_signoff\"))\n",
    "\n",
    "print(\"\\n== CATEGORICAL SUMMARY (case_type × risk_band) ==\")\n",
    "summary = eda.group_summary(\n",
    "    by=[\"case_type\", \"risk_band\"],\n",
    "    metrics={\n",
    "        \"n\": (\"id\", \"count\"),\n",
    "        \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "        \"med_alloc\": (\"days_to_signoff\", \"median\"),\n",
    "    },\n",
    ")\n",
    "print(summary.head(12))\n",
    "\n",
    "print(\"\\n== NUMERIC CORRELATIONS (Spearman) ==\")\n",
    "print(eda.numeric_correlations(\"spearman\"))\n",
    "\n",
    "print(\"\\n== REDUNDANCY DROP LIST (|r|>0.9) ==\")\n",
    "print(eda.redundancy_drop_list())\n",
    "\n",
    "print(\"\\n== CLASS IMBALANCE ==\")\n",
    "print(eda.imbalance_summary())\n",
    "\n",
    "print(\"\\n== LEAKAGE SCAN ==\")\n",
    "print(eda.leakage_scan([\"post\", \"signed\", \"decision\", \"outcome\"]))\n",
    "\n",
    "print(\"\\n== INTERACTION: risk_band × binned days_to_signoff -> legal_review rate ==\")\n",
    "print(eda.binned_interaction_rate(\"days_to_signoff\", \"risk_band\"))\n",
    "\n",
    "print(\"\\n== RESAMPLED TIME SERIES (daily) ==\")\n",
    "ts = eda.resample_time_series(\n",
    "    {\n",
    "        \"backlog\": (\"backlog\", \"last\"),\n",
    "        \"inv_mean\": (\"investigators_on_duty\", \"mean\"),\n",
    "    }\n",
    ")\n",
    "print(ts.tail())\n",
    "\n",
    "print(\"\\n== LAG CORRELATIONS: backlog vs inv_mean ==\")\n",
    "print(eda.lag_correlations(ts[\"backlog\"], ts[\"inv_mean\"]))\n",
    "\n",
    "print(\"\\n== KM QUANTILES by risk_band (signoff) ==\")\n",
    "print(eda.km_quantiles_by_group(\"days_to_signoff\", \"event_signed_off\", \"risk_band\"))\n",
    "\n",
    "print(\"\\n== MONTHLY KPIs by team ==\")\n",
    "print(eda.monthly_kpis().head(12))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# High-level orchestrator class combining pipeline + EDA tools\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "class InvestigationBacklogProject:\n",
    "    \"\"\"\n",
    "    High-level helper that wires together the core engineering\n",
    "    pipeline from ``Build_Investigator_Daily_from_Raw_14_11_25``\n",
    "    and the EDA toolkit from ``eda.ipynb``.\n",
    "\n",
    "    Typical usage\n",
    "    -------------\n",
    "    >>> from pathlib import Path\n",
    "    >>> project = InvestigationBacklogProject()\n",
    "    >>> outputs = project.run_full_pipeline()\n",
    "    >>> daily = outputs[\"daily\"]\n",
    "    >>> daily.head()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_path: \"Path\" = None, out_dir: \"Path\" = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_path:\n",
    "            Path to the CSV extract with investigations data. If not\n",
    "            provided, uses the global RAW_PATH defined in the\n",
    "            engineering module.\n",
    "        out_dir:\n",
    "            Directory where derived CSVs and plots will be written.\n",
    "            If not provided, uses the global OUT_DIR.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> from pathlib import Path\n",
    "        >>> project = InvestigationBacklogProject(\n",
    "        ...     raw_path=Path(\"data/raw/raw.csv\"),\n",
    "        ...     out_dir=Path(\"data/out\"),\n",
    "        ... )\n",
    "        \"\"\"\n",
    "        from pathlib import Path as _Path\n",
    "\n",
    "        # Fall back to module-level defaults if not supplied\n",
    "        global RAW_PATH, OUT_DIR\n",
    "        self.raw_path = _Path(raw_path) if raw_path is not None else RAW_PATH\n",
    "        self.out_dir = _Path(out_dir) if out_dir is not None else OUT_DIR\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Core engineering steps\n",
    "    # -------------------------\n",
    "\n",
    "    def load_raw(self):\n",
    "        \"\"\"\n",
    "        Load the raw investigations extract via :func:`load_raw`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        raw : pandas.DataFrame\n",
    "        colmap : dict\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> raw, colmap = project.load_raw()\n",
    "        >>> raw.shape  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        return load_raw(self.raw_path)\n",
    "\n",
    "    def engineer(self, raw=None, colmap=None, only_reallocated: bool = False):\n",
    "        \"\"\"\n",
    "        Run the :func:`engineer` step to create a clean, typed case-level table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw, colmap:\n",
    "            Optionally pass in the raw DataFrame and column map. If omitted\n",
    "            they are loaded from ``self.raw_path`` using :func:`load_raw`.\n",
    "        only_reallocated:\n",
    "            See the underlying :func:`engineer` docstring.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        typed : pandas.DataFrame\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> typed = project.engineer()\n",
    "        >>> typed.columns.tolist()[:5]  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if raw is None or colmap is None:\n",
    "            raw, colmap = self.load_raw()\n",
    "        return engineer(raw, colmap, only_reallocated=only_reallocated)\n",
    "\n",
    "    def build_event_log(self, typed=None):\n",
    "        \"\"\"\n",
    "        Build an investigator-centric event log from the engineered table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        typed:\n",
    "            Engineered DataFrame. If omitted it is computed via :meth:`engineer`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        events : pandas.DataFrame\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> typed = project.engineer()\n",
    "        >>> events = project.build_event_log(typed)\n",
    "        >>> events.head()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if typed is None:\n",
    "            typed = self.engineer()\n",
    "        return build_event_log(typed)\n",
    "\n",
    "    def build_wip_series(self, events=None):\n",
    "        \"\"\"\n",
    "        Build the Work-In-Progress (WIP) daily time series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        events:\n",
    "            Event log from :meth:`build_event_log`. If omitted it is built on the fly.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wip : pandas.DataFrame\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> wip = project.build_wip_series()\n",
    "        >>> wip.head()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if events is None:\n",
    "            events = self.build_event_log()\n",
    "        return build_wip_series(events)\n",
    "\n",
    "    def build_backlog_series(self, events=None):\n",
    "        \"\"\"\n",
    "        Build the backlog daily time series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        events:\n",
    "            Event log from :meth:`build_event_log`. If omitted it is built on the fly.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        backlog : pandas.DataFrame\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> backlog = project.build_backlog_series()\n",
    "        >>> backlog.head()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if events is None:\n",
    "            events = self.build_event_log()\n",
    "        return build_backlog_series(events)\n",
    "\n",
    "    def build_daily_panel(self, typed=None, wip=None, backlog=None):\n",
    "        \"\"\"\n",
    "        Build a daily, investigator-level panel suitable for modelling.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        typed, wip, backlog:\n",
    "            Optional intermediate tables. If omitted they are all recomputed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        daily : pandas.DataFrame\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> daily = project.build_daily_panel()\n",
    "        >>> daily.head()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if typed is None:\n",
    "            typed = self.engineer()\n",
    "        if wip is None:\n",
    "            wip = self.build_wip_series()\n",
    "        if backlog is None:\n",
    "            backlog = self.build_backlog_series()\n",
    "        return build_daily_panel(typed, wip, backlog)\n",
    "\n",
    "    def run_full_pipeline(self):\n",
    "        \"\"\"\n",
    "        Convenience method: run the entire core pipeline in one call.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : dict\n",
    "            A dictionary with keys:\n",
    "            ``raw``, ``colmap``, ``typed``, ``events``,\n",
    "            ``wip``, ``backlog``, ``daily``.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> outputs = project.run_full_pipeline()\n",
    "        >>> sorted(outputs.keys())  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        raw, colmap = self.load_raw()\n",
    "        typed = self.engineer(raw, colmap)\n",
    "        events = self.build_event_log(typed)\n",
    "        wip = self.build_wip_series(events)\n",
    "        backlog = self.build_backlog_series(events)\n",
    "        daily = self.build_daily_panel(typed, wip, backlog)\n",
    "        return {\n",
    "            \"raw\": raw,\n",
    "            \"colmap\": colmap,\n",
    "            \"typed\": typed,\n",
    "            \"events\": events,\n",
    "            \"wip\": wip,\n",
    "            \"backlog\": backlog,\n",
    "            \"daily\": daily,\n",
    "        }\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Interval analysis and EDA\n",
    "    # -------------------------\n",
    "\n",
    "    def build_interval_frame(self, typed=None, backlog_series=None, bank_holidays=None):\n",
    "        \"\"\"\n",
    "        Build the interval-analysis-ready frame using :class:`IntervalAnalysis`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        typed:\n",
    "            Engineered table (output of :meth:`engineer`).\n",
    "        backlog_series:\n",
    "            Backlog time series (output of :meth:`build_backlog_series`).\n",
    "        bank_holidays:\n",
    "            Optional list/Series of bank-holiday dates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        di : pandas.DataFrame\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> outputs = project.run_full_pipeline()\n",
    "        >>> di = project.build_interval_frame(outputs[\"typed\"], outputs[\"backlog\"])\n",
    "        >>> di.head()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if typed is None or backlog_series is None:\n",
    "            outputs = self.run_full_pipeline()\n",
    "            typed = outputs[\"typed\"]\n",
    "            backlog_series = outputs[\"backlog\"]\n",
    "        return IntervalAnalysis.build_interval_frame(\n",
    "            typed, backlog_series=backlog_series, bank_holidays=bank_holidays\n",
    "        )\n",
    "\n",
    "    def monthly_trend(self, di, metric=\"days_to_pg_signoff\", agg=\"median\", by=None):\n",
    "        \"\"\"\n",
    "        Thin wrapper around :meth:`IntervalAnalysis.monthly_trend`.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> outputs = project.run_full_pipeline()\n",
    "        >>> di = project.build_interval_frame(outputs[\"typed\"], outputs[\"backlog\"])\n",
    "        >>> trend = project.monthly_trend(di, metric=\"days_to_pg_signoff\")\n",
    "        >>> trend.head()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        return IntervalAnalysis.monthly_trend(di, metric=metric, agg=agg, by=by)\n",
    "\n",
    "    def make_eda(self, df=None, config=None):\n",
    "        \"\"\"\n",
    "        Construct an :class:`OPGInvestigationEDA` instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df:\n",
    "            DataFrame to analyse. If omitted uses the engineered case-level table.\n",
    "        config:\n",
    "            :class:`EDAConfig` defining the column mapping. If omitted, a\n",
    "            minimal sensible default is inferred.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        eda : OPGInvestigationEDA\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> outputs = project.run_full_pipeline()\n",
    "        >>> eda = project.make_eda(outputs[\"typed\"])\n",
    "        >>> overview = eda.quick_overview()\n",
    "        \"\"\"\n",
    "\n",
    "        if df is None:\n",
    "            outputs = self.run_full_pipeline()\n",
    "            df = outputs[\"typed\"]\n",
    "        if config is None:\n",
    "            # Fallback minimal config assuming the engineered column names\n",
    "\n",
    "            config = EDAConfig(\n",
    "                id_col=\"case_id\",\n",
    "                date_received=\"dt_received_opg\",\n",
    "                date_allocated=\"dt_alloc_invest\",\n",
    "                date_signed_off=\"dt_pg_signoff\",\n",
    "            )\n",
    "        return OPGInvestigationEDA(df, config)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Collective demo\n",
    "    # -------------------------\n",
    "\n",
    "    def demo_all(self):\n",
    "        \"\"\"\n",
    "        Run an end-to-end demo:\n",
    "\n",
    "        1. Run the full engineering pipeline.\n",
    "        2. Build the interval frame and monthly trend KPIs.\n",
    "        3. Construct an EDA object and compute a quick overview.\n",
    "        4. Generate and save monthly PG sign-off trend plots.\n",
    "\n",
    "        This mirrors the demonstration cells from the original notebooks\n",
    "        but in a single callable method.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> project = InvestigationBacklogProject()\n",
    "        >>> demo_outputs = project.demo_all()  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # 1) Core pipeline\n",
    "        outputs = self.run_full_pipeline()\n",
    "        typed = outputs[\"typed\"]\n",
    "        backlog = outputs[\"backlog\"]\n",
    "\n",
    "        # 2) Interval frame + monthly trend\n",
    "        di = self.build_interval_frame(typed, backlog)\n",
    "        trend = IntervalAnalysis.monthly_trend(\n",
    "            di,\n",
    "            metric=\"days_to_pg_signoff\",\n",
    "            agg=\"median\",\n",
    "            by=[\"case_type\"],\n",
    "        ).copy()\n",
    "        trend[\"month\"] = pd.to_datetime(trend[\"yyyymm\"] + \"-01\")\n",
    "\n",
    "        piv = trend.pivot(\n",
    "            index=\"month\", columns=\"case_type\", values=\"days_to_pg_signoff\"\n",
    "        ).sort_index()\n",
    "        piv_delta = trend.pivot(\n",
    "            index=\"month\", columns=\"case_type\", values=\"mom_delta\"\n",
    "        ).sort_index()\n",
    "\n",
    "        # 3) Simple EDA overview\n",
    "        eda = self.make_eda(typed)\n",
    "        overview = eda.quick_overview()\n",
    "\n",
    "        # 4) Plots (saved into self.out_dir)\n",
    "        outdir = self.out_dir\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        for col in piv.columns:\n",
    "            plt.plot(piv.index, piv[col], alpha=0.6, label=str(col))\n",
    "        plt.title(\"Monthly median days_to_pg_signoff by case_type\")\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"Median days to PG sign-off\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(ncol=2, fontsize=8)\n",
    "        plot1 = outdir / \"monthly_median_days_to_pg_signoff_by_case_type.png\"\n",
    "        plt.savefig(plot1, bbox_inches=\"tight\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        for col in piv_delta.columns:\n",
    "            plt.plot(piv_delta.index, piv_delta[col], alpha=0.6, label=str(col))\n",
    "        plt.title(\"Monthly MoM delta: days_to_pg_signoff by case_type\")\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"MoM delta (days)\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(ncol=2, fontsize=8)\n",
    "        plot2 = outdir / \"monthly_mom_delta_days_to_pg_signoff_by_case_type.png\"\n",
    "        plt.savefig(plot2, bbox_inches=\"tight\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        print(\"Demo complete.\")\n",
    "        print(\"Key outputs:\")\n",
    "        print(\"  - Engineered table shape:\", typed.shape)\n",
    "        print(\"  - Daily panel shape:\", outputs[\"daily\"].shape)\n",
    "        print(\"  - Interval frame shape:\", di.shape)\n",
    "        print(\"  - Overview columns:\", list(overview.columns))\n",
    "        print(\"  - Plots saved to:\", plot1, \"and\", plot2)\n",
    "\n",
    "        return {\n",
    "            **outputs,\n",
    "            \"interval_frame\": di,\n",
    "            \"trend\": trend,\n",
    "            \"overview\": overview,\n",
    "            \"plots\": {\"median_trend\": plot1, \"delta_trend\": plot2},\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
