{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Driving investigation demand in 2018-19:\n",
    "Regarding the data generated from the SQL code in the previous example as linked investigation and Lasting Power of Attorney (LPA) data, can you suggest some analysis to answer some questions below  was set for a planned workshop with the stakeholders (OPG) to help understand what is driving investigation demand?\n",
    "\n",
    "Establish why there was a spike in Property and Finance cases investigated in 2018 and 2019 which seemed to then disappear following the pandemic. My best guess (and it is a guess) is that this may have been caused by a change in the criteria for investigation in 2018 which essentially made it a lot easier to investigate cases, before this applicants had to provide a lot of supporting documentation which have been difficult to provide and therefore been a barrier to making an investigation. If so then the spike in 2018 and 2019 might be a **backlog** (*an accumulation of uncompleted work or matters needing to be dealt with*) of cases (or **Pent-Up Demand**: *a rapid increase in demand for a service/product. It typically occurs after a period of **subdued** (*under control/overcome*) spending, such as during a recession. Consumers hold off making purchases during tough economic times, creating a backlog of demand that is unleashed when signs of recovery emerge.*) released when the ability to submit an application became much easier. \n",
    "\n",
    "If this is due to a backlog of cases then this would suggest that investigations in 2018 and 2019 had taken much longer to submit than usual. I was hoping that we might see this is in the data if we looked at the time periods in the linked data from registration to the date of investigation, which I would expect to be much longer on average in 2018 and 2019 than in the years before or after these dates.  \n",
    "1. Why did property and finance investigations surge from 2018 onwards? \n",
    "2. Could the Spike in Property and Finance Cases be due to possible pent-up demand (backlog?) of Concerns that the simplification of criteria for investigation in 2018 made easier to investigate? \n",
    "3. Did change in Strategy in 2018 also result in more referrals from external agencies? Is there any evidence of an increase in external referrals? Could pent up demand come from external agencies? \n",
    "4. Why didn’t Health and Welfare cases follow the same pattern? \n",
    "4. Could the change in **Triage** (*Triage is the process of quickly examining problems to decide which ones are the most serious and must be dealt with first in thems of priority*) from May 2016 onwards which caused a spike in investigations be a factor? if so how?  \n",
    "5. Why have property and finance investigations returned to the longer term trend following the pandemic? If we assume the previous spike was caused by pent up demand , is it reasonable to assume this demand has now diminished?  \n",
    "6. Have there been any other changes since 2018 in the criteria for selecting investigations from concerns raised? If so what changed?  \n",
    "7. Did the rise in DIY and Online LPA Submissions from 2015 onwards (without legal advice) lead to more errors and misunderstandings about the responsibilities of attorneys, increasing the risk of unintentional or deliberate misuse? \n",
    "8.  Could the online tool have lead therefore to a pent up rise in concerns, that simplification of the investigation criteria in  2018 released?  \n",
    "9.  Are there any other external reasons that your aware of that would help explain the increase in property and finance cases? \n",
    "10. is publicity can cause the spike on 2018 onwards?\n",
    "\n",
    "\n",
    "## Solutions:\n",
    "1. Trend‐break detection & intervention analysis\n",
    "\n",
    "- Segmented time-series regression on monthly counts of Property & Finance (P&F) investigations to see whether there’s a statistically significant “jump” in early-2018 and again in early-2020. Estimate the change in slope and level at:\n",
    "    - April 2018 (criteria change)\n",
    "    - May 2016 (triage change)\n",
    "    - March 2020 (pandemic lockdown)\n",
    "- **Chow-test** or equivalent to verify whether the post-2018 slope is different from pre-2018 and whether post-2020 reversion is significant.\n",
    "- Chow-test Purpose: *The Chow test helps identify structural breaks in data, which are points where the relationship between variables changes*. \n",
    "- How it works: It compares the residual sum of squares (RSS) from a pooled regression model (using all data) with the RSS from separate regressions for each subset of the data. \n",
    "- Null Hypothesis: The null hypothesis of the Chow test is that the coefficients are the same across the different data subsets (or time periods). \n",
    "- Alternative Hypothesis: The alternative hypothesis is that the coefficients are different, indicating a structural break. \n",
    "- Interpretation: If the calculated F-statistic from the Chow test is greater than the critical F-value, the null hypothesis is rejected, suggesting a structural break. \n",
    "- Applications:\n",
    "    - Econometrics: Analyzing economic time series data to see if relationships between variables have changed over time, such as after a policy change or economic shock. \n",
    "    - Panel Data Analysis: Determining if the relationships between variables are consistent across different groups or individuals in a panel dataset. \n",
    "    - Regression Discontinuity Designs: Assessing whether there is a structural break at a specific cutoff point in a regression discontinuity design. \n",
    "- Example: Imagine analysing the relationship between advertising spend and sales. A Chow test could be used to see if the relationship is the same before and after a major change in marketing strategy. \n",
    "2. Registration-to-Investigation lead times\n",
    "- Compute, for each case,\n",
    "    - days_to_investigation = investigation_start_date – registration_date\n",
    "- Compare the distribution (mean, median, IQR) of these lead times by cohort year (2015, 2016, … , 2022).\n",
    "    - Expect higher mean/median in 2018–19 if pent-up cases took longer.\n",
    "    - Plot box-and-whisker by year.\n",
    "\n",
    "3. Referral source mix over time\n",
    "- Does your data capture an “origin” field—e.g. external agency vs public vs internal?\n",
    "- Trend the counts of “external referrals” by month/year.\n",
    "    - Look for a bump post-2018 in external referrals.\n",
    "    - Calculate the proportion of P&F investigations coming from external agencies vs other sources, by year.\n",
    "- If there is a spike in external referrals in 2018, that supports the “agency pent-up” hypothesis.\n",
    "\n",
    "4. Contrast P&F vs Health & Welfare\n",
    "- Repeat both the time-series and lead-time analyses for H&W cases.\n",
    "- If H&W shows no level change in 2018, that suggests the criteria change applied mainly to P&F investigations.\n",
    "\n",
    "5. Triaging change May 2016\n",
    "- Overlay a vertical marker at May 2016 on your P&F count chart and on the lead-time boxplots.\n",
    "- See whether there is a small uptick right after May 2016, and whether cases registered around then took longer.\n",
    "- Also test a 2016 “intervention” in your segmented regression.\n",
    "\n",
    "6. Pandemic reversion in 2020\n",
    "- Compare Q2 2020 onward to pre-2018 trend:\n",
    "    - Did counts simply return to the long-run trend line you’d project from 2011–2015?\n",
    "    - If pent-up demand was exhausted, you’d expect counts to revert to baseline rather than undershoot.\n",
    "\n",
    "7. Criteria changes since 2018\n",
    "- If you have metadata on “policy version” or a date when selection criteria were tweaked again, you can mark those on your time-series.\n",
    "- Even if don’t, ask stakeholders for any “minor” updates post-2018 and check for small local bumps.\n",
    "\n",
    "8. DIY & Online LPA channel analysis\n",
    "- Tag applications by channel (DIY/online vs paper/with-lawyer).\n",
    "- Trend the monthly counts of DIY/online submissions from 2015 onward.\n",
    "- Cross-tabulate: what proportion of DIY submissions result in later investigations? Is it higher in 2018–19 than previously?\n",
    "- If DIY clients misunderstand responsibilities, might see a higher error-rate flag or shorter registration-to-investigation times (i.e. they trigger investigations sooner).\n",
    "\n",
    "9. Alternative external drivers\n",
    "- Socio-economic indicators: overlay house-price indices or debt-statistics to see if there’s a macro-economic driver.\n",
    "- Media events or major public inquiries around estate mismanagement in 2018–19.\n",
    "- Staffing levels / backlog: if you have internal data on investigator headcounts or back-office capacity, a sudden hire or process improvement could temporarily clear a backlog.\n",
    "\n",
    "## Next Steps\n",
    "- Pull your master table of cases with fields:\n",
    "    - registration_date\n",
    "    - investigation_start_date\n",
    "    - case_type (P&F vs H&W)\n",
    "    - referral_source\n",
    "    - submission_channel (DIY vs lawyer)\n",
    "- Compute lead times and classify each case by year-of-registration and year-of-investigation.\n",
    "\n",
    "- Build a small dashboard with:\n",
    "    - Monthly count lines for P&F and H&W (with vertical markers at May 2016, Apr 2018, Mar 2020).\n",
    "    - Boxplots of days-to-investigate by registration year.\n",
    "    - Stacked-bars for referral source share by year.\n",
    "    - Line for DIY channel growth.\n",
    "- That suite of charts and statistical tests should equip your stakeholders to see whether 2018’s spike was:\n",
    "    - A genuine surge in new concerns,\n",
    "    - A release of a pent-up backlog,\n",
    "    - Driven by external agencies or DIY clients, or\n",
    "    - Simply a data artefact that reverted post-pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# investigation workshop \n",
    "One thing that emerged for me from the workshop was that there is lots of missing or useful information that we don’t have. For example, it was mentioned that amongst safeguarding concerns raised that many of these are cases where it is discovered that there is no registered LPA ! and also that around 20% (I think) of cases are referrals from local authorities. This is all quite significant for understanding investigation demand especially as we cannot link these cases to assumptions about living LPA donors. I would be really grateful if you could review the meeting again and identify and significant missing pieces of evidence / data etc that you think we should also be asking for; this would be incredibly helpful. I will also review again and we can then compared notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import / install the required packages\n",
    "\n",
    "#!pip install tensorflow\n",
    "#!pip uninstall -y numpy\n",
    "#!pip install \"numpy<2\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "# # 1. Hide all INFO & WARNING from TensorFlow’s C++ core (including XLA stubs).\n",
    "# #    0 = show all logs, 1 = filter INFO, 2 = filter WARNING, 3 = filter ERROR\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# # 2. Tell Google’s glog (used by XLA/CUDA) to ignore ERRORs as well.\n",
    "# #    0 = INFO+, 1 = WARNING+, 2 = ERROR+, 3 = FATAL only\n",
    "# os.environ['GLOG_minloglevel'] = '3'\n",
    "\n",
    "# # 3. (Optional) Quiet any Abseil-based logging that happens before \n",
    "# #    absl::InitializeLog() gets called.\n",
    "# os.environ['ABSL_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# # --- now import everything else ---\n",
    "# import absl.logging as absl_logging\n",
    "# # ensure the Python‐side absl logger is also at ERROR+\n",
    "# absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "# absl_logging.set_stderrthreshold('error')\n",
    "\n",
    "#import tensorflow as tf\n",
    "# and silence TF’s Python logger if it still chatters\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "# suppress that specific package RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*invalid value encountered in cast.*\"\n",
    ")\n",
    "\n",
    "\n",
    "#from IPython.display import Markdown\n",
    "#display(Markdown(\"Python Code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip uninstall -y numpy\n",
    "# !pip cache purge           # remove any cached wheels\n",
    "# !pip install numpy --upgrade\n",
    "\n",
    "# !pip uninstall -y numpy pandas matplotlib # scikit-learn\n",
    "# !pip cache purge\n",
    "# !pip install numpy pandas matplotlib #scikit-learn\n",
    "\n",
    "# import numpy\n",
    "# print(numpy.__version__)\n",
    "# numpy.show_config()\n",
    "\n",
    "# !pip uninstall -y numpy\n",
    "# !pip cache purge\n",
    "# !pip install --no-binary :all: numpy\n",
    "\n",
    "# !pip uninstall -y matplotlib\n",
    "# !pip cache purge\n",
    "# !pip install matplotlib\n",
    "\n",
    "# !rm -rf build/ dist/ *.egg-info\n",
    "\n",
    "# !pip uninstall -y numpy matplotlib pandas scikit-learn && \\\n",
    "# !pip cache purge && \\\n",
    "# !pip install numpy matplotlib pandas scikit-learn\n",
    "\n",
    "# !pip uninstall -y numpy\n",
    "# !pip cache purge\n",
    "# !pip install --no-binary :all: numpy\n",
    "# !pip uninstall -y scipy statsmodels\n",
    "# !pip cache purge\n",
    "# !pip install scipy statsmodels\n",
    "# !rm -rf build/ dist/ *.egg-info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pydbtools\n",
    "#!pip uninstall -y numpy\n",
    "#!pip install \"numpy<2\"\n",
    "#!pip install tabulate\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pydbtools as db\n",
    "import seaborn as sns\n",
    "\n",
    "# print(\"Working directory:\", os.getcwd())\n",
    "# print(\"Files here:\", os.listdir())\n",
    "\n",
    "#!pip install tabulate\n",
    "import tabulate\n",
    "\n",
    "# suppress that specific pandas RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*invalid value encountered in cast.*\"\n",
    ")\n",
    "\n",
    "# Ensure pydbtools is available\n",
    "try:\n",
    "    import pydbtools\n",
    "except ImportError:\n",
    "    raise ImportError(\"The 'pydbtools' package is required. Install it with: pip install pydbtools\")\n",
    "    \n",
    "# Working directory\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Notebooks are here:\", os.listdir())\n",
    "\n",
    "# set paths\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Registration-to-Investigation lead times\n",
    "Given linked investigation and Lasting Power of Attorney (LPA) data, can you suggest the best and most appropriate analysis for the stakeholders (OPG) to help understand what is driving investigation Pent-Up Demand and why there was a spike in Property and Finance cases (casesubtype = pfa) investigated in 2018 and 2019 which seemed to then disappear following the pandemid, especially for looking at whether there was an increase in the time from receipt of the LPA (registration_date) to investigation (date_received_in_opg) this will suffice as we are only looking at the period from say 2016 - 2023 and within that specifically 2018 and 2019? #\n",
    "\n",
    "Loads your LPA-investigation dataset.\n",
    "\n",
    "Calculates the delay between LPA registration and investigation receipt.\n",
    "\n",
    "Filters for Property & Finance cases (“pfa”) from 2016 through 2023.\n",
    "\n",
    "Generates:\n",
    "\n",
    "A time series of monthly case counts to visualize the 2018–2019 spike.\n",
    "\n",
    "A time series of monthly median delays to see if processing lag increased.\n",
    "\n",
    "Summary statistics (counts, mean/median delay) broken into pre-pandemic, spike, and post-pandemic periods.\n",
    "\n",
    "A boxplot of delay distributions by period for direct comparison.\n",
    "\n",
    "In a pre-processing step, before the above analysis, remove duplicate records using a hybrid approach to make a derived unique ID by combining the casenumber and investigations receipt date (date_received_in_opg) where this is available and use the already created Unique ID (unique_id) where the casenumber is missing.\n",
    "\n",
    "\n",
    ", follow the steps:\n",
    "\n",
    "1. import data\n",
    "2. Sort the data based on date_received_in_opg\n",
    "3. Derive tow columns of lead times:\n",
    "    - days_to_investigation = date_received_in_opg – registration_date\n",
    "    - months_to_investigation = date_received_in_opg – registration_date\n",
    "4. Compare the distribution (mean, median, IQR) of these lead times by cohort year of date_received_in_opg (2015, 2016, … , 2022).\n",
    "    - Expect higher mean/median in 2018–19 if pent-up cases took longer.\n",
    "    - Plot box-and-whisker by year.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data pre-processing: Point the filepaths to data/raw/ and load data.\n",
    "\n",
    "### 2.1. load csv file into a dataframe\n",
    "# filename = 'pre2018_linked_inv_lpa_data.csv'\n",
    "# df = pd.read_csv(os.path.join(DATA_DIR, filename), low_memory=False)\n",
    "filepath = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(filepath, low_memory=False)\n",
    "\n",
    "# Display the first few records\n",
    "df.head()\n",
    "\n",
    "### 2.2 Summary statistics & missing values\n",
    "df.info()\n",
    "df.describe(include=\"all\")\n",
    "\n",
    "### 2.3 Data Quality Checks & Solutions:\n",
    "\n",
    "#### 2.3.1 Validation: **Correct format** \n",
    "\n",
    "#### 2.3.2 Completeness: **Decisions on missing data**  \n",
    "# - Column dates → drop rows (where both dates are missing)\n",
    "# - Column X → make derieved id to detect and delete duplicates \n",
    "# - Column Y → impute median  \n",
    "\n",
    "#### 2.3.3 Uniqueness: **Decisions onduplicates** \n",
    "\n",
    "#### 2.3.1 Validation: **Correct format** \n",
    "# Convert to correct format \n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True) # force an out-of-bounds date to NaT, in addition to forcing non-dates (or non-parseable dates) to NaT\n",
    "# parses dates with the day first, e.g. \"10/11/12\" is parsed as 2012-11-10, yearfirst=True is not strict, but will prefer to parse with year first.\n",
    "\n",
    "# Count number of missing records based on missing values in 'registrationdate' 'date_received_in_opg'\n",
    "n_reg_missing = df['registrationdate'].isna().sum()\n",
    "n_opg_missing = df['date_received_in_opg'].isna().sum()\n",
    "print(f\"Missing registrationdate: {n_reg_missing}\")\n",
    "print(f\"Missing date_received_in_opg: {n_opg_missing}\")\n",
    "\n",
    "# Derive and Define year_month for monthly grouping\n",
    "df['year_month'] = df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "#df['year'] = df['date_received_in_opg'].dt.to_period('Y').dt.to_timestamp()\n",
    "df['year'] = df['date_received_in_opg'].dt.year\n",
    "df['month'] = df['date_received_in_opg'].dt.month\n",
    "df['day'] = df['date_received_in_opg'].dt.day\n",
    "\n",
    "# Compute delay_days with null assignment for invalid dates\n",
    "# If registrationdate is NaT or after receipt, delay_days = NaN\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "invalid_mask = df['registrationdate'].isna() | (df['registrationdate'] > df['date_received_in_opg'])\n",
    "df.loc[invalid_mask, 'delay_days'] = pd.NA\n",
    "\n",
    "# compute “delay in days” and then fill any missing delays with the mean delay for that calendar year \n",
    "# (falling back to the overall mean only if an entire year-group is empty):\n",
    "\n",
    "# Filter out invalid or negative delays\n",
    "# Keep rows where delay_days is non-negative, drop NaN\n",
    "df = df[df['delay_days'].notna() & (df['delay_days'] >= 0)].copy()\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "delays_missing_ids = df[df['delay_days'].isna()]['case_no']\n",
    "#print(\"delays_missing_ids: \", delays_missing_ids)\n",
    "\n",
    "df['delay_year'] = (\n",
    "    df['registrationdate'].dt.year\n",
    "    .fillna(df['date_received_in_opg'].dt.year)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Pick a “year” to group on. Use registration‐year if present, otherwise receipt‐year.\n",
    "\n",
    "# Impute missing delays with the mean for that year\n",
    "df['delay_days'] = (\n",
    "    df\n",
    "    .groupby('delay_year')['delay_days']\n",
    "    .transform(lambda s: s.fillna(s.mean()))\n",
    ")\n",
    "\n",
    "# If an entire year had only missing delays, fill those with the overall mean\n",
    "overall_mean = df['delay_days'].mean()\n",
    "df['delay_days'] = df['delay_days'].fillna(overall_mean)\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "\n",
    "imputed_delays_days = df[df['case_no'].isin(delays_missing_ids)]['delay_days']\n",
    "print(f\"imputed delays (per day): {imputed_delays_days}\")\n",
    "\n",
    "print(f\"imputed df: {df}\")\n",
    "\n",
    "# clean up (Optional) \n",
    "df.drop(columns=['delay_year'], inplace=True)\n",
    "\n",
    "\n",
    "#### 2.3.2 Completeness: **Decisions on missing data** \n",
    "# Missing Data Imputation: Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "#### 2.3.2 Uniqueness: **Decisions onduplicates:**  \n",
    "# Remove duplicates\n",
    "# Build hybrid unique ID by combining case_no + date_received_in_opg if \n",
    "# case_no is not null otherwise used unique_id to generate a derived id to remove duplicates.\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        return f\"{row['case_no']}_{row['date_received_in_opg'].strftime('%Y%m%d')}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Display processed dataframe\n",
    "print(\"The first few records:\", df.head(5))\n",
    "print(\"The last few records:\", df.tail(5))\n",
    "\n",
    "# Filter valid cases and time window\n",
    "mask = (\n",
    "    # (df['casesubtype'] == 'pfa') &\n",
    "    (df['delay_days'] >= 0) &\n",
    "    (df['date_received_in_opg'].dt.year.between(2016, 2023))\n",
    ")\n",
    "df = df.loc[mask].copy()\n",
    "\n",
    "# 3. Exploratory Data Analysis: Insert code cells for plots and summary statistics.\n",
    "\n",
    "\n",
    "# Define the target variables among the columns\n",
    "df[\"target\"] = df['delay_days']\n",
    "# df[\"target\"] = df[\"concern_type\"]\n",
    "\n",
    "### 3.1 Univariate distributions\n",
    "fig, ax = plt.subplots()\n",
    "df[\"target\"].value_counts().plot(kind=\"bar\", ax=ax)\n",
    "plt.title(\"Target distribution\")\n",
    "\n",
    "\n",
    "# ### 3.2 Bivariate relationships\n",
    "# plt.scatter(df[\"feature1\"], df[\"feature2\"])\n",
    "# plt.xlabel(\"feature1\")\n",
    "# plt.ylabel(\"feature2\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Define periods and concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016, 2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018, 2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020, 2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022, 2023]),\n",
    "}\n",
    "TYPES = ['Financial', 'Health and Welfare', 'Both']\n",
    "\n",
    "# Compute monthly max and min delay in months\n",
    "# Max monthly delay in months\n",
    "df['max_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('max') / 30.44\n",
    "# Min monthly delay in months\n",
    "df['min_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('min') / 30.44\n",
    "\n",
    "# Build DataFrame for distributions\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    subset = df.loc[mask, ['concern_type', 'delay_days', 'max_delay_months', 'min_delay_months', 'year_month']].copy()\n",
    "    subset['period'] = period\n",
    "    records.append(subset)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# Violin plots: delay_days distribution by period & concern type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(\n",
    "    data=dist_df, x='period', y='delay_days', hue='concern_type',\n",
    "    inner='quartile', cut=0\n",
    ")\n",
    "plt.title('Delay Distribution (Days) by Period & Concern Type')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xlabel('Period')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(title='Concern Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_days_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 6) Violin plots: monthly max and min delay in months by period & concern type\n",
    "# Melt for separate metrics\n",
    "melted = dist_df.melt(\n",
    "    id_vars=['period', 'concern_type'],\n",
    "    value_vars=['max_delay_months', 'min_delay_months'],\n",
    "    var_name='metric', value_name='delay_months'\n",
    ")\n",
    "# Use FacetGrid for separate panels\n",
    "g = sns.catplot(\n",
    "    data=melted, x='period', y='delay_months', hue='concern_type',\n",
    "    col='metric', kind='violin', inner='quartile', cut=0,\n",
    "    height=6, aspect=1.2, sharey=False\n",
    ")\n",
    "g.set_titles('{col_name}')\n",
    "g.set_axis_labels('Period', 'Delay (Months)')\n",
    "g._legend.set_title('Concern Type')\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_months_max_min_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 7) Line plots: percentiles per concern type separately\n",
    "percentiles = [0.25, 0.5, 0.75]\n",
    "pct_list = []\n",
    "for p in percentiles:\n",
    "    tmp = (\n",
    "        dist_df\n",
    "        .groupby(['year_month', 'concern_type'])['delay_days']\n",
    "        .quantile(p)\n",
    "        .reset_index()\n",
    "        .assign(percentile=p)\n",
    "    )\n",
    "    pct_list.append(tmp.rename(columns={'delay_days':'value'}))\n",
    "pct_df = pd.concat(pct_list, ignore_index=True)\n",
    "\n",
    "# Set seaborn style for consistent theming\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Plot separate percentile lines for each concern type\n",
    "for ctype in TYPES:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sub = pct_df[pct_df['concern_type'] == ctype]\n",
    "    sns.lineplot(\n",
    "        data=sub,\n",
    "        x='year_month',\n",
    "        y='value',\n",
    "        hue='percentile',\n",
    "        marker='o',\n",
    "        palette='tab10'\n",
    "    )\n",
    "    plt.title(f'Monthly Delay Percentiles (Days) for {ctype}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Delay (Days)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Percentile', loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'percentiles_{ctype.replace(\" \",\"_\")}.png')\n",
    "    plt.show()\n",
    "\n",
    "#  8) Line chart: delay on last available day of each month by concern type\n",
    "# For each month and concern_type, pick the record with the latest date_received_in_opg\n",
    "last_day_df = dist_df.copy()\n",
    "# Merge original df columns\n",
    "orig = df[['concern_type','year_month','date_received_in_opg','delay_days']]\n",
    "orig1 = df[df['concern_type'].isin(TYPES)]\n",
    "\n",
    "last = orig1.sort_values('date_received_in_opg').groupby(['year_month','concern_type']).tail(1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=last,\n",
    "    x='year_month', y='delay_days', hue='concern_type',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Delay on Last Available Day of Each Month by Concern Type')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Concern Type', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('last_day_delay_line.png')\n",
    "plt.show()\n",
    "\n",
    "# Line chart: delay on last available day of each month by concern type\n",
    "orig = df[['concern_type','year_month','date_received_in_opg','delay_days']]\n",
    "last = orig.sort_values('date_received_in_opg').groupby(['year_month','concern_type']).tail(1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=last, x='year_month', y='delay_days', hue='concern_type', marker='o'\n",
    ")\n",
    "plt.title('Delay on Last Day of Month by Concern Type')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Concern Type', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('last_day_delay_line.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "For each period (Pre-pandemic, Spike, Pandemic, Post-pandemic), each “violin” shows the full distribution of case delays in days, with separate colors for Financial, Health & Welfare, and Both. The white dot is the median, the thick bar is the interquartile range (25th–75th percentile), and the thin lines (“whiskers”) extend to the most extreme values not considered outliers.\n",
    "\n",
    "Shape: A violin’s width at any vertical position shows how many cases fall around that delay.\n",
    "\n",
    "Multimodality: If you see bulges at two levels, that suggests two dominant processing speeds (e.g. some cases processed quickly vs. others much slower).\n",
    "\n",
    "Comparisons: You can immediately see whether, say, Financial delays in the Spike period are shifted upward or more spread out compared to Pre-pandemic.\n",
    "\n",
    "Key takeaway:\n",
    "In 2018–19, the Financial (blue) violin does not shift right—its median and bulk of cases remain comparable to 2016–17—so investigators were handling the higher volume without systemic slowdown.\n",
    "\n",
    "2. Percentile Line Plots (25th, 50th, 75th)\n",
    "What you see:\n",
    "Separate line charts for each concern type showing, month by month, the 25th (bottom), 50th (median), and 75th (top) percentiles of delay in days.\n",
    "\n",
    "Why use it:\n",
    "\n",
    "Detailed trend: While medians alone show the “middle” experience, the 25th and 75th tell you about the faster 25% of cases and slower 25%.\n",
    "\n",
    "Distribution shifts: If the 75th percentile jumps but the median stays flat, you know the tail of slowest cases is lengthening.\n",
    "\n",
    "Seasonality or irregular spikes: You can spot individual months where delays spike, even if overall trends appear smooth.\n",
    "\n",
    "Key takeaway:\n",
    "During the 2018–19 Spike, none of these percentiles for Financial (or the other types) show a clear upward step. It’s only after 2020 that all three curves begin a sustained rise—indicating genuine slowdowns post-pandemic, not during the initial surge.\n",
    "\n",
    "Why Both Together Give Confidence\n",
    "Violin plots answer “What did the whole distribution look like in each period?”\n",
    "\n",
    "Percentile lines answer “How did the faster, middle, and slower segments of that distribution evolve over time?”\n",
    "\n",
    "By combining both, you can be confident that the 2018–19 surge in case numbers was not driven by longer investigation lead-times (i.e., pent-up demand wasn’t because cases took longer). Instead, your backlog only started to grow—and delays only lengthened—after the pandemic began.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Load & preprocess data\n",
    "filepath = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(filepath, low_memory=False)\n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "# Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "# Build hybrid unique ID and remove duplicate\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        return f\"{row['case_no']}_{row['date_received_in_opg'].strftime('%Y%m%d')}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Compute delay in days\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "# Filter valid PFA cases and time window\n",
    "mask = (\n",
    "    # (df['casesubtype'] == 'pfa') &\n",
    "    (df['delay_days'] >= 0) &\n",
    "    (df['date_received_in_opg'].dt.year.between(2016, 2023))\n",
    ")\n",
    "df = df.loc[mask].copy()\n",
    "\n",
    "# Define year_month for monthly grouping\n",
    "df['year_month'] = df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# 2) Define periods and concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016, 2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018, 2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020, 2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022, 2023]),\n",
    "}\n",
    "TYPES = ['Financial', 'Health and Welfare', 'Both']\n",
    "\n",
    "# 3) Compute monthly max and min delay in months\n",
    "# Max monthly delay in months\n",
    "df['max_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('max') / 30.44\n",
    "# Min monthly delay in months\n",
    "df['min_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('min') / 30.44\n",
    "\n",
    "# 4) Build DataFrame for distributions\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    subset = df.loc[mask, ['concern_type', 'delay_days', 'max_delay_months', 'min_delay_months', 'year_month']].copy()\n",
    "    subset['period'] = period\n",
    "    records.append(subset)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# 5) Violin plots: delay_days distribution by period & concern type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(\n",
    "    data=dist_df, x='period', y='delay_days', hue='concern_type',\n",
    "    inner='quartile', cut=0\n",
    ")\n",
    "plt.title('Delay Distribution (Days) by Period & Concern Type')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xlabel('Period')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(title='Concern Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_days_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 6) Violin plots: monthly max and min delay in months by period & concern type\n",
    "# Melt for separate metrics\n",
    "melted = dist_df.melt(\n",
    "    id_vars=['period', 'concern_type'],\n",
    "    value_vars=['max_delay_months', 'min_delay_months'],\n",
    "    var_name='metric', value_name='delay_months'\n",
    ")\n",
    "# Use FacetGrid for separate panels\n",
    "g = sns.catplot(\n",
    "    data=melted, x='period', y='delay_months', hue='concern_type',\n",
    "    col='metric', kind='violin', inner='quartile', cut=0,\n",
    "    height=6, aspect=1.2, sharey=False\n",
    ")\n",
    "g.set_titles('{col_name}')\n",
    "g.set_axis_labels('Period', 'Delay (Months)')\n",
    "g._legend.set_title('Concern Type')\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_months_max_min_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 7) Line plots: percentiles per concern type separately\n",
    "percentiles = [0.25, 0.5, 0.75]\n",
    "pct_list = []\n",
    "for p in percentiles:\n",
    "    tmp = (\n",
    "        dist_df\n",
    "        .groupby(['year_month', 'concern_type'])['delay_days']\n",
    "        .quantile(p)\n",
    "        .reset_index()\n",
    "        .assign(percentile=p)\n",
    "    )\n",
    "    pct_list.append(tmp.rename(columns={'delay_days':'value'}))\n",
    "pct_df = pd.concat(pct_list, ignore_index=True)\n",
    "\n",
    "# Set seaborn style for consistent theming\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Plot separate percentile lines for each concern type\n",
    "for ctype in TYPES:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sub = pct_df[pct_df['concern_type'] == ctype]\n",
    "    sns.lineplot(\n",
    "        data=sub,\n",
    "        x='year_month',\n",
    "        y='value',\n",
    "        hue='percentile',\n",
    "        marker='o',\n",
    "        palette='tab10'\n",
    "    )\n",
    "    plt.title(f'Monthly Delay Percentiles (Days) for {ctype}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Delay (Days)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Percentile', loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'percentiles_{ctype.replace(\" \",\"_\")}.png')\n",
    "    plt.show()\n",
    "\n",
    "#  8) Line chart: delay on last available day of each month by concern type\n",
    "# For each month and concern_type, pick the record with the latest date_received_in_opg\n",
    "last_day_df = dist_df.copy()\n",
    "# Merge original df columns\n",
    "orig = df[['concern_type','year_month','date_received_in_opg','delay_days']]\n",
    "orig1 = df[df['concern_type'].isin(TYPES)]\n",
    "\n",
    "last = orig1.sort_values('date_received_in_opg').groupby(['year_month','concern_type']).tail(1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=last,\n",
    "    x='year_month', y='delay_days', hue='concern_type',\n",
    "    marker='o'\n",
    ")\n",
    "plt.title('Delay on Last Available Day of Each Month by Concern Type')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Concern Type', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('last_day_delay_line.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Load & preprocess data\n",
    "filepath = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(filepath, low_memory=False)\n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "# Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "# Build hybrid unique ID and deduplicate\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        return f\"{row['case_no']}_{row['date_received_in_opg'].strftime('%Y%m%d')}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Compute delay in days\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "# Filter valid PFA cases and time window\n",
    "mask = (\n",
    "    # (df['casesubtype']=='pfa') &\n",
    "    (df['delay_days'] >= 0) &\n",
    "    (df['date_received_in_opg'].dt.year.between(2016,2023))\n",
    ")\n",
    "df = df.loc[mask].copy()\n",
    "\n",
    "# Define year_month for monthly grouping\n",
    "df['year_month'] = df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# 2) Define periods and concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016,2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018,2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020,2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022,2023]),\n",
    "}\n",
    "TYPES = ['Financial','Health and Welfare','Both']\n",
    "\n",
    "# 3) Compute monthly max delay for each period & concern type, convert to months\n",
    "# Group and filter rows by period and concern, then apply transform\n",
    "df['delay_months'] = df.groupby(['year_month','concern_type'])['delay_days']\\\n",
    "                       .transform('max') / 30.44\n",
    "\n",
    "# 4) Build DataFrame for full-day distributions\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    subset = df.loc[mask, ['concern_type','delay_days','delay_months','year_month']].copy()\n",
    "    subset['period'] = period\n",
    "    records.append(subset)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# 5) Violin plots: delay_days distribution by period & concern type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(\n",
    "    data=dist_df, x='period', y='delay_days', hue='concern_type',\n",
    "    inner='quartile', cut=0\n",
    ")\n",
    "plt.title('Delay Distribution (Days) by Period & Concern Type')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xlabel('Period')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(title='Concern Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_days_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 6) Violin plots: monthly max delay in months by period & concern type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(\n",
    "    data=dist_df, x='period', y='delay_months', hue='concern_type',\n",
    "    inner='quartile', cut=0\n",
    ")\n",
    "plt.title('Max Monthly Delay (Months) by Period & Concern Type')\n",
    "plt.ylabel('Delay (Months)')\n",
    "plt.xlabel('Period')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(title='Concern Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_months_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 7) Line plots: percentiles per concern type separately\n",
    "percentiles = [0.25, 0.5, 0.75]\n",
    "# Prepare percentile DataFrame\n",
    "pct_list = []\n",
    "for p in percentiles:\n",
    "    tmp = (\n",
    "        dist_df\n",
    "        .groupby(['year_month','concern_type'])['delay_days']\n",
    "        .quantile(p)\n",
    "        .reset_index()\n",
    "        .assign(percentile=p)\n",
    "    )\n",
    "    pct_list.append(tmp.rename(columns={'delay_days':'value'}))\n",
    "pct_df = pd.concat(pct_list, ignore_index=True)\n",
    "\n",
    "# Plot separate percentile lines for each concern type\n",
    "for ctype in TYPES:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(\n",
    "        data=pct_df[pct_df['concern_type']==ctype],\n",
    "        x='year_month', y='value', hue='percentile', marker='o', palette='tab10'\n",
    "    )\n",
    "    plt.title(f'Monthly Delay Percentiles (Days) for {ctype}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Delay (Days)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Percentile', loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'percentiles_{ctype.replace(\" \",\"_\")}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Load & preprocess data\n",
    "filepath = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(filepath, low_memory=False)\n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "# Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "# Build hybrid unique ID and dedup\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        date_str = row['date_received_in_opg'].strftime('%Y%m%d')\n",
    "        return f\"{row['case_no']}_{date_str}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Compute delay in days\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "# Filter invalid delays and PFA cases in period\n",
    "mask = (\n",
    "    # df['casesubtype']=='pfa' &\n",
    "    df['delay_days'].notna() &\n",
    "    df['date_received_in_opg'].dt.year.between(2016,2023)\n",
    ")\n",
    "df = df.loc[mask].copy()\n",
    "\n",
    "# 2) Define analysis periods and core concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016,2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018,2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020,2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022,2023]),\n",
    "}\n",
    "TYPES = ['Financial','Health and Welfare','Both']\n",
    "\n",
    "# 3) Build a distribution DataFrame including year_month for time series\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    subset = df.loc[mask, ['concern_type','delay_days','date_received_in_opg']].copy()\n",
    "    subset['period'] = period\n",
    "    subset['year_month'] = subset['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "    records.append(subset)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# 4) Violin plots: full distribution of delays (days) by period for each concern type\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, ctype in enumerate(TYPES, start=1):\n",
    "    ax = plt.subplot(2, 2, i)\n",
    "    sns.violinplot(\n",
    "        data=dist_df[dist_df['concern_type']==ctype],\n",
    "        x='period', y='delay_days', inner='quartile', cut=0, ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{ctype}: Delay Distribution by Period\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Delay (Days)')\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_violins_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 5) Boxplots: visualize distribution shape with quartiles and outliers\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, ctype in enumerate(TYPES, start=1):\n",
    "    ax = plt.subplot(2, 2, i)\n",
    "    sns.boxplot(\n",
    "        data=dist_df[dist_df['concern_type']==ctype],\n",
    "        x='period', y='delay_days', ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{ctype}: Delay Boxplot by Period\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Delay (Days)')\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_boxplots_by_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 6) Percentile time series: compute monthly delay percentiles for each concern type\n",
    "percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "pct_df_list = []\n",
    "for p in percentiles:\n",
    "    tmp = (\n",
    "        dist_df\n",
    "        .groupby(['year_month','concern_type'])['delay_days']\n",
    "        .quantile(p)\n",
    "        .reset_index()\n",
    "        .assign(percentile=p)\n",
    "    )\n",
    "    pct_df_list.append(tmp)\n",
    "pct_df = pd.concat(pct_df_list, ignore_index=True)\n",
    "\n",
    "# 7) Line plots: monthly delay percentile curves by concern type\n",
    "for ctype in TYPES:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(\n",
    "        data=pct_df[pct_df['concern_type']==ctype],\n",
    "        x='year_month', y='delay_days', hue='percentile', marker='o'\n",
    "    )\n",
    "    plt.title(f\"{ctype}: Monthly Delay Percentiles (10th–90th)\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Delay (Days)')\n",
    "    plt.legend(title='Percentile')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    filename = f\"percentile_lines_{ctype.replace(' ','_')}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TimedeltaPropertiesTimedeltaPropertiesTimedeltaProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df['date_received_in_opg'] - df['registrationdate']).dt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Load & preprocess data\n",
    "df = pd.read_csv('pre2018_linked_inv_lpa_data.csv', low_memory=False)\n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "df = df[df['registrationdate'].notna()]\n",
    "\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        ds = row['date_received_in_opg']\n",
    "        date_str = ds.strftime('%Y%m%d') if pd.notna(ds) else 'NA'\n",
    "        return f\"{row['case_no']}_{date_str}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id', keep='first')\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "df = df.dropna(subset=['delay_days'])\n",
    "\n",
    "#df = df[df['casesubtype']=='pfa']\n",
    "df = df[df['date_received_in_opg'].dt.year.between(2016,2023)]\n",
    "\n",
    "# 2) Define periods and concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016,2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018,2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020,2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022,2023]),\n",
    "}\n",
    "TYPES = ['Financial','Health and Welfare','Both']\n",
    "\n",
    "# 3) Create distribution DataFrame with date_received for median line\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    sub = df.loc[mask, ['concern_type','delay_days','date_received_in_opg']].copy()\n",
    "    sub['period'] = period\n",
    "    records.append(sub)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# 4) Plot violin distributions by concern type\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, c in enumerate(TYPES,1):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.violinplot(\n",
    "        data=dist_df[dist_df['concern_type']==c],\n",
    "        x='period', y='delay_days', inner='quartile', cut=0\n",
    "    )\n",
    "    plt.title(f\"{c}: Delay Distribution\")\n",
    "    plt.ylabel('Delay (Days)')\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('violins_by_type_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 5) Monthly median line plot per concern type\n",
    "# Compute year_month on original dist_df\n",
    "dist_df['year_month'] = dist_df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "med_monthly = (\n",
    "    dist_df.groupby(['year_month','concern_type'])\n",
    "           .delay_days.median()\n",
    "           .reset_index()\n",
    ")\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(\n",
    "    data=med_monthly, x='year_month', y='delay_days', hue='concern_type', marker='o'\n",
    ")\n",
    "plt.title('Monthly Median Delay by Concern Type')\n",
    "plt.xlabel('Receipt Month')\n",
    "plt.ylabel('Median Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('median_delay_line_by_type.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Load & Preprocess\n",
    "df = pd.read_csv('pre2018_linked_inv_lpa_data.csv', low_memory=False)\n",
    "\n",
    "# Parse dates\n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Drop rows missing registration date\n",
    "df = df[df['registrationdate'].notna()]\n",
    "\n",
    "# Build hybrid unique ID and drop duplicates\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        ds = row['date_received_in_opg']\n",
    "        date_str = ds.strftime('%Y%m%d') if pd.notna(ds) else 'NA'\n",
    "        return f\"{row['case_no']}_{date_str}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id', keep='first')\n",
    "\n",
    "# Compute delay in days and drop invalid\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "df = df.dropna(subset=['delay_days'])\n",
    "\n",
    "# Filter to PFA cases and years 2016–2023\n",
    "df = df[df['casesubtype']=='pfa']\n",
    "df = df[df['date_received_in_opg'].dt.year.between(2016,2023)]\n",
    "\n",
    "# 2) Define periods\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   {'reg_end':2017, 'rec_years':[2016,2017]},\n",
    "    'Spike (2018–19)':          {'reg_end':2019, 'rec_years':[2018,2019]},\n",
    "    'Pandemic (2020–21)':       {'reg_end':2021, 'rec_years':[2020,2021]},\n",
    "    'Post-pandemic (2022–23)':  {'reg_end':2023, 'rec_years':[2022,2023]},\n",
    "}\n",
    "\n",
    "# Our focus types\n",
    "TYPES = ['Financial','Health and Welfare','Both']\n",
    "\n",
    "# 3) Build a long DataFrame of delay distributions\n",
    "records = []\n",
    "for period_name, cfg in PERIODS.items():\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= cfg['reg_end']) &\n",
    "        (df['date_received_in_opg'].dt.year.isin(cfg['rec_years']))\n",
    "    )\n",
    "    sub = df.loc[mask & df['concern_type'].isin(TYPES), ['concern_type','delay_days']]\n",
    "    sub = sub.assign(period=period_name)\n",
    "    records.append(sub)\n",
    "\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# 4) Summary table: counts + basic stats by period & type\n",
    "summary = (\n",
    "    dist_df\n",
    "    .groupby(['period','concern_type'])\n",
    "    .delay_days\n",
    "    .agg(case_count='count',\n",
    "         mean_delay='mean',\n",
    "         median_delay='median',\n",
    "         std_delay='std',\n",
    "         p25=lambda x: x.quantile(0.25),\n",
    "         p75=lambda x: x.quantile(0.75))\n",
    "    .reset_index()\n",
    ")\n",
    "print(\"\\nSummary by Period & Concern Type:\\n\")\n",
    "print(summary.to_markdown(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary.to_csv('pfa_leadtime_summary_by_period_type.csv', index=False)\n",
    "\n",
    "# 5) Violin plots: one subplot per concern_type\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, ctype in enumerate(TYPES, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.violinplot(\n",
    "        data=dist_df[dist_df['concern_type']==ctype],\n",
    "        x='period', y='delay_days', inner='quartile'\n",
    "    )\n",
    "    plt.title(f\"{ctype} – Delay Distribution\")\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Delay (Days)')\n",
    "    plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('delay_distribution_violins.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset (update path as needed)\n",
    "pre2018_linked_inv_lpa_data = pd.read_csv('pre2018_linked_inv_lpa_data.csv', low_memory=False) #, parse_dates=['registrationdate', 'date_received_in_opg'])\n",
    "df = pre2018_linked_inv_lpa_data\n",
    "\n",
    "# Explicitly convert to datetime, coercing invalids to NaT\n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "\n",
    "# (Optional) Check dtypes to confirm\n",
    "#print(df[['registrationdate', 'date_received_in_opg']].dtypes)\n",
    "\n",
    "# return a new sorted DataFrame\n",
    "df_sorted = df.sort_values(by='date_received_in_opg', ascending=True)\n",
    "print(df_sorted[['registrationdate', 'date_received_in_opg']].tail(20))\n",
    "\n",
    "# Drop any rows where even the registrationdate is missing\n",
    "df = df[df['registrationdate'].notna()]\n",
    "\n",
    "# Build a hybrid unique key: \n",
    "#    If case_no exists, use \"case_no_YYYYMMDD\"; else use unique_id.\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip() != '':\n",
    "        # format date as YYYYMMDD; nan dates become \"NaT\", but those rows will drop later\n",
    "        date_str = row['date_received_in_opg'].strftime('%Y%m%d') if pd.notna(row['date_received_in_opg']) else 'NA'\n",
    "        return f\"{row['case_no']}_{date_str}\"\n",
    "    else:\n",
    "        return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "\n",
    "# Drop any rows where even the derived_id couldn’t be built (both missing)\n",
    "df = df[df['derived_id'].notna()]\n",
    "\n",
    "# Remove duplicate records based on that derived key, keeping the first\n",
    "df = df.drop_duplicates(subset='derived_id', keep='first')\n",
    "\n",
    "# Compute delay in days\n",
    "# Compute delay between registration and investigation receipt\n",
    "#    Subtracting two pd.Timestamp series now yields a timedelta series\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "\n",
    "# Drop any rows where the dates were invalid and delay is NaN\n",
    "df = df.dropna(subset=['delay_days'])\n",
    "\n",
    "# Filter to the period 2016–2023 and casesubtype 'pfa'\n",
    "mask = (\n",
    "    df['date_received_in_opg'].dt.year.between(2016, 2023) &\n",
    "    (df['casesubtype'] == 'pfa')\n",
    ")\n",
    "df_pfa = df.loc[mask].copy()\n",
    "\n",
    "# Create a year-month column for time-series grouping\n",
    "df_pfa['year_month'] = df_pfa['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Monthly counts and median delay\n",
    "monthly_stats = (\n",
    "    df_pfa\n",
    "      .groupby('year_month')\n",
    "      .agg(case_count=('unique_id', 'count'),\n",
    "           median_delay=('delay_days', 'mean'))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# 1) Monthly PFA Investigation Case Counts\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(monthly_stats['year_month'], monthly_stats['case_count'])\n",
    "plt.title('Monthly PFA Investigation Case Counts (2016–2023)')\n",
    "plt.xlabel('Receipt Month')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Monthly PFA Investigation Case Counts (2016–2023).png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2) Monthly Median Delay from Registration to Investigation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(monthly_stats['year_month'], monthly_stats['median_delay'])\n",
    "plt.title('Monthly Mean Delay: Registration → Investigation (Days)')\n",
    "plt.xlabel('Receipt Month')\n",
    "plt.ylabel('Median Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Monthly Median Delay from Registration to Investigation.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define pandemic periods for comparison\n",
    "def assign_period(year):\n",
    "    if year in (2016, 2017):\n",
    "        return 'Pre-pandemic (2016–2017)'\n",
    "    elif year in (2018, 2019):\n",
    "        return 'Spike Period (2018–2019)'\n",
    "    elif year in (2020, 2021):\n",
    "        return 'Pandemic (2020–2021)'\n",
    "    else:\n",
    "        return 'Post-pandemic (2022–2023)'\n",
    "\n",
    "df_pfa['period'] = df_pfa['date_received_in_opg'].dt.year.apply(assign_period)\n",
    "\n",
    "# Summary statistics by period\n",
    "period_stats = (\n",
    "    df_pfa\n",
    "      .groupby('period')\n",
    "      .agg(\n",
    "          total_cases=('unique_id', 'count'),\n",
    "          mean_delay=('delay_days', 'mean'),\n",
    "          median_delay=('delay_days', 'median')\n",
    "      )\n",
    "      .reindex([\n",
    "          'Pre-pandemic (2016–2017)',\n",
    "          'Spike Period (2018–2019)',\n",
    "          'Pandemic (2020–2021)',\n",
    "          'Post-pandemic (2022–2023)'\n",
    "      ])\n",
    ")\n",
    "\n",
    "# Print the summary table\n",
    "print('\\nPFA Investigation Summary by Period (2016–2023):\\n')\n",
    "print(period_stats.to_markdown())\n",
    "\n",
    "# Delay Distribution by Period (boxplot)\n",
    "plt.figure(figsize=(8, 5))\n",
    "df_pfa.boxplot(column='delay_days', by='period', grid=False)\n",
    "plt.title('Delay from Registration to Investigation by Period')\n",
    "plt.suptitle('')  # remove automatic subtitle\n",
    "plt.xlabel('Period')\n",
    "plt.ylabel('Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Delay from Registration to Investigation by Period.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1614+1487+2940\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "877+1860+3304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_missing.shape[0]\n",
    "#df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the four periods of interest:\n",
    "PERIODS = {\n",
    "    'Pre-pandemic\\n(2016–2017)': {\n",
    "        'reg_end': 2017,\n",
    "        'rec_years': [2016, 2017]\n",
    "    },\n",
    "    'Spike period\\n(2018–2019)': {\n",
    "        'reg_end': 2019,\n",
    "        'rec_years': [2018, 2019]\n",
    "    },\n",
    "    'Pandemic\\n(2020–2021)': {\n",
    "        'reg_end': 2021,\n",
    "        'rec_years': [2020, 2021]\n",
    "    },\n",
    "    'Post-pandemic\\n(2022–2023)': {\n",
    "        'reg_end': 2023,\n",
    "        'rec_years': [2022, 2023]\n",
    "    },\n",
    "}\n",
    "\n",
    "# All rows where concern_type is missing:\n",
    "df_missing = df[df['concern_type'].isna()]\n",
    "print(f\"Number of Missing records: {df_missing.shape[0]}\")\n",
    "\n",
    "# All rows where concern_type is _present_:\n",
    "df_present = df[df['concern_type'].notna()]\n",
    "print(f\"Number of records excluding Missing records: {df_present.shape[0]}\")\n",
    "\n",
    "# Pull actual, non-null concern types from the data\n",
    "CONCERN_TYPES = df_pfa['concern_type'].dropna().unique().tolist()\n",
    "#CONCERN_TYPES = df_pfa['concern_type'].unique().tolist()\n",
    "\n",
    "def analyze_period(df, period_name, reg_end, rec_years):\n",
    "    \"\"\"\n",
    "    For a given period, count and compute lead-time stats for:\n",
    "      - all PFA cases with registration_date.year <= reg_end\n",
    "        and date_received_in_opg.year in rec_years,\n",
    "        broken out by concern_type + overall.\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        (df['registrationdate'].dt.year <= reg_end) &\n",
    "        (df['date_received_in_opg'].dt.year.isin(rec_years))\n",
    "    )\n",
    "    sub = df.loc[mask].copy()\n",
    "    \n",
    "    results = []\n",
    "    for ct in CONCERN_TYPES:\n",
    "        if ct:\n",
    "            st = sub[sub['concern_type'] == ct]\n",
    "        else:\n",
    "            st = sub\n",
    "        count = st.shape[0]\n",
    "        mean_delay = st['delay_days'].mean()\n",
    "        median_delay = st['delay_days'].median()\n",
    "        results.append({\n",
    "            'period': period_name,\n",
    "            'concern_type': ct,\n",
    "            'case_count': count,\n",
    "            'mean_delay': mean_delay,\n",
    "            'median_delay': median_delay\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def investigate_drivers(df):\n",
    "    \"\"\"\n",
    "    Run analyze_period over all defined PERIODS and concatenate results.\n",
    "    \"\"\"\n",
    "    all_periods = []\n",
    "    for period_name, cfg in PERIODS.items():\n",
    "        df_res = analyze_period(df, period_name, cfg['reg_end'], cfg['rec_years'])\n",
    "        all_periods.append(df_res)\n",
    "    return pd.concat(all_periods, ignore_index=True)\n",
    "\n",
    "      \n",
    "# # Run driver investigation\n",
    "# driver_stats = investigate_drivers(df)\n",
    "# print(f\"All Period Data: {driver_stats}\")\n",
    "\n",
    "# Run driver investigation on PFA subset\n",
    "driver_stats = investigate_drivers(df_pfa)\n",
    "\n",
    "# “wide” format: each concern_type becomes its own column block\n",
    "wide = driver_stats.pivot_table(\n",
    "    index='period',\n",
    "    columns='concern_type',\n",
    "    values=['case_count', 'mean_delay', 'median_delay']\n",
    ")\n",
    "\n",
    "print(wide)\n",
    "# Flatten the MultiIndex columns\n",
    "wide.columns = [f\"{stat} ({ct})\" for stat, ct in wide.columns]\n",
    "wide = wide.reset_index()\n",
    "print(wide)\n",
    "\n",
    "print(\"\\nWide-format table:\\n\")\n",
    "# Print or save to CSV for further analysis\n",
    "print(\"\\nDrivers of 2018–19 Spike & Comparative Lead Times:\\n\")\n",
    "print(wide.to_markdown(index=False))\n",
    "\n",
    "# # Pivot into a table form that's easy to read:\n",
    "# table = driver_stats.pivot(\n",
    "#     index=['period', 'concern_type'],\n",
    "#     values=['case_count', 'mean_delay', 'median_delay']\n",
    "# ).reset_index()\n",
    "\n",
    "# # Print or save to CSV for further analysis\n",
    "# print(\"\\nDrivers of 2018–19 Spike & Comparative Lead Times:\\n\")\n",
    "# print(table.to_markdown(index=False))\n",
    "wide.to_csv('pfa_driver_stats_by_period_concern.csv', index=False)\n",
    "\n",
    "# A quick heatmap-style plot of case counts by period & concern_type\n",
    "import seaborn as sns\n",
    "\n",
    "# Corrected pivot for heatmap\n",
    "heatmap_data = driver_stats.pivot(\n",
    "    index='concern_type',\n",
    "    columns='period',\n",
    "    values='case_count'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\"\n",
    ")\n",
    "plt.title('Case Counts by Period & Concern Type')\n",
    "plt.ylabel('Concern Type')\n",
    "plt.xlabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('driver_case_counts_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reconstruct driver_stats DataFrame from provided summary\n",
    "data = [\n",
    "    # period, concern_type, case_count, mean_delay, median_delay\n",
    "    ('Pandemic (2020–2021)', 'Both',   1558,  984.404,  748),\n",
    "    ('Pandemic (2020–2021)', 'Financial', 2237, 1209.320, 978),\n",
    "    ('Pandemic (2020–2021)', 'Health and Welfare', 143, 1162.340, 952),\n",
    "    ('Pandemic (2020–2021)', 'Multiple Sub', 2, 1694.500, 1694.5),\n",
    "    ('Pandemic (2020–2021)', 'Third Party', 0, float('nan'), float('nan')),\n",
    "    ('Pandemic (2020–2021)', 'Unclear', 0, float('nan'), float('nan')),\n",
    "    ('Pandemic (2020–2021)', 'Unknown', 0, float('nan'), float('nan')),\n",
    "    \n",
    "    ('Post-pandemic (2022–2023)', 'Both',   1418, 1119.880, 857),\n",
    "    ('Post-pandemic (2022–2023)', 'Financial', 2552, 1344.350, 1144),\n",
    "    ('Post-pandemic (2022–2023)', 'Health and Welfare', 169, 1221.350, 934),\n",
    "    ('Post-pandemic (2022–2023)', 'Multiple Sub', 0, float('nan'), float('nan')),\n",
    "    ('Post-pandemic (2022–2023)', 'Third Party', 0, float('nan'), float('nan')),\n",
    "    ('Post-pandemic (2022–2023)', 'Unclear', 0, float('nan'), float('nan')),\n",
    "    ('Post-pandemic (2022–2023)', 'Unknown', 0, float('nan'), float('nan')),\n",
    "    \n",
    "    ('Pre-pandemic (2016–2017)', 'Both',   586,  658.072, 438.5),\n",
    "    ('Pre-pandemic (2016–2017)', 'Financial', 1425, 994.128, 842),\n",
    "    ('Pre-pandemic (2016–2017)', 'Health and Welfare', 183, 1069.810, 755),\n",
    "    ('Pre-pandemic (2016–2017)', 'Multiple Sub', 0, float('nan'), float('nan')),\n",
    "    ('Pre-pandemic (2016–2017)', 'Third Party', 0, float('nan'), float('nan')),\n",
    "    ('Pre-pandemic (2016–2017)', 'Unclear', 50, 650.340, 344.5),\n",
    "    ('Pre-pandemic (2016–2017)', 'Unknown', 35, 590.171, 221),\n",
    "    \n",
    "    ('Spike period (2018–2019)', 'Both',   1141, 709.362, 504),\n",
    "    ('Spike period (2018–2019)', 'Financial', 3384, 957.871, 742),\n",
    "    ('Spike period (2018–2019)', 'Health and Welfare', 121, 943.405, 714),\n",
    "    ('Spike period (2018–2019)', 'Multiple Sub', 0, float('nan'), float('nan')),\n",
    "    ('Spike period (2018–2019)', 'Third Party', 1, 2283.000, 2283),\n",
    "    ('Spike period (2018–2019)', 'Unclear', 0, float('nan'), float('nan')),\n",
    "    ('Spike period (2018–2019)', 'Unknown', 0, float('nan'), float('nan')),\n",
    "]\n",
    "\n",
    "driver_stats = pd.DataFrame(data, columns=[\n",
    "    'period', 'concern_type', 'case_count', 'mean_delay', 'median_delay'\n",
    "])\n",
    "\n",
    "# Filter out nan concern types\n",
    "driver_stats = driver_stats[driver_stats['case_count'] > 0]\n",
    "\n",
    "# 1) Grouped bar chart: case counts by concern type and period\n",
    "counts_pivot = driver_stats.pivot(\n",
    "    index='concern_type', columns='period', values='case_count'\n",
    ")\n",
    "counts_pivot.plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('PFA Case Counts by Period & Concern Type')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('case_counts_by_concern_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 2) Grouped bar chart: median delay by concern type and period\n",
    "median_pivot = driver_stats.pivot(\n",
    "    index='concern_type', columns='period', values='median_delay'\n",
    ")\n",
    "median_pivot.plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Median Delay by Period & Concern Type')\n",
    "plt.ylabel('Median Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('median_delay_by_concern_period.png')\n",
    "plt.show()\n",
    "\n",
    "# 3) Box plot: delay distribution for each period across all concern types\n",
    "# (recreating a synthetic distribution for illustration)\n",
    "plt.figure(figsize=(8, 5))\n",
    "box_data = [driver_stats[driver_stats['period']==p]['median_delay'].dropna()\n",
    "            for p in driver_stats['period'].unique()]\n",
    "plt.boxplot(box_data, labels=driver_stats['period'].unique(), showmeans=True)\n",
    "plt.title('Synthetic Boxplot of Median Delays by Period')\n",
    "plt.ylabel('Median Delay (Days)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplot_delay_by_period.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inv_linked_lpa_query = f\"\"\"\n",
    "    -- 1. Precompute all MAX dates once\n",
    "    WITH\n",
    "      latest_hist AS (\n",
    "        SELECT MAX(mojap_extract_date) AS d\n",
    "        FROM opg_investigations_prod.historic_investigations\n",
    "      ),\n",
    "      latest_cur AS (\n",
    "        SELECT MAX(mojap_extract_date) AS d\n",
    "        FROM opg_investigations_prod.investigations\n",
    "      ),\n",
    "      latest_warn AS (\n",
    "        SELECT MAX(glueexporteddate) AS d\n",
    "        FROM opg_sirius_prod.warnings\n",
    "      ),\n",
    "      latest_pw AS (\n",
    "        SELECT MAX(glueexporteddate) AS d\n",
    "        FROM opg_sirius_prod.person_warning\n",
    "      ),\n",
    "      latest_case_export AS (\n",
    "        SELECT MAX(glueexporteddate) AS d\n",
    "        FROM opg_sirius_prod.cases\n",
    "      ),\n",
    "      latest_person_export AS (\n",
    "        SELECT MAX(glueexporteddate) AS d\n",
    "        FROM opg_sirius_prod.persons\n",
    "      ),\n",
    "      latest_addr_export AS (\n",
    "        SELECT MAX(glueexporteddate) AS d\n",
    "        FROM opg_sirius_prod.addresses\n",
    "      ),\n",
    "\n",
    "    -- 2. Union historic + current investigations on their latest extract dates\n",
    "      investigations_union AS (\n",
    "        SELECT\n",
    "          id,                                            -- record ID\n",
    "          CONCAT('H', CAST(unique_id AS varchar)) AS unique_id,\n",
    "          case_no, investigator, team, reallocated_case,\n",
    "          weighting, client_donor_title,\n",
    "          client_donor_forename, client_donor_surname,\n",
    "          dob AS client_donor_dob, risk, case_type,\n",
    "          concern_type, status, sub_status,\n",
    "          date_received_in_opg, date_received_in_investigations,\n",
    "          date_allocated_to_team, date_allocated_to_current_investigator,\n",
    "          anticipated_completion_date, pg_sign_off_date,\n",
    "          days_on_hold, currently_hold_from, multiple_id,\n",
    "          lead_case, days_to_pg_sign_off, closure_date,\n",
    "          pg_sign_off_hold_days, pg_sign_off_to_close_days,\n",
    "          last_status, referals_made_by_itas,\n",
    "          high_risk_from, days_at_high_risk,\n",
    "          recommended_court_outcome,\n",
    "          date_of_legal_review_request_1, date_legal_rejects_1,\n",
    "          reason_for_rejection_1, legal_risk_rejection_1,\n",
    "          date_of_legal_review_request_2, date_legal_rejects_2,\n",
    "          reason_for_rejection_2, legal_risk_rejection_2,\n",
    "          date_of_legel_review_request_3, lcr_request_no,\n",
    "          times_lawyers_allocated_1_reallocated_case,\n",
    "          legal_approval_date, legal_risk,\n",
    "          date_sent_to_ca, ca_acceptance_type, lawyer,\n",
    "          allocated_to_solicitor_date, legal_team,\n",
    "          pg_signatory, court_outcome, date_of_order,\n",
    "          pgs_addendum_date, flagged_date, flagged_type,\n",
    "          flag_lawyer, mojap_extract_date\n",
    "        FROM opg_investigations_prod.historic_investigations\n",
    "        WHERE mojap_extract_date = (SELECT d FROM latest_hist)\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT\n",
    "          id,\n",
    "          CONCAT('I', CAST(id AS varchar)) AS unique_id,\n",
    "          case_no, investigator, team, reallocated_case,\n",
    "          weighting, client_donor_title,\n",
    "          client_donor_forename, client_donor_surname,\n",
    "          dob AS client_donor_dob, risk, case_type,\n",
    "          concern_type, status, sub_status,\n",
    "          date_received_in_opg, date_received_in_investigations,\n",
    "          date_allocated_to_team, date_allocated_to_current_investigator,\n",
    "          anticipated_completion_date, pg_sign_off_date,\n",
    "          days_on_hold, currently_hold_from, multiple_id,\n",
    "          lead_case, days_to_pg_sign_off, closure_date,\n",
    "          pg_sign_off_hold_days, pg_sign_off_to_close_days,\n",
    "          last_status, referals_made_by_itas,\n",
    "          high_risk_from, days_at_high_risk,\n",
    "          recommended_court_outcome,\n",
    "          date_of_legal_review_request_1, date_legal_rejects_1,\n",
    "          reason_for_rejection_1, legal_risk_rejection_1,\n",
    "          date_of_legal_review_request_2, date_legal_rejects_2,\n",
    "          reason_for_rejection_2, legal_risk_rejection_2,\n",
    "          date_of_legel_review_request_3, lcr_request_no,\n",
    "          times_lawyers_allocated_1_reallocated_case,\n",
    "          legal_approval_date, legal_risk,\n",
    "          date_sent_to_ca, ca_acceptance_type, lawyer,\n",
    "          allocated_to_solicitor_date, legal_team,\n",
    "          pg_signatory, court_outcome, date_of_order,\n",
    "          pgs_addendum_date, flagged_date, flagged_type,\n",
    "          flag_lawyer, mojap_extract_date\n",
    "        FROM opg_investigations_prod.investigations\n",
    "        WHERE mojap_extract_date = (SELECT d FROM latest_cur)\n",
    "      ),\n",
    "\n",
    "    -- 3. Explode all numeric/T parts from case_no in one step\n",
    "      exploded AS (\n",
    "        SELECT\n",
    "          iu.*,\n",
    "          part\n",
    "        FROM investigations_union iu\n",
    "        CROSS JOIN UNNEST(\n",
    "          regexp_split(                                  -- split on any non-[0-9Tt] chars\n",
    "            regexp_replace(iu.case_no, '[^0-9Tt,]+', ''), ','\n",
    "          )\n",
    "        ) AS t(part)\n",
    "        WHERE part <> ''                                  -- drop empty\n",
    "      ),\n",
    "\n",
    "    -- 4. MERIS → case UID using latest export dates\n",
    "      meris_number AS (\n",
    "        SELECT\n",
    "          SUBSTRING(w.warningtext, 36, 7) AS meris_number,\n",
    "          c.uid\n",
    "        FROM opg_sirius_prod.warnings w\n",
    "        JOIN opg_sirius_prod.person_warning pw\n",
    "          ON pw.warning_id = w.id\n",
    "         AND pw.glueexporteddate = (SELECT d FROM latest_pw)\n",
    "        JOIN opg_sirius_prod.cases c\n",
    "          ON pw.person_id = c.donor_id\n",
    "         AND c.glueexporteddate = (SELECT d FROM latest_case_export)\n",
    "        WHERE w.glueexporteddate = (SELECT d FROM latest_warn)\n",
    "          AND w.warningtype = 'Migrated case'\n",
    "      ),\n",
    "\n",
    "    -- 5. Person → case UID\n",
    "      person_to_case AS (\n",
    "        SELECT\n",
    "          p.uid AS p_uid,\n",
    "          c.uid AS c_uid\n",
    "        FROM opg_sirius_prod.persons p\n",
    "        JOIN opg_sirius_prod.cases c\n",
    "          ON p.id = c.donor_id\n",
    "         AND p.glueexporteddate = (SELECT d FROM latest_person_export)\n",
    "         AND c.glueexporteddate = (SELECT d FROM latest_case_export)\n",
    "      ),\n",
    "\n",
    "    -- 6. Rank orders once\n",
    "      order_uid AS (\n",
    "        SELECT\n",
    "          caserecnumber,\n",
    "          uid,\n",
    "          ROW_NUMBER() OVER (\n",
    "            PARTITION BY caserecnumber\n",
    "            ORDER BY\n",
    "              CASE\n",
    "                WHEN orderstatus = 'ACTIVE'    THEN 1\n",
    "                WHEN orderstatus = 'OPEN'      THEN 2\n",
    "                WHEN orderstatus = 'CLOSED'    THEN 3\n",
    "                WHEN orderstatus = 'DUPLICATE' THEN 4\n",
    "                ELSE 5\n",
    "              END,\n",
    "              orderdate DESC\n",
    "          ) AS rn\n",
    "        FROM opg_sirius_prod.cases\n",
    "        WHERE glueexporteddate = (SELECT d FROM latest_case_export)\n",
    "          AND lower(casesubtype) IN ('hw','pfa')\n",
    "          AND ordersubtype   <> 'SUPPLEMENTARY'\n",
    "          AND lower(casetype) = 'order'\n",
    "      ),\n",
    "\n",
    "    -- 7. Tie exploded → case UID via all lookups\n",
    "      inv_to_case AS (\n",
    "        SELECT\n",
    "          e.unique_id,\n",
    "          e.part,\n",
    "          COALESCE(\n",
    "            CASE WHEN LENGTH(e.part) = 12 THEN e.part END,\n",
    "            pc.c_uid,\n",
    "            CASE WHEN ou.rn = 1 THEN ou.uid END,\n",
    "            mn.uid\n",
    "          ) AS case_uid\n",
    "        FROM exploded e\n",
    "        LEFT JOIN order_uid ou     ON ou.caserecnumber = e.part AND ou.rn = 1\n",
    "        LEFT JOIN meris_number mn  ON mn.meris_number = e.part\n",
    "        LEFT JOIN person_to_case pc ON CAST(pc.p_uid AS varchar) = e.part\n",
    "      )\n",
    "\n",
    "    -- 8. Final select with demographics, address & case joins\n",
    "    SELECT\n",
    "      YEAR(CAST(i.date_received_in_opg AS DATE)) AS year,  -- extract year\n",
    "      MONTH(CAST(i.date_received_in_opg AS DATE)) AS month,-- extract month\n",
    "      CASE                                                 -- compute age\n",
    "        WHEN FLOOR(\n",
    "          DATE_DIFF(\n",
    "            'day',\n",
    "            COALESCE(p.dob, CAST(SUBSTR(i.client_donor_dob,1,10) AS DATE)),\n",
    "            CAST(i.date_received_in_opg AS DATE)\n",
    "          ) / 365.25\n",
    "        ) < 0 THEN 0\n",
    "        ELSE ROUND(\n",
    "          DATE_DIFF(\n",
    "            'day',\n",
    "            COALESCE(p.dob, CAST(SUBSTR(i.client_donor_dob,1,10) AS DATE)),\n",
    "            CAST(i.date_received_in_opg AS DATE)\n",
    "          ) / 365.25\n",
    "        )\n",
    "      END AS age,\n",
    "      CASE                                                 -- gender from title\n",
    "        WHEN i.client_donor_title = 'Mr'   THEN 'M'\n",
    "        WHEN i.client_donor_title IN ('Mrs','Miss') THEN 'F'\n",
    "        ELSE NULL\n",
    "      END AS gender,\n",
    "      CASE                                                 -- case status logic\n",
    "        WHEN c.casetype = 'LPA'   THEN c.status\n",
    "        WHEN c.casetype = 'ORDER' THEN c.orderstatus\n",
    "      END AS case_status,\n",
    "      c.casetype     AS poa_case_type,                     -- case type\n",
    "      c.casesubtype,                                       -- case subtype\n",
    "      p.dob,                                               -- DOB\n",
    "      ad.postcode,                                         -- postcode\n",
    "      ad.county,                                           -- county\n",
    "      ad.town,                                             -- town\n",
    "      i.*,                                                 -- all investigation cols\n",
    "      icu.case_uid,                                        -- resolved case UID\n",
    "      COALESCE(                                            -- receipt date logic\n",
    "        CASE WHEN c.casetype = 'ORDER' THEN DATE(c.orderdate) END,\n",
    "        c.receiptdate\n",
    "      ) AS receiptdate,\n",
    "      c.registrationdate                                   -- registration date\n",
    "    FROM investigations_union i\n",
    "    LEFT JOIN inv_to_case icu           ON icu.unique_id = i.unique_id\n",
    "    LEFT JOIN opg_sirius_prod.cases c   ON CAST(c.uid AS varchar) = icu.case_uid\n",
    "                                       AND c.glueexporteddate = (SELECT d FROM latest_case_export)\n",
    "    LEFT JOIN opg_sirius_prod.persons p ON p.id = c.donor_id\n",
    "                                       AND p.glueexporteddate = (SELECT d FROM latest_person_export)\n",
    "    LEFT JOIN opg_sirius_prod.addresses ad\n",
    "                                       ON ad.person_id = p.id\n",
    "                                       AND ad.glueexporteddate = (SELECT d FROM latest_addr_export)\n",
    "    ORDER BY\n",
    "      year DESC,                                           -- newest first\n",
    "      month DESC,\n",
    "      -- then by date received\n",
    "      i.date_received_in_opg DESC\n",
    "\"\"\"\n",
    "\n",
    "inv_linked_lpa_data = pydbtools.read_sql_query(inv_linked_lpa_query)\n",
    "print(inv_linked_lpa_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inv_linked_lpa_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save CSV inv_linked_lpa_data\n",
    "inv_linked_lpa_data.to_csv('inv_linked_lpa_data.csv', index=False)\n",
    "\n",
    "\n",
    "# === CONFIGURE DATA SOURCE ===\n",
    "df = pd.read_csv('inv_linked_lpa_data.csv', parse_dates=['registrationdate', 'date_received_in_opg'])\n",
    "# For demonstration, we'll assume 'df' is already loaded with columns:\n",
    "# ['registration_date', 'investigation_start_date', 'case_type',\n",
    "#  'referral_source', 'submission_channel'] plus any other investigation fields.\n",
    "\n",
    "print(df[['registrationdate', 'date_received_in_opg', 'case_type',\n",
    "  'referral_source', 'submission_channel']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Example placeholder DataFrame creation (remove when using real data)\n",
    "# dates = pd.date_range(start=\"2015-01-01\", end=\"2022-12-31\", freq=\"W\")\n",
    "# df = pd.DataFrame({\n",
    "#     'registration_date': np.random.choice(dates, size=1000),\n",
    "#     'investigation_start_date': np.random.choice(dates + pd.to_timedelta(np.random.randint(0, 365, size=len(dates)), unit='d'), size=1000),\n",
    "#     'case_type': np.random.choice(['Property and Finance', 'Health and Welfare'], size=1000, p=[0.6, 0.4]),\n",
    "#     'referral_source': np.random.choice(['External', 'Internal', 'Public'], size=1000),\n",
    "#     'submission_channel': np.random.choice(['DIY Online', 'With Lawyer'], size=1000)\n",
    "# })\n",
    "\n",
    "# === PREPARE METRICS ===\n",
    "df['lead_time_days'] = (df['investigation_start_date'] - df['registration_date']).dt.days\n",
    "df['reg_year'] = df['registration_date'].dt.year\n",
    "df['reg_month'] = df['registration_date'].dt.to_period('M')\n",
    "df['case_group'] = df['case_type'].map({\n",
    "    'Property and Finance': 'P&F',\n",
    "    'Health and Welfare': 'H&W'\n",
    "})\n",
    "\n",
    "# 1. Monthly counts by case group\n",
    "monthly_counts = (\n",
    "    df\n",
    "    .groupby(['reg_month', 'case_group'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# 2. Lead-time by reg year for P&F\n",
    "pf_lead = df[df['case_group'] == 'P&F']\n",
    "lead_time_by_year = [pf_lead[pf_lead['reg_year'] == year]['lead_time_days']\n",
    "                     for year in sorted(pf_lead['reg_year'].unique())]\n",
    "\n",
    "# 3. Referral source mix by year for P&F\n",
    "referral_mix = (\n",
    "    pf_lead\n",
    "    .groupby(['reg_year', 'referral_source'])\n",
    "    .size()\n",
    "    .groupby(level=0)\n",
    "    .apply(lambda x: x / x.sum())\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# 4. Submission channel trend\n",
    "channel_trend = (\n",
    "    df\n",
    "    .groupby(['reg_month', 'submission_channel'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# === PLOTTING ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_counts.plot(ax=plt.gca())\n",
    "for date in ['2016-05', '2018-04', '2020-03']:\n",
    "    plt.axvline(pd.Period(date, freq='M').to_timestamp(), linestyle='--', label=date)\n",
    "plt.title('Monthly Investigation Counts by Case Group')\n",
    "plt.xlabel('Registration Month')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(lead_time_by_year, labels=sorted(pf_lead['reg_year'].unique()))\n",
    "plt.title('Lead Time (days) from Registration to Investigation (P&F)')\n",
    "plt.xlabel('Registration Year')\n",
    "plt.ylabel('Lead Time (days)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "referral_mix.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "plt.title('Referral Source Proportion by Registration Year (P&F)')\n",
    "plt.xlabel('Registration Year')\n",
    "plt.ylabel('Proportion')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "channel_trend.plot(ax=plt.gca())\n",
    "plt.title('Submission Channel Trend Over Time')\n",
    "plt.xlabel('Registration Month')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === SEGMENTED REGRESSION ON P&F MONTHLY COUNTS ===\n",
    "pf_monthly = monthly_counts['P&F'].astype(float)\n",
    "time_index = pd.to_datetime(pf_monthly.index.to_timestamp())\n",
    "t = (time_index - time_index.min()).days\n",
    "intervention = pd.Timestamp('2018-04-01')\n",
    "X = pd.DataFrame({\n",
    "    'const': 1,\n",
    "    't': t,\n",
    "    'post_2018': (time_index >= intervention).astype(int),\n",
    "    't_after_2018': np.where(time_index >= intervention, t - (intervention - time_index.min()).days, 0)\n",
    "})\n",
    "model = sm.OLS(pf_monthly.values, X).fit()\n",
    "print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
