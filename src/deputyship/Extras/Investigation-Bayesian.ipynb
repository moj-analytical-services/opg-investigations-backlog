{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Understanding the step reduction in investigation for LPA cases post-pandemic (2022-24) compared to investigation for LPA cases in pre-pandemic (2016-19):\n",
    "\n",
    "## Dynamic Bayesian Network (DBN)\n",
    "\n",
    "If we have a dataset with many columns and want to understand the causes of step reduction in the investigation demands pre- and post-pandemic, which varies for different casesubtypes (\"HW\" / \"PFA\") in LPA demands, is the dynamic Bayesian network a useful method to capture the drivers (variables / columns) that affect aged population demands for the different concern type (\"Financial\" / \"Health and Welfare\" / both) of the investigation cases? note that \"HW\" and \"PFA\" in LPA casesubtypes in related to the concerntype of \"Health and Welfare\" and \"Financial\" in investigation, respectively. The linked LPA and Investigation data includes: LPA variables (uid\tdonor_id\tlpa_reg_date\tlpa_status\tlpa_rec_date\tpoa_type\tid_x\tparent_id\tdob\tsalutation\tfirstname\tmiddlenames\tsurname\ttype\tapplyingforfeeremission\treceivingbenefits\thaslowincome\tmaritalstatus\tclientstatus\trelationshiptoclient\tclient_id\tadd_id\ttown\tcountry\tcounty\tpostcode\tns_donor_id\trecdate\tregdate\tsigdate\tns_uid\tns_type\tapplicationtype\tdonor_gender\tdonor_type\tdonor_dob\tdonor_age\tdonor_age_band\tdonor_opg_rgn_name\tdonor_opg_ctry_name\tdonor_opg_laua_name\tdonor_nspl_country_name\tdonor_nspl_region_name\tdonor_nspl_lau_name\tdonor_nspl_rui_name\tdonor_nspl_oac\tdonor_nspl_imd\tattorney_count\timd_decile\tdonor_age_at_rec\tdonor_age_at_reg\tdonor_age_at_sig\td_age_rec_band\td_age_reg_band\td_age_sig_band\tid_y), Investigation Variables (unique_id\tcase_no\tinvestigator\tteam\treallocated_case\tweighting\tclient_donor_title\tclient_donor_forename\tclient_donor_surname\tclient_donor_dob\trisk\tcase_type\tconcern_type\tstatus\tsub_status\tdate_received_in_opg\tdate_received_in_investigations\tdate_allocated_to_team\tdate_allocated_to_current_investigator\tanticipated_completion_date\tpg_sign_off_date\tdays_on_hold\tcurrently_hold_from\tmultiple_id\tlead_case\tdays_to_pg_sign_off\tclosure_date\tpg_sign_off_hold_days\tpg_sign_off_to_close_days\tlast_status\treferals_made_by_itas\thigh_risk_from\tdays_at_high_risk\trecommended_court_outcome\tdate_of_legal_review_request_1\tdate_legal_rejects_1\treason_for_rejection_1\tlegal_risk_rejection_1\tdate_of_legal_review_request_2\tdate_legal_rejects_2\treason_for_rejection_2\tlegal_risk_rejection_2\tdate_of_legel_review_request_3\tlcr_request_no\ttimes_lawyers_allocated_1_reallocated_case\tlegal_approval_date\tlegal_risk\tdate_sent_to_ca\tca_acceptance_type\tlawyer\tallocated_to_solicitor_date\tlegal_team\tpg_signatory\tcourt_outcome\tdate_of_order\tpgs_addendum_date\tflagged_date\tflagged_type\tflag_lawyer\tday_40_review_date\tday_40_review_completion_date\tday_70_review_date\tday_70_review_completion_date\tday_100_review_date\tday_100_review_completion_date\tfurther_review_date), and the derived variables from these two dataset-LPA and Investigation- (unique_id\tcase_no\tinvestigator\tteam\treallocated_case\tweighting\tclient_donor_title\tclient_donor_forename\tclient_donor_surname\tclient_donor_dob\trisk\tcase_type\tconcern_type\tstatus\tsub_status\tdate_received_in_opg\tdate_received_in_investigations\tdate_allocated_to_team\tdate_allocated_to_current_investigator\tanticipated_completion_date\tpg_sign_off_date\tdays_on_hold\tcurrently_hold_from\tmultiple_id\tlead_case\tdays_to_pg_sign_off\tclosure_date\tpg_sign_off_hold_days\tpg_sign_off_to_close_days\tlast_status\treferals_made_by_itas\thigh_risk_from\tdays_at_high_risk\trecommended_court_outcome\tdate_of_legal_review_request_1\tdate_legal_rejects_1\treason_for_rejection_1\tlegal_risk_rejection_1\tdate_of_legal_review_request_2\tdate_legal_rejects_2\treason_for_rejection_2\tlegal_risk_rejection_2\tdate_of_legel_review_request_3\tlcr_request_no\ttimes_lawyers_allocated_1_reallocated_case\tlegal_approval_date\tlegal_risk\tdate_sent_to_ca\tca_acceptance_type\tlawyer\tallocated_to_solicitor_date\tlegal_team\tpg_signatory\tcourt_outcome\tdate_of_order\tpgs_addendum_date\tflagged_date\tflagged_type\tflag_lawyer\tday_40_review_date\tday_40_review_completion_date\tday_70_review_date\tday_70_review_completion_date\tday_100_review_date\tday_100_review_completion_date\tfurther_review_date).\n",
    "\n",
    "**Dynamic Bayesian Network (DBN)** can be a useful method for capturing the drivers of step reduction in investigation demands for different case subtypes (Health & Welfare (HW) and Property & Finance (PFA)) in an aged population.\n",
    "\n",
    "### Why DBN?\n",
    "\n",
    "#### Capturing Temporal Dependencies\n",
    "\n",
    "Investigation demand trends change over time (pre- and post-pandemic), and DBNs model such dependencies explicitly.\n",
    "\n",
    "If the reduction is due to shifts in operational policies, demographic changes, or other external factors, a DBN can model these temporal influences.\n",
    "\n",
    "#### Handling Multi-Variable Interactions\n",
    "\n",
    "A DBN can incorporate multiple columns (features) such as age distributions, case types, policy changes, and external factors to identify the most influential variables.\n",
    "\n",
    "It allows for capturing conditional dependencies between features, e.g., whether a change in triage policy has impacted HW vs. PFA cases differently.\n",
    "\n",
    "#### Differentiating Effects Across Case Subtypes\n",
    "\n",
    "HW and PFA cases may be influenced by different sets of factors.\n",
    "\n",
    "A DBN can structure dependencies separately for each case type and analyse whether certain age groups (e.g., older individuals) are more affected in HW vs. PFA cases.\n",
    "\n",
    "#### Causal Insights vs. Correlations\n",
    "\n",
    "Unlike standard time series models or correlation-based analysis, DBNs allow for causal interpretation by structuring the relationships between observed variables.\n",
    "\n",
    "If you suspect changes in demand are driven by a policy intervention (e.g., triage removal) or pandemic effects, a DBN can help quantify their impacts.\n",
    "\n",
    "#### Missing Data and Probabilistic Reasoning\n",
    "\n",
    "DBNs handle missing data well using probabilistic inference, which is useful when dealing with incomplete historical records.\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "#### Define Key Variables (Columns)\n",
    "\n",
    "Age groups, case subtype (HW/PFA), investigation demand, policy changes, external socioeconomic factors, etc.\n",
    "\n",
    "#### Structure the DBN\n",
    "\n",
    "Define temporal connections (pre/post-pandemic time steps).\n",
    "\n",
    "Model case subtype dependencies separately to identify diverging patterns.\n",
    "\n",
    "#### Inference & Validation\n",
    "\n",
    "Use methods like Expectation-Maximization (EM) or Bayesian Inference to estimate probabilities and validate findings against historical data.\n",
    "\n",
    "### Alternative Approaches\n",
    "\n",
    "If interpretability and robustness are priorities, you might also consider:\n",
    "\n",
    "- Causal Discovery (e.g., Granger Causality, DoWhy, Causal Graphs)\n",
    "- Structural Equation Modeling (SEM)\n",
    "- Generalized Additive Models (GAMs) for time series trends\n",
    "\n",
    "\n",
    "## structuring the DBN or selecting key features for the analysis?\n",
    "\n",
    "### 1. Define Key Variables (Columns)\n",
    "\n",
    "We need to identify the most relevant factors affecting investigation demand for Health & Welfare (HW) and Property & Finance (PFA) cases.\n",
    "\n",
    "#### Potential Features for the DBN:\n",
    "\n",
    "| Variable                  | Description                                          | Expected Impact                              |\n",
    "|---------------------------|------------------------------------------------------|----------------------------------------------|\n",
    "| time_period               | Pre/Post-pandemic indicator or continuous time variable | Tracks changes over time                     |\n",
    "| investigation_cases       | Number of investigation cases per time period        | Target variable                               |\n",
    "| case_subtype              | HW or PFA                                            | Category-specific trends                      |\n",
    "| age_group                 | Age brackets (e.g., 65-74, 75-84, 85+)               | Elderly populations might have different case distributions |\n",
    "| triage_policy             | Binary (1 = triage process was in place, 0 = removed) | Policy change effects                         |\n",
    "| LPA_applications          | Number of new LPA applications                       | More applications may lead to fewer investigations |\n",
    "| financial_vulnerability   | Economic indicator (e.g., median income, benefits claims) | Affects PFA cases                             |\n",
    "| health_risk_index         | Proxy for elder care needs                           | Affects HW cases                              |\n",
    "| public_awareness_campaigns| Whether informational campaigns were run (yes/no)    | May influence reporting trends                |\n",
    "\n",
    "### 2. Structure the DBN\n",
    "\n",
    "A DBN consists of nodes (variables) and directed edges (dependencies). The structure can be learned from data or predefined based on domain knowledge.\n",
    "\n",
    "A simplified DBN structure could look like:\n",
    "\n",
    "```\n",
    "(time t-1)  ----->  (time t)\n",
    "-----------------------------------------\n",
    " Investigation Cases (t-1) ---> Investigation Cases (t)\n",
    "       |                          |\n",
    "       v                          v\n",
    "  Case Subtype ---------------> Investigation Cases (t)\n",
    "       |                          |\n",
    "       v                          v\n",
    "   Age Group                   Age Group\n",
    "       |                          |\n",
    "       v                          v\n",
    "  Triage Policy ---------------> Investigation Cases (t)\n",
    "       |                          |\n",
    "       v                          v\n",
    "    LPA Apps                 Financial Vulnerability\n",
    "       |                          |\n",
    "       v                          v\n",
    "  Public Awareness  --------> Investigation Cases (t)\n",
    "```\n",
    "\n",
    "- **Investigation Cases (t-1) ‚Üí Investigation Cases (t)**: Captures temporal dependence.\n",
    "- **Triage Policy ‚Üí Investigation Cases (t)**: Tests if removing triage reduced investigations.\n",
    "- **Case Subtype ‚Üí Investigation Cases (t)**: Identifies whether HW or PFA cases drive the trend.\n",
    "- **LPA Applications ‚Üí Investigation Cases (t)**: Tests if increased LPA use lowers investigation demand.\n",
    "- **Financial Vulnerability ‚Üí Investigation Cases (t)**: Captures external economic effects.\n",
    "\n",
    "### 3. Learning & Inference\n",
    "\n",
    "There are two main ways to estimate the structure and parameters:\n",
    "\n",
    "#### Learn from Data\n",
    "\n",
    "- Use structure learning algorithms (e.g., Hill Climbing, Greedy Search, PC Algorithm) to determine dependencies.\n",
    "- Train the model using Expectation-Maximization (EM) to handle missing data.\n",
    "- Use Bayesian inference (e.g., Junction Tree Algorithm) to compute probabilities.\n",
    "\n",
    "#### Manual Expert-Driven Approach\n",
    "\n",
    "- Define key relationships based on business knowledge.\n",
    "- Manually set conditional probabilities using historical insights.\n",
    "\n",
    "### 4. Validating Insights\n",
    "\n",
    "- **Intervention Analysis**: Compare pre/post-pandemic case trends to confirm step reduction patterns.\n",
    "- **Counterfactual Scenarios**: Simulate policy changes (e.g., \"What if triage was never removed?\").\n",
    "- **Model Performance**: Cross-check DBN results with alternative models (e.g., Granger Causality, GAMs).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- A Python implementation using `pgmpy` or `bnlearn`?\n",
    "- A feature selection analysis to refine input variables?\n",
    "- A causal graph visualization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 1. Feature Selection using Mutual Information\n",
    "- Identify the most relevant features affecting investigation case volume.\n",
    "- Select features separately for Financial and Health & Welfare cases.\n",
    "\n",
    "**Performing mutual information analysis to select the most relevant features affecting investigation case volume.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performing mutual information analysis to select the most relevant features affecting investigation case volume. \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#!pip install --upgrade pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Upgrade numpy\n",
    "#!pip install --upgrade numpy\n",
    "# !pip uninstall numpy\n",
    "# !pip install numpy==1.23\n",
    "\n",
    "# Upgrade scikit-learn\n",
    "!pip install --upgrade scikit-learn\n",
    "#import sklearn\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"dummy.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "#It seems like there is a mismatch in dimensions while imputing missing values. \n",
    "#inspect the number of missing values per column and ensure proper handling before proceeding with mutual information computation.\n",
    "# check for missing values in your dataset using:\n",
    "# Checking missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "#This will help identify columns with missing values so they can be handled appropriately.\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "## Analyse mutual information between features and the target variables (concern type and case subtype).\n",
    "\n",
    "#The dataset contains 39 columns with a mix of categorical and numerical features. \n",
    "#There are some missing values, particularly in `parent_id`, `reallocated_case`, `risk`, `sub_status`, `closure_date`, and `high_risk_from`.  \n",
    "\n",
    "# 1. Encoding categorical variables.  \n",
    "# 2. Handling missing values.  \n",
    "# 3. Calculating mutual information scores for `concern_type` and `casesubtype` to identify the most relevant features.\n",
    "\n",
    "\n",
    "# Proceed with constructing the Dynamic Bayesian Network (DBN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Selecting target variables\n",
    "target_columns = [\"concern_type\", \"casesubtype\"]\n",
    "\n",
    "# Dropping rows with missing target values\n",
    "df = df.dropna(subset=target_columns)\n",
    "\n",
    "# Identifying categorical columns\n",
    "categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Encoding categorical features using Label Encoding\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))  # Convert to string before encoding\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Handling missing values by imputing with the median for numerical columns\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "df[df.select_dtypes(include=[\"number\"]).columns] = imputer.fit_transform(df.select_dtypes(include=[\"number\"]))\n",
    "#It looks like there's a mismatch in the number of columns when imputing missing values. \n",
    "#adjust the approach to ensure the correct transformation is applied.\n",
    "\n",
    "\n",
    "# Computing mutual information scores for both target variables\n",
    "X = df.drop(columns=target_columns)\n",
    "y_concern = df[\"concern_type\"]\n",
    "y_casesubtype = df[\"casesubtype\"]\n",
    "\n",
    "mi_concern = mutual_info_classif(X, y_concern, discrete_features='auto')\n",
    "mi_casesubtype = mutual_info_classif(X, y_casesubtype, discrete_features='auto')\n",
    "\n",
    "# Creating a DataFrame for feature importance\n",
    "mi_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"MI_Concern_Type\": mi_concern,\n",
    "    \"MI_Case_Subtype\": mi_casesubtype\n",
    "}).sort_values(by=[\"MI_Concern_Type\", \"MI_Case_Subtype\"], ascending=False)\n",
    "\n",
    "# Displaying the top 10 features based on mutual information\n",
    "mi_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Interpretation of Feature Selection Results\n",
    "The Mutual Information (MI) scores indicate how much information each feature contributes in predicting concern type and case subtype.\n",
    "\n",
    "Key Observations:\n",
    "\"poas_involved\" (MI = 0.716)\n",
    "\n",
    "The most significant driver for concern type.\n",
    "\n",
    "Indicates whether multiple power of attorneys (POAs) are involved in a case.\n",
    "\n",
    "High MI suggests that cases with multiple POAs are more likely to be investigated due to increased complexity or potential disputes.\n",
    "\n",
    "\"imd_decile\" (MI = 0.268)\n",
    "\n",
    "The Index of Multiple Deprivation (IMD) score shows socio-economic status.\n",
    "\n",
    "A higher MI suggests a relationship between socio-economic deprivation and investigation concerns, likely due to financial vulnerability or suspected abuse.\n",
    "\n",
    "\"attorney_count\" (MI = 0.234)\n",
    "\n",
    "Number of attorneys associated with an LPA.\n",
    "\n",
    "More attorneys may lead to complex dynamics, increasing the likelihood of investigations.\n",
    "\n",
    "\"donor_gender\" (MI = 0.125)\n",
    "\n",
    "Gender of the LPA donor.\n",
    "\n",
    "Gender-related patterns might influence concern types, possibly due to historical financial control patterns.\n",
    "\n",
    "\"poa_case_type\" & \"applicationtype\" (MI = 0.118 & 0.101)\n",
    "\n",
    "Different types of power of attorney cases and their application methods impact investigations.\n",
    "\n",
    "Some case types may have a higher likelihood of financial concerns or health-related issues.\n",
    "\n",
    "\"donor_opg_ctry_name\" & \"client_donor_dob\" (MI = 0.086 & 0.065)\n",
    "\n",
    "Geographic distribution and donor age might influence investigation trends.\n",
    "\n",
    "Older donors or specific regions may correlate with higher investigation rates.\n",
    "\n",
    "\"risk\" (MI = 0.033)\n",
    "\n",
    "Assigned risk level of the case.\n",
    "\n",
    "Lower MI suggests risk labels alone do not strongly determine concern type but are still relevant.\n",
    "\n",
    "Next Step: Implementing a Dynamic Bayesian Network (DBN)\n",
    "We will:\n",
    "‚úÖ Build a DBN using pgmpy or bnlearn to model temporal relationships.\n",
    "‚úÖ Capture dependencies between selected features and investigation trends over time.\n",
    "‚úÖ Analyze investigation trends before and after the pandemic, focusing on financial and health-related concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# 2. Dynamic Bayesian Network (DBN) Implementation\n",
    "- Build a DBN using pgmpy or bnlearn to analyze investigation trends.\n",
    "- Capture temporal dependencies and key drivers.\n",
    "\n",
    "## One DBN for Pre-pandemic and another DBN for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df2 = df[['donor_gender', 'donor_age', 'poas_involved', 'imd_decile', 'attorney_count', \n",
    "          'poa_case_type', 'concern_type', 'applicationtype', 'client_donor_dob', 'risk']]\n",
    "df1 = df2\n",
    "\n",
    "# DBNs require categorical/discrete data. If features like imd_decile or poas_involved are continuous, discretize them:\n",
    "# df1['imd_decile'] = pd.cut(df1['imd_decile'], bins=5, labels=[1, 2, 3, 4, 5])\n",
    "df1['donor_gender'] = df1['donor_gender'].astype('category')\n",
    "df1['donor_age'] = df1['donor_age'].astype('category')\n",
    "df1['poas_involved'] = df1['poas_involved'].astype('category')\n",
    "df1['imd_decile'] = df1['imd_decile'].astype('category')\n",
    "df1['attorney_count'] = df1['attorney_count'].astype('category')\n",
    "df1['poa_case_type'] = df1['poa_case_type'].astype('category')\n",
    "df1['concern_type'] = df1['concern_type'].astype('category')\n",
    "df1['applicationtype'] = df1['applicationtype'].astype('category')\n",
    "df1['client_donor_dob'] = df1['client_donor_dob'].astype('category')\n",
    "df1['risk'] = df1['risk'].astype('category')\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df2 = df[['donor_gender', 'donor_age', 'poas_involved', 'imd_decile', 'attorney_count', \n",
    "          'poa_case_type', 'concern_type', 'applicationtype', 'client_donor_dob', 'risk']]\n",
    "df1 = df2\n",
    "\n",
    "# DBNs require categorical/discrete data. If features like imd_decile or poas_involved are continuous, discretize them:\n",
    "# df1['imd_decile'] = pd.cut(df1['imd_decile'], bins=5, labels=[1, 2, 3, 4, 5])\n",
    "# df1['donor_age'] = pd.cut(df1['donor_age'], bins=5, labels=[1, 2, 3, 4, 5])\n",
    "# df1['client_donor_dob'] = pd.cut(df1['client_donor_dob'], bins=5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Define age bins (adjust as needed)\n",
    "age_bins = [0, 60, 70, 80, 90, 100, np.inf]\n",
    "age_labels = [1, 2, 3, 4, 5, 6]\n",
    "df1['donor_age'] = pd.cut(df1['donor_age'], bins=age_bins, labels=age_labels).astype(int)\n",
    "\n",
    "\n",
    "print(\"Checking for missing values:\")\n",
    "print(df1.isnull().sum())  # Should all be 0\n",
    "df1 = df1.dropna()  # Remove missing values if needed\n",
    "\n",
    "# DBNs require time slices (t=0, t=1) in the column names. Let's assume your data represents different snapshots over time:\n",
    "df1.columns = pd.MultiIndex.from_product([df1.columns, [0]])  # Assign time slice 0\n",
    "# For t=1 data (if available), append new data and assign t=1:\n",
    "df1_t1 = df1.copy()\n",
    "df1_t1.columns = pd.MultiIndex.from_product([df1_t1.columns.get_level_values(0), [1]])  # Assign time slice 1\n",
    "df1_combined = pd.concat([df1, df1_t1], axis=1)\n",
    "\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "from pgmpy.inference import DBNInference\n",
    "\n",
    "# Define DBN structure\n",
    "dbn = DBN()\n",
    "dbn.add_edges_from([\n",
    "    (('poas_involved', 0), ('poas_involved', 1)),  # Temporal dependency\n",
    "    (('imd_decile', 0), ('imd_decile', 1)),\n",
    "    (('donor_age', 0), ('donor_age', 1)),\n",
    "    (('concern_type', 0), ('concern_type', 1))  # Concern type over time\n",
    "])\n",
    "\n",
    "# Fit DBN with Maximum Likelihood Estimation\n",
    "dbn.fit(df1_combined, estimator=\"MLE\")\n",
    "\n",
    "# Verify if CPDs were learned correctly\n",
    "cpds = dbn.get_cpds()\n",
    "if not cpds:\n",
    "    print(\"‚ö†Ô∏è No CPDs were learned!\")\n",
    "else:\n",
    "    print(f\"‚úÖ {len(cpds)} CPDs learned successfully!\")\n",
    "    for cpd in cpds:\n",
    "        print(cpd)\n",
    "        \n",
    "print(\"DBN Nodes:\", dbn.nodes())\n",
    "print(\"Data Columns:\", df1_combined.columns.tolist())\n",
    "\n",
    "\n",
    "df1.columns = pd.MultiIndex.from_tuples([(col, 0) for col in df1.columns])\n",
    "df1_t1 = df1.copy()\n",
    "df1_t1.columns = pd.MultiIndex.from_tuples([(col, 1) for col in df1_t1.columns])\n",
    "df1_combined = pd.concat([df1, df1_t1], axis=1)\n",
    "\n",
    "for cpd in dbn.get_cpds():\n",
    "    print(\"CPD variable:\", cpd.variable)\n",
    "    \n",
    "    \n",
    "# Step 3: Perform Inference\n",
    "# If CPDs exist, perform inference:\n",
    "dbn_infer = DBNInference(dbn)\n",
    "\n",
    "query_result = dbn_infer.query(variables=[('concern_type', 1)], \n",
    "                               evidence={('poas_involved', 0): 1, ('imd_decile', 0): 3})\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pgmpy\n",
    "#!pip install --upgrade typing_extensions\n",
    "#!pip install --upgrade BicScore\n",
    "#print(dir(pgmpy.estimators))\n",
    "from pgmpy.models import DynamicBayesianNetwork as DBN\n",
    "#from pgmpy.estimators import StructureScore, BicScore\n",
    "from pgmpy.inference import DBNInference\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1['donor_gender'] = df1['donor_gender'].astype('category')\n",
    "df1['donor_age'] = df1['donor_age'].astype('category')\n",
    "df1['poas_involved'] = df1['poas_involved'].astype('category')\n",
    "df1['imd_decile'] = df1['imd_decile'].astype('category')\n",
    "df1['attorney_count'] = df1['attorney_count'].astype('category')\n",
    "df1['poa_case_type'] = df1['poa_case_type'].astype('category')\n",
    "df1['concern_type'] = df1['concern_type'].astype('category')\n",
    "df1['applicationtype'] = df1['applicationtype'].astype('category')\n",
    "df1['client_donor_dob'] = df1['client_donor_dob'].astype('category')\n",
    "df1['risk'] = df1['risk'].astype('category')\n",
    "\n",
    "# Define a Dynamic Bayesian Network\n",
    "dbn = DBN()\n",
    "\n",
    "# Adding temporal relationships based on high MI features\n",
    "dbn.add_edges_from([\n",
    "    (('poas_involved', 0), ('poas_involved', 1)),  # Temporal dependency\n",
    "    (('imd_decile', 0), ('imd_decile', 1)),\n",
    "    (('attorney_count', 0), ('attorney_count', 1)),\n",
    "    (('poa_case_type', 0), ('concern_type', 0)),\n",
    "    (('applicationtype', 0), ('concern_type', 0)),\n",
    "    (('client_donor_dob', 0), ('concern_type', 0)),\n",
    "    (('risk', 0), ('concern_type', 0)),\n",
    "    (('concern_type', 0), ('concern_type', 1))  # Concern type over time\n",
    "])\n",
    "\n",
    "# Step 1: Transform Column Names to Include Time Slices\n",
    "# Modify the dataset so that each variable has a corresponding time slice:\n",
    "# Convert column names to tuples (feature, time_slice)\n",
    "time_slice = 0  # Assuming you are working with the first time slice\n",
    "df1.columns = [(col, time_slice) for col in df1.columns]\n",
    "\n",
    "# Ensure it‚Äôs a MultiIndex DataFrame\n",
    "df1.columns = pd.MultiIndex.from_tuples(df1.columns)\n",
    "\n",
    "# Step 2: Fit the DBN Model Again\n",
    "# Learning CPDs using Maximum Likelihood Estimation\n",
    "#dbn.fit(df, estimator=MaximumLikelihoodEstimator)\n",
    "# Fit the model using Maximum Likelihood Estimation\n",
    "dbn.fit(df1, estimator=\"MLE\")\n",
    "\n",
    "# Explicitly retrieve and add the learned CPDs to the model\n",
    "for cpd in dbn.get_cpds():\n",
    "    dbn.add_cpds(cpd)  # Ensure they are assigned correctly\n",
    "    print(cpd)\n",
    "    \n",
    "# Check if all CPDs are correctly added\n",
    "assert dbn.check_model(), \"The DBN model is not valid!\"\n",
    "\n",
    "# If CPDs are missing, it means the data might be improperly formatted.\n",
    "# Ensure your dataset has discrete categorical values (DBNs don‚Äôt work well with continuous data unless discretised).   \n",
    "# If df.columns does not look like ('poas_involved', 0), then apply:\n",
    "\n",
    "#df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "#print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Performing inference\n",
    "dbn_infer = DBNInference(dbn)\n",
    "query_result = dbn_infer.query(variables=['concern_type'], evidence={'poas_involved': 1, 'imd_decile': 5})\n",
    "\n",
    "query_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# 3. Causal Graph Visualisation\n",
    "- Generate a directed causal graph showing relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pydbtools\n",
    "# !pip install awswrangler\n",
    "# !pip install pandas --upgrade\n",
    "# !pip install time\n",
    "# !pip install numpy --upgrade\n",
    "# !pip install jinja2\n",
    "# !pip install lovely_logger \n",
    "\n",
    "# import pydbtools\n",
    "# import awswrangler as wr\n",
    "# import model_stages.stage1_get_invest_data as stage1\n",
    "# import model_stages.stage2_prepare_invests_for_merging as stage2\n",
    "# import model_stages.stage3_merge_with_POA_data as stage3\n",
    "\n",
    "\n",
    "# ## Config\n",
    "\n",
    "# date_cols = ['date_received_in_investigations', 'date_allocated_to_team',\n",
    "#                   'date_allocated_to_current_investigator', 'pg_sign_off_date',\n",
    "#                   'closure_date',\n",
    "#                   'date_received_in_opg', 'legal_approval_date_clean',\n",
    "#                  'receiptdate', 'registrationdate']\n",
    "\n",
    "# cols_to_keep = ['unique_id', \n",
    "#                 # 'client_donor_title',\n",
    "#                 # 'client_donor_forename', 'client_donor_surname', 'client_donor_dob',\n",
    "#                 'case_type', 'concern_type', 'status', 'sub_status',\n",
    "#                 'date_received_in_opg', 'multiple_id', 'lead_case',\n",
    "#                 'days_to_pg_sign_off', 'closure_date', 'case_uid', 'receiptdate', 'registrationdate',\n",
    "#                 'case_status', 'poa_case_type', 'casesubtype', 'poas_involved',\n",
    "#                 'order_involved', 'legal_approval_date_clean', 'invest_concluded',\n",
    "#                 'year_concluded', 'case_type_agg']\n",
    "# first_date = '01/01/2019'\n",
    "# last_date = '31/12/2024'\n",
    "\n",
    "# min_LPA_receiptdate = '2017-01-01'\n",
    "# max_LPA_receiptdate = '2024-12-31'\n",
    "# sample_size = '1000000'\n",
    "\n",
    "\n",
    "# ## Read/Link Data\n",
    "\n",
    "# # Read Investigations Data\n",
    "# df_inv_data = stage1.main(date_cols, first_date, last_date, cols_to_keep)\n",
    "\n",
    "# # Expand Investigations Data\n",
    "# expanded_df = stage2.main(df_inv_data)\n",
    "\n",
    "# # Link to LPA Data\n",
    "# linked_df = stage3.main(expanded_df, \n",
    "#                         min_LPA_receiptdate, \n",
    "#                         max_LPA_receiptdate, \n",
    "#                         sample_size)\n",
    "\n",
    "# # print(linked_df)\n",
    "\n",
    "# linked_Investigation_LPA_data = linked_df\n",
    "# linked_Investigation_LPA_data.to_csv('linked_Investigation_LPA_data.csv', index=False)\n",
    "\n",
    "# linked_df\n",
    "\n",
    "# inv = linked_df.loc[linked_df['investigator'].notnull()]\n",
    "# inv.to_csv('linked_Investigation_LPA_inv.csv', index=False)\n",
    "\n",
    "# inv_linked_lpa = linked_df[['uid','donor_id','lpa_reg_date','lpa_status','lpa_rec_date','poa_type','unique_id','case_no','client_donor_dob','case_type','concern_type','date_received_in_opg','status','mojap_extract_date','poa_case_type','casesubtype','poa_rec_to_invest_rec','year_concluded','link_id','uid_to_link']]\n",
    "# inv_linked_lpa\n",
    "# inv_linked_lpa_data = inv_linked_lpa\n",
    "# inv_linked_lpa_data.to_csv('inv_linked_lpa_data.csv', index=False)\n",
    "\n",
    "# pydbtools.read_sql_query(\"SELECT DISTINCT(mojap_extract_date) FROM opg_investigations_prod.investigations ORDER BY mojap_extract_date DESC\")\n",
    "# lpa_dashboard = pydbtools.read_sql_query(\"SELECT * FROM sirius_derived.opg_lpa_dashboard LIMIT 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "How to explain the drivers and factors that caused a step reduction in the investigation cases for Lasting Power of Attorney (LPA) demands from Office of Public Guardian (OPG)? To achieve this, basically we linked Investigation and LPA dataset which resulted a data frame in Python with the following variables (columns): linked_df[['uid','donor_id','lpa_reg_date','lpa_status','lpa_rec_date','poa_type','unique_id','case_no','client_donor_dob','case_type','concern_type','date_received_in_opg','status','mojap_extract_date','poa_case_type','casesubtype','poa_rec_to_invest_rec','year_concluded','link_id','uid_to_link']].   Regarding these, can you explain what machine learning and statistical techniques is useful and how to implement them to show 1. How to investigate the age distribution (based on the dob of donor ('client_donor_dob') at the investigation ('date_received_in_opg') changes before and after pandemic that might be the reason for the step reduction in investigation of LPA cases? 2. How to show that different 'casesubtype' and 'case_type' changes influenced this step reduction in the investigation of LPA cases? 3. investigate whether: ‚Ä¢\tIt suggests that the downward trend in the investigations rate for Health and Welfare cases or where investigations have included both Health and Welfare AND Property and Finance concerns have been gradual since 2016 rather than a step reduction associated with the pandemic. Having said that the rate of investigations particularly for Health and Welfare cases levelled off after the pandemic.\n",
    "‚Ä¢\tThere isn‚Äôt any evidence of a gradual decline in Finance and Property cases, but instead the pattern that we have discussed before of a sustained stepped reduction following the pandemic.\n",
    "‚Ä¢\tThe gradual decline in Health and Welfare and combined concerns from 2016 is interesting because it also coincides with what I believe was an operational decision at that time to remove the triage process for LPA investigations. This had the immediate effect of increasing the number of concerns accepted for investigation, which can be seen in the attached charts, followed by a gradual decline.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install --upgrade pandas\n",
    "import pandas as pd\n",
    "!pip install --upgrade numpy\n",
    "import numpy as np\n",
    "# print(np.__version__)\n",
    "# print(np.__path__)\n",
    "linked_df = pd.read_csv('inv_linked_lpa_data.csv')\n",
    "linked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "1. Investigating Age Distribution Changes Pre- and Post-Pandemic\n",
    "To check whether changes in donor age at the time of investigation contributed to the reduction:\n",
    "\n",
    "Techniques:\n",
    "Descriptive Statistics & Visualization: Calculate mean, median, and IQR of donor age pre- and post-pandemic.\n",
    "\n",
    "Kernel Density Estimation (KDE) & Histograms: Compare the age distributions before and after the pandemic.\n",
    "\n",
    "Kolmogorov-Smirnov (KS) Test: Check if the distribution of ages significantly changed.\n",
    "\n",
    "Causal Inference (Difference-in-Differences - DiD): Compare the mean age before and after the pandemic with a control period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This updated code includes:\n",
    "# Kernel Density Estimation (KDE) plots to show the age distribution of donors at investigation before and after the pandemic.\n",
    "# Bar charts to visualize the number of investigations by age group before and after the pandemic.\n",
    "# These visualizations should provide a clearer picture of how the investigation demand and age distribution have changed due to the pandemic. \n",
    "\n",
    "!pip install --upgrade scipy matplotlib\n",
    "#!pip install matplotlib\n",
    "# !pip uninstall seaborn\n",
    "# !pip install seaborn\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Convert to datetime\n",
    "linked_df['client_donor_dob'] = pd.to_datetime(linked_df['client_donor_dob'], errors='coerce', dayfirst=True)\n",
    "linked_df['date_received_in_opg'] = pd.to_datetime(linked_df['date_received_in_opg'])\n",
    "\n",
    "# Calculate donor age at investigation\n",
    "linked_df['donor_age_at_investigation'] = (linked_df['date_received_in_opg'] - linked_df['client_donor_dob']).dt.days / 365.25\n",
    "\n",
    "# Define age groups\n",
    "bins = [0, 18, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0-18', '19-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "linked_df['age_group'] = pd.cut(linked_df['donor_age_at_investigation'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Split pre- and post-pandemic (assuming March 2020 as pandemic start)\n",
    "pre_pandemic = linked_df[linked_df['date_received_in_opg'] < '2020-03-01']\n",
    "post_pandemic = linked_df[linked_df['date_received_in_opg'] >= '2020-03-01']\n",
    "\n",
    "# Plot KDE for age distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(pre_pandemic['donor_age_at_investigation'], label='Pre-Pandemic', shade=True)\n",
    "sns.kdeplot(post_pandemic['donor_age_at_investigation'], label='Post-Pandemic', shade=True)\n",
    "plt.legend()\n",
    "plt.title('Age Distribution of Donors at Investigation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Plot bar chart for investigation demand by age group\n",
    "plt.figure(figsize=(12, 6))\n",
    "pre_counts = pre_pandemic['age_group'].value_counts().sort_index()\n",
    "post_counts = post_pandemic['age_group'].value_counts().sort_index()\n",
    "bar_width = 0.35\n",
    "index = range(len(labels))\n",
    "\n",
    "plt.bar(index, pre_counts, bar_width, label='Pre-Pandemic')\n",
    "plt.bar([i + bar_width for i in index], post_counts, bar_width, label='Post-Pandemic')\n",
    "\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.title('Investigation Demand by Age Group Before and After Pandemic')\n",
    "plt.xticks([i + bar_width / 2 for i in index], labels)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform KS test\n",
    "ks_stat, p_value = ks_2samp(pre_pandemic['donor_age_at_investigation'], post_pandemic['donor_age_at_investigation'])\n",
    "print(f\"KS Statistic: {ks_stat}, P-Value: {p_value}\")\n",
    "\n",
    "# If the KS test p-value is low, the distributions are significantly different.\n",
    "# If the post-pandemic mean is significantly lower, older donors may have been investigated less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Overall proportion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Convert to datetime\n",
    "linked_df['client_donor_dob'] = pd.to_datetime(linked_df['client_donor_dob'], errors='coerce', dayfirst=True)\n",
    "linked_df['date_received_in_opg'] = pd.to_datetime(linked_df['date_received_in_opg'])\n",
    "\n",
    "# Calculate donor age at investigation\n",
    "linked_df['donor_age_at_investigation'] = (linked_df['date_received_in_opg'] - linked_df['client_donor_dob']).dt.days / 365.25\n",
    "\n",
    "# Define age groups\n",
    "bins = [0, 18, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0-18', '19-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "linked_df['age_group'] = pd.cut(linked_df['donor_age_at_investigation'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Split pre- and post-pandemic (assuming March 2020 as pandemic start)\n",
    "pre_pandemic = linked_df[linked_df['date_received_in_opg'] < '2020-03-01']\n",
    "post_pandemic = linked_df[linked_df['date_received_in_opg'] >= '2020-03-01']\n",
    "\n",
    "# Calculate overall proportion of investigations in each age group\n",
    "age_group_counts = linked_df['age_group'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Calculate proportion of investigations in each age group for pre- and post-pandemic periods\n",
    "pre_proportion = pre_pandemic['age_group'].value_counts(normalize=True).sort_index()\n",
    "post_proportion = post_pandemic['age_group'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Plot KDE for age distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(pre_pandemic['donor_age_at_investigation'], label='Pre-Pandemic', shade=True)\n",
    "sns.kdeplot(post_pandemic['donor_age_at_investigation'], label='Post-Pandemic', shade=True)\n",
    "plt.legend()\n",
    "plt.title('Age Distribution of Donors at Investigation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for absolute number of investigations by age group\n",
    "plt.figure(figsize=(12, 6))\n",
    "pre_counts = pre_pandemic['age_group'].value_counts().sort_index()\n",
    "post_counts = post_pandemic['age_group'].value_counts().sort_index()\n",
    "bar_width = 0.35\n",
    "index = range(len(labels))\n",
    "\n",
    "plt.bar(index, pre_counts, bar_width, label='Pre-Pandemic')\n",
    "plt.bar([i + bar_width for i in index], post_counts, bar_width, label='Post-Pandemic')\n",
    "\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.title('Investigation Demand by Age Group Before and After Pandemic')\n",
    "plt.xticks([i + bar_width / 2 for i in index], labels)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for overall proportion of investigation demands by age group\n",
    "plt.figure(figsize=(10, 5))\n",
    "age_group_counts.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Proportion of Investigations')\n",
    "plt.title('Overall Proportion of Investigation Demands by Age Group')\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side bar chart for proportion of investigations pre- vs post-pandemic\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(labels))\n",
    "\n",
    "plt.bar(index, pre_proportion, bar_width, label='Pre-Pandemic', color='blue', alpha=0.6)\n",
    "plt.bar([i + bar_width for i in index], post_proportion, bar_width, label='Post-Pandemic', color='red', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Proportion of Investigations')\n",
    "plt.title('Proportion of Investigations by Age Group: Pre vs Post Pandemic')\n",
    "plt.xticks([i + bar_width / 2 for i in index], labels)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform KS test\n",
    "ks_stat, p_value = ks_2samp(pre_pandemic['donor_age_at_investigation'], post_pandemic['donor_age_at_investigation'])\n",
    "print(f\"KS Statistic: {ks_stat}, P-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "1. Investigating Age Distribution Changes Pre- and Post-Pandemic\n",
    "Refinement: Bayesian Analysis & Causal Inference\n",
    "Rather than just comparing distributions, we can:\n",
    "\n",
    "Use a Bayesian model to estimate how much the mean donor age changed.\n",
    "\n",
    "Apply Causal Impact Analysis (Google‚Äôs CausalImpact package) to check if the change was due to the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Suppress warnings from statsmodels\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# !pip install --upgrade pymc3\n",
    "# #!python -m pip install --upgrade pip\n",
    "# #!pip uninstall pymc3\n",
    "# #!pip install git+https://github.com/pymc-devs/pymc3\n",
    "# # !git clone https://github.com/pymc-devs/pymc3\n",
    "# # !cd pymc3\n",
    "# # !pip install -r requirements.txt\n",
    "# # !python setup.py install\n",
    "# # !python setup.py develop\n",
    "# # !python -m pip install --upgrade pip\n",
    "# #!pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose\n",
    "# import pymc3 as pm\n",
    "# import arviz as az\n",
    "# from scipy.stats import kstest\n",
    "# print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # A. Bayesian Estimation of Age Differences\n",
    "# #!pip install pymc3\n",
    "\n",
    "# # Define Bayesian Model\n",
    "# with pm.Model():\n",
    "#     mu_pre = pm.Normal(\"mu_pre\", mu=70, sigma=10)  # Prior for Pre-Pandemic Age Mean\n",
    "#     mu_post = pm.Normal(\"mu_post\", mu=70, sigma=10)  # Prior for Post-Pandemic Age Mean\n",
    "#     sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "\n",
    "#     # Likelihood\n",
    "#     age_pre = pm.Normal(\"age_pre\", mu=mu_pre, sigma=sigma, observed=pre_pandemic['donor_age_at_investigation'])\n",
    "#     age_post = pm.Normal(\"age_post\", mu=mu_post, sigma=sigma, observed=post_pandemic['donor_age_at_investigation'])\n",
    "\n",
    "#     # Difference\n",
    "#     diff = pm.Deterministic(\"diff\", mu_post - mu_pre)\n",
    "\n",
    "#     trace = pm.sample(2000, return_inferencedata=True)\n",
    "\n",
    "# # Plot Posterior Distribution of Age Difference\n",
    "# import arviz as az\n",
    "# az.plot_posterior(trace, var_names=[\"diff\"])\n",
    "\n",
    "# # This approach gives a probability distribution of the change in age, rather than a simple p-value.\n",
    "\n",
    "# # If most of the posterior distribution is negative, it means donor age at investigation decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install causalimpact\n",
    "# from causalimpact import CausalImpact\n",
    "# import pandas as pd\n",
    "\n",
    "# # Ensure proper datetime conversion\n",
    "# linked_df['year_received'] = pd.to_datetime(linked_df['date_received_in_opg']).dt.year\n",
    "\n",
    "# # Compute mean donor age per year\n",
    "# ts_data = linked_df.groupby('year_received')['donor_age_at_investigation'].mean()\n",
    "\n",
    "# # Ensure DataFrame format\n",
    "# ts_data = ts_data.to_frame(name=\"value\")\n",
    "\n",
    "# # Convert index to datetime format\n",
    "# ts_data.index = pd.to_datetime(ts_data.index, format='%Y')\n",
    "\n",
    "# # Drop missing values\n",
    "# ts_data.dropna(inplace=True)\n",
    "\n",
    "# # Define pre/post intervention periods based on actual years in index\n",
    "# pre_period = [ts_data.index.min().year, 2019]\n",
    "# post_period = [2020, ts_data.index.max().year]\n",
    "\n",
    "# print(pre_period)\n",
    "# print(post_period)\n",
    "# print(len(ts_data))\n",
    "# # Ensure there are enough data points\n",
    "# if len(ts_data) < 12:\n",
    "#     print(\"üö® Not enough data points for CausalImpact. Try alternative methods.\")\n",
    "# else:\n",
    "#     # Run CausalImpact\n",
    "#     impact = CausalImpact(data=ts_data, pre_period=pre_period, post_period=post_period)\n",
    "\n",
    "#     # Check if the model ran successfully\n",
    "#     if impact.inferences is not None:\n",
    "#         impact.plot()\n",
    "#         print(impact.summary())\n",
    "#         print(impact.summary(output='report'))\n",
    "#     else:\n",
    "#         print(\"üö® CausalImpact failed. Check data formatting.\")\n",
    "\n",
    "# # If you only have 6 data points, CausalImpact won‚Äôt work reliably because:\n",
    "\n",
    "# # Bayesian structural time series (BSTS) models need at least 12+ observations.\n",
    "\n",
    "# # With just 6 years, there's not enough variance to estimate a meaningful impact.\n",
    "\n",
    "# from causalimpact import CausalImpact\n",
    "# import pandas as pd\n",
    "\n",
    "# # Ensure proper datetime conversion\n",
    "# linked_df['date_received'] = pd.to_datetime(linked_df['date_received_in_opg'])#.dt.strftime('%Y-%m').astype(str)\n",
    "\n",
    "# # Compute mean donor age per month\n",
    "# ts_data = linked_df.groupby(linked_df['date_received'].dt.to_period('M'))['donor_age_at_investigation'].mean()\n",
    "# # ts_data = linked_df.groupby(linked_df['date_received'])['donor_age_at_investigation'].mean()\n",
    "\n",
    "# # Ensure DataFrame format\n",
    "# ts_data = ts_data.to_frame(name=\"value\")\n",
    "\n",
    "# # Convert index to datetime format\n",
    "# ts_data.index = ts_data.index.to_timestamp()\n",
    "\n",
    "# # Drop missing values\n",
    "# ts_data.dropna(inplace=True)\n",
    "\n",
    "# #linked_df['date_received'].loc[1:1][1]\n",
    "\n",
    "# # Define pre/post intervention periods based on actual months in index\n",
    "# # pre_period = [ts_data.index.min().strftime('%Y-%m'), '2019-12']\n",
    "# # post_period = ['2020-01', ts_data.index.max().strftime('%Y-%m')]\n",
    "# # pre_period = [ts_data.index.min(), '2019-12']\n",
    "# # post_period = ['2020-01', ts_data.index.max()]\n",
    "# pre_period = [ts_data.index.min(), pd.Timestamp('2019-12-01')]\n",
    "# post_period = [pd.Timestamp('2020-01-01'), ts_data.index.max()]\n",
    "\n",
    "# print(pre_period)\n",
    "# print(post_period)\n",
    "# print(len(ts_data))\n",
    "# print(ts_data.head(10))\n",
    "# data = ts_data.reset_index()\n",
    "\n",
    "# # Ensure there are enough data points\n",
    "# if len(ts_data) < 12:\n",
    "#     print(\"üö® Not enough data points for CausalImpact. Try alternative methods.\")\n",
    "# else:\n",
    "#     # Run CausalImpact\n",
    "#     impact = CausalImpact(data=data, pre_period=pre_period, post_period=post_period)\n",
    "\n",
    "#     # Check if the model ran successfully\n",
    "#     if impact.inferences is not None:\n",
    "#         impact.plot()\n",
    "#         print(impact.summary())\n",
    "#         print(impact.summary(output='report'))\n",
    "#     else:\n",
    "#         print(\"üö® CausalImpact failed. Check data formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# First, uninstall the current versions\n",
    "!pip uninstall numpy statsmodels -y\n",
    "\n",
    "# Then, install compatible versions\n",
    "!pip install numpy==1.26.4 statsmodels\n",
    "#!pip install --upgrade numpy\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Ensure proper datetime conversion\n",
    "linked_df['year_received'] = pd.to_datetime(linked_df['date_received_in_opg']).dt.year\n",
    "\n",
    "# Compute mean donor age per year\n",
    "ts_data = linked_df.groupby('year_received')['donor_age_at_investigation'].mean()\n",
    "\n",
    "# Create a dummy variable: 1 if post-2020, 0 if before\n",
    "linked_df['post_policy'] = (linked_df['year_received'] >= 2020).astype(int)\n",
    "\n",
    "# Dummy variable for treatment (e.g., Health & Welfare cases vs. Finance cases)\n",
    "linked_df['treated'] = (linked_df['casesubtype'] == 'hw').astype(int)\n",
    "\n",
    "# Interaction term (DiD effect)\n",
    "linked_df['post_treated'] = linked_df['post_policy'] * linked_df['treated']\n",
    "\n",
    "# Run a DiD regression\n",
    "model = smf.ols(\"donor_age_at_investigation ~ post_policy + treated + post_treated\", data=linked_df).fit()\n",
    "print(model.summary())\n",
    "\n",
    " # If the post_treated coefficient is statistically significant, \n",
    " #    it suggests that the intervention (policy change) affected investigations.\n",
    "# Dependent Variable: donor_age_at_investigation\n",
    "# R-squared: 0.006\n",
    "# This indicates that the model explains only 0.6% of the variance in the dependent variable. \n",
    "# Essentially, the model has very low explanatory power.\n",
    "# Adjusted R-squared: 0.006\n",
    "# Similar to R-squared, it adjusts for the number of predictors in the model. Here, it also indicates very low explanatory power.\n",
    "# F-statistic: 39.50\n",
    "# This tests the overall significance of the model. A higher F-statistic suggests that the model is statistically significant.\n",
    "# Prob (F-statistic): 1.96e-25\n",
    "# The p-value associated with the F-statistic. Since it is much less than 0.05, the model is statistically significant.\n",
    "# Coefficients:\n",
    "# Intercept: 81.6097\n",
    "# This is the average donor age at investigation when all other variables are zero.\n",
    "# post_policy: 0.3935\n",
    "# This coefficient represents the change in donor age at investigation post-2020. The p-value (0.239) indicates that this\n",
    "# change is not statistically significant.\n",
    "# treated: -4.7951\n",
    "# This coefficient represents the difference in donor age at investigation between Health & Welfare cases and Finance cases. \n",
    "# The p-value (0.000) indicates that this difference is statistically significant.\n",
    "# post_treated: 1.6825\n",
    "# This interaction term represents the combined effect of post-2020 and treatment type. The p-value (0.182) indicates that this interaction effect is not statistically significant.\n",
    "# Diagnostic Tests:\n",
    "# Omnibus: 8707.441\n",
    "# This tests for the normality of residuals. A high value indicates non-normality.\n",
    "# Durbin-Watson: 2.006\n",
    "# This tests for autocorrelation in residuals. A value close to 2 suggests no autocorrelation.\n",
    "# Jarque-Bera (JB): 51834.606\n",
    "# This also tests for normality. A high value indicates non-normality.\n",
    "# Skew: -2.133\n",
    "# This indicates the distribution of residuals is left-skewed.\n",
    "# Kurtosis: 9.839\n",
    "# This indicates the distribution of residuals has heavy tails (leptokurtic).\n",
    "# Conclusion:\n",
    "# The model is statistically significant overall, but it explains very little of the variance in donor age at investigation. \n",
    "# The coefficient for treated is significant, suggesting that Health & Welfare cases have a lower average donor age at \n",
    "# investigation compared to Finance cases. However, the coefficients for post_policy and post_treated are not significant, \n",
    "# indicating no substantial change in donor age at investigation post-2020 or due to the interaction effect.\n",
    "# The diagnostics suggest non-normality in residuals, which might affect the reliability of the model. \n",
    "#You might want to consider additional variables or different model specifications to improve explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, wilcoxon\n",
    "\n",
    "# Split data before and after 2020\n",
    "before_pandemic = linked_df[linked_df['year_received'] < 2020]['donor_age_at_investigation']\n",
    "after_pandemic = linked_df[linked_df['year_received'] >= 2020]['donor_age_at_investigation']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(before_pandemic, after_pandemic)\n",
    "print(f\"T-Test: t={t_stat}, p={p_value}\")\n",
    "\n",
    "# Perform Wilcoxon test if data is non-normal\n",
    "if len(before_pandemic) == len(after_pandemic):  # Wilcoxon requires paired samples\n",
    "    w_stat, w_p_value = wilcoxon(before_pandemic, after_pandemic)\n",
    "    print(f\"Wilcoxon Test: w={w_stat}, p={w_p_value}\")\n",
    "\n",
    "    # If p-value < 0.05, the difference is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "1. Investigating Age Distribution Changes Before and After the Pandemic\n",
    "To investigate how the age distribution of donors at the time of investigation changed before and after the pandemic, you can use the following steps:\n",
    "\n",
    "Techniques:\n",
    "Descriptive Statistics: Calculate summary statistics (mean, median, standard deviation) for the age of donors before and after the pandemic.\n",
    "Visualization: Use histograms, box plots, and density plots to visualize the age distribution.\n",
    "Hypothesis Testing: Perform statistical tests (e.g., t-test, Mann-Whitney U test) to determine if there are significant differences in age distribution before and after the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Assuming linked_df is your DataFrame\n",
    "linked_df['age_at_investigation'] = (pd.to_datetime(\n",
    "    linked_df['date_received_in_opg']) - pd.to_datetime(\n",
    "    linked_df['client_donor_dob'])).dt.days / 365.25\n",
    "\n",
    "# Split data into before and after pandemic\n",
    "before_pandemic = linked_df[linked_df['date_received_in_opg'] < '2020-03-01']\n",
    "after_pandemic = linked_df[linked_df['date_received_in_opg'] >= '2020-03-01']\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f\"Pre-pandemic: {before_pandemic['age_at_investigation'].describe()}\")\n",
    "print(f\"Post-pandemic: {after_pandemic['age_at_investigation'].describe()}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(before_pandemic['age_at_investigation'], color='blue', label='Before Pandemic', kde=True)\n",
    "sns.histplot(after_pandemic['age_at_investigation'], color='red', label='After Pandemic', kde=True)\n",
    "plt.legend()\n",
    "plt.title('Age Distribution of Donors at Investigation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Hypothesis testing\n",
    "t_stat, p_value = ttest_ind(before_pandemic['age_at_investigation'], after_pandemic['age_at_investigation'])\n",
    "print(f'T-test: t_stat={t_stat}, p_value={p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "2. Analysing Changes in Case Types (‚Äòcasesubtype‚Äô and ‚Äòcase_type‚Äô)\n",
    "To assess whether shifts in case types contributed to the reduction:\n",
    "\n",
    "Techniques:\n",
    "Time Series Analysis: Visualizing case types over time.\n",
    "\n",
    "Chi-Square Test: Checking if case distributions changed pre- and post-pandemic.\n",
    "\n",
    "Logistic Regression: Predicting investigation likelihood based on case type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate case types by year\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "case_type_counts = linked_df.groupby(['year_received', 'casesubtype']).size().unstack()\n",
    "\n",
    "# Plot trends\n",
    "case_type_counts.plot(kind='line', figsize=(10,5), title=\"Case Type Trends Over Time\")\n",
    "plt.show()\n",
    "\n",
    "# Chi-square test\n",
    "pre_post_pivot = linked_df.pivot_table(\n",
    "    index='casesubtype', \n",
    "    columns=linked_df['date_received_in_opg'] >= '2020-03-01', \n",
    "    aggfunc='size', fill_value=0)\n",
    "\n",
    "chi2, p, dof, ex = chi2_contingency(pre_post_pivot)\n",
    "print(f\"Chi-Square Statistic: {chi2}, P-Value: {p}\")\n",
    "\n",
    "# Logistic Regression - Predicting Investigation Likelihood\n",
    "# linked_df['post_pandemic'] = linked_df['date_received_in_opg'] >= '2020-03-01'\n",
    "linked_df['post_pandemic'] = (linked_df['date_received_in_opg'] >= '2020-03-01').astype(int)\n",
    "\n",
    "model = smf.logit(\"post_pandemic ~ C(casesubtype)\", data=linked_df).fit()\n",
    "# model = smf.logit(\"post_pandemic ~ C(case_type) + C(casesubtype)\", data=linked_df).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# If chi-square p-value is low, case type proportions changed.\n",
    "# The chi-square test is used to determine if there is a significant association between two categorical variables. \n",
    "# In this case, it tests the association between casesubtype and whether the case was received before or \n",
    "# after the pandemic (post_pandemic).\n",
    "# Chi-Square Statistic: A high value (48.08) indicates a strong association between the variables.\n",
    "# P-Value: The extremely low p-value (4.09e-12) suggests that the association is statistically significant. \n",
    "# This means that the distribution of case subtypes before and after the pandemic is significantly different.\n",
    "\n",
    "# Logistic regression shows which case types were more/less likely to be investigated post-pandemic.\n",
    "# The logistic regression model predicts the likelihood of a case being received post-pandemic based on\n",
    "# the case subtype.\n",
    "# Intercept: 2.9569 (highly significant with p-value < 0.0001)\n",
    "# C(casesubtype)[T.pfa]: -0.7498 (also highly significant with p-value < 0.0001)\n",
    "# Intercept: The positive coefficient (2.9569) indicates that, in the absence of other factors, \n",
    "# the likelihood of a case being received post-pandemic is high.\n",
    "# C(casesubtype)[T.pfa]: The negative coefficient (-0.7498) suggests that cases of subtype pfa are less \n",
    "# likely to be received post-pandemic compared to the baseline subtype.\n",
    "\n",
    "# The chi-square test shows a significant change in case subtype distribution post-pandemic.\n",
    "# The logistic regression indicates that the subtype pfa is less likely to be received post-pandemic,\n",
    "# while the overall likelihood of cases being received post-pandemic is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "2. Analyzing Changes in Case Types (‚Äòcasesubtype‚Äô and ‚Äòcase_type‚Äô)\n",
    "Refinement: NLP & Clustering\n",
    "If there are text-based ‚Äòconcern_type‚Äô descriptions, we can use NLP-based topic modeling to group similar concerns over time.\n",
    "\n",
    "Clustering (K-Means, DBSCAN) can group case subtypes to reveal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A. Topic Modeling for Case Subtypes\n",
    "\n",
    "# Converts text data into a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# Applies Latent Dirichlet Allocation (LDA) for topic modeling.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# TF-IDF Vectorizer: This transforms the text data (casesubtype) into a matrix of TF-IDF features. \n",
    "# TF-IDF stands for Term Frequency-Inverse Document Frequency, which helps in highlighting important words in the documents.\n",
    "# Stop Words: Common words like \"the\", \"and\", etc., are removed to focus on more meaningful words.\n",
    "# Convert case types into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(linked_df['casesubtype'].astype(str))\n",
    "\n",
    "# creates an LDA model with 5 topics (n_components=5). LDA is a generative probabilistic \n",
    "# model that assumes each document is a mixture of topics and each topic is a mixture of words.\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "#  The model is fitted to the TF-IDF matrix and transforms it into topic distributions.\n",
    "topics = lda.fit_transform(X)\n",
    "\n",
    "# For each topic, the top words are displayed. topic.argsort()[-5:] \n",
    "# sorts the words by their importance in the topic and selects the top 5.\n",
    "# Display top words in each topic\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i}: {[vectorizer.get_feature_names_out()[j] for j in topic.argsort()[-5:]]}\")\n",
    "\n",
    "# This topic modeling helps identify patterns in case subtypes that may have changed pre/post-pandemic. \n",
    "# By understanding these patterns, you can gain insights into how different case types have evolved over time.\n",
    "# Helps identify case subtype patterns that may have changed pre/post-pandemic.\n",
    "# The results show the top words for each topic. However, the presence of 'nan' suggests that \n",
    "# there might be missing or improperly formatted data in the casesubtype column. \n",
    "# clean the data to remove or handle 'nan' values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# B. Clustering Subtypes Over Time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Convert categorical case subtypes into numeric representations\n",
    "linked_df['casesubtype_encoded'] = linked_df['casesubtype'].astype('category').cat.codes\n",
    "\n",
    "print(linked_df['casesubtype_encoded'])\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "linked_df['cluster'] = kmeans.fit_predict(\n",
    "    linked_df[['casesubtype_encoded', 'year_received']])\n",
    "\n",
    "# Visualize Changes in Clusters Over Time\n",
    "sns.lineplot(data=linked_df, x='year_received', y='cluster', hue='casesubtype', marker='o')\n",
    "plt.title('Case Subtype Clustering Over Time')\n",
    "plt.show()\n",
    "\n",
    "# If certain clusters disappear or emerge post-pandemic, they could explain the step reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_subtype_counts_after['pfa']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "2. Showing Influence of 'casesubtype' and 'case_type' Changes\n",
    "To show how different 'casesubtype' and 'case_type' changes influenced the step reduction in investigation cases, you can use:\n",
    "\n",
    "Techniques:\n",
    "Categorical Analysis: Analyze the frequency and distribution of different case subtypes and types before and after the pandemic.\n",
    "Chi-Square Test: Perform chi-square tests to determine if there are significant differences in the distribution of case subtypes and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Split data into before and after pandemic\n",
    "before_pandemic = linked_df[linked_df['date_received_in_opg'] < '2020-03-01']\n",
    "after_pandemic = linked_df[linked_df['date_received_in_opg'] >= '2020-03-01']\n",
    "\n",
    "# Frequency distribution\n",
    "case_subtype_counts_before = before_pandemic['casesubtype'].value_counts()\n",
    "case_subtype_counts_after = after_pandemic['casesubtype'].value_counts()\n",
    "\n",
    "case_type_counts_before = before_pandemic['poa_case_type'].value_counts()\n",
    "case_type_counts_after = after_pandemic['poa_case_type'].value_counts()\n",
    "\n",
    "print(f\"Pre-pandemic: {case_subtype_counts_before}\")\n",
    "print(f\"Pre-pandemic (pfa/hw): {round(case_subtype_counts_before['pfa']/case_subtype_counts_before['hw'],2)}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=case_subtype_counts_before.index, \n",
    "            y=case_subtype_counts_before.values, \n",
    "            color='blue', label='Before Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Case Subtype Distribution Before Pandemic')\n",
    "plt.xlabel('Case Subtype')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Post-pandemic: {case_subtype_counts_after}\")\n",
    "print(f\"Post-pandemic (pfa/hw): {round(case_subtype_counts_after['pfa']/case_subtype_counts_after['hw'],2)}\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=case_subtype_counts_after.index, y=case_subtype_counts_after.values, \n",
    "            color='red', label='After Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Case Subtype Distribution After Pandemic')\n",
    "plt.xlabel('Case Subtype')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=case_type_counts_before.index, y=case_type_counts_before.values, \n",
    "            color='blue', label='Before Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Case Type Distribution Before Pandemic')\n",
    "plt.xlabel('Case Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=case_type_counts_after.index, y=case_type_counts_after.values, \n",
    "            color='red', label='After Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Case Type Distribution After Pandemic')\n",
    "plt.xlabel('Case Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Chi-square test\n",
    "contingency_table_subtype = pd.crosstab(linked_df['casesubtype'], \n",
    "                                        linked_df['date_received_in_opg'] >= '2020-03-01')\n",
    "chi2_stat_subtype, p_val_subtype, dof_subtype, ex_subtype = chi2_contingency(contingency_table_subtype)\n",
    "print(f'Chi-square test for case subtype: chi2_stat={chi2_stat_subtype}, p_value={p_val_subtype}')\n",
    "\n",
    "contingency_table_type = pd.crosstab(linked_df['poa_case_type'], \n",
    "                                     linked_df['date_received_in_opg'] >= '2020-03-01')\n",
    "chi2_stat_type, p_val_type, dof_type, ex_type = chi2_contingency(contingency_table_type)\n",
    "print(f'Chi-square test for case type: chi2_stat={chi2_stat_type}, p_value={p_val_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggregate case types by year\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "case_type_counts = linked_df.groupby(['year_received', 'casesubtype']).size().unstack()\n",
    "\n",
    "# Plot trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=case_type_counts, palette=\"tab10\")\n",
    "plt.title(\"Case Type Trends Over Time\")\n",
    "plt.xlabel(\"Year Received\")\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.legend(title=\"Case Subtype\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Chi-square test\n",
    "pre_post_pivot = linked_df.pivot_table(\n",
    "    index='casesubtype', \n",
    "    columns=linked_df['date_received_in_opg'] >= '2020-03-01', \n",
    "    aggfunc='size', fill_value=0)\n",
    "\n",
    "chi2, p, dof, ex = chi2_contingency(pre_post_pivot)\n",
    "print(f\"Chi-Square Statistic: {chi2}, P-Value: {p}\")\n",
    "\n",
    "# Bar plot to show differences before and after pandemic\n",
    "pre_post_counts = linked_df.groupby(['casesubtype', linked_df['date_received_in_opg'] >= '2020-03-01']).size().unstack()\n",
    "pre_post_counts.columns = ['Before Pandemic', 'After Pandemic']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pre_post_counts[['Before Pandemic', 'After Pandemic']].plot(kind='bar', color=['skyblue', 'salmon'], edgecolor='black')\n",
    "plt.title(\"Case Subtype Counts Before and After Pandemic\")\n",
    "plt.xlabel(\"Case Subtype\")\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Period\")\n",
    "plt.grid(True)\n",
    "\n",
    "# # Annotate bars\n",
    "# for idx, row in pre_post_counts.loc[['pfa', 'hw']].iterrows():\n",
    "#     for col, value in row.items():\n",
    "#         plt.text(idx, value + 50, f'{value}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression - Predicting Investigation Likelihood\n",
    "linked_df['post_pandemic'] = (linked_df['date_received_in_opg'] >= '2020-03-01').astype(int)\n",
    "model = smf.logit(\"post_pandemic ~ C(casesubtype)\", data=linked_df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "3. Investigating Gradual vs. Step Decline in Investigation Cases (2016 Onwards)\n",
    "To determine if the decline was gradual (since 2016) or a sharp step drop post-pandemic:\n",
    "\n",
    "Techniques:\n",
    "Time Series Decomposition: Breaking down long-term trends and seasonality.\n",
    "\n",
    "Change Point Detection: Identifying structural breaks in investigation rates.\n",
    "\n",
    "Interrupted Time Series Analysis (ITSA): Evaluating the impact of pandemic on investigation trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ruptures\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Aggregate investigation counts by year\n",
    "investigations_by_year = linked_df.groupby('year_received').size()\n",
    "\n",
    "# Time Series Decomposition\n",
    "decomposed = seasonal_decompose(investigations_by_year, model='additive', period=1)\n",
    "decomposed.plot()\n",
    "plt.show()\n",
    "\n",
    "# Change Point Detection\n",
    "algo = rpt.Pelt(model=\"rbf\").fit(investigations_by_year.values.reshape(-1, 1))\n",
    "breakpoints = algo.predict(pen=10)\n",
    "print(\"Change Points Detected at Years:\", \n",
    "      [list(investigations_by_year.index)[bp] for bp in breakpoints[:-1]])\n",
    "\n",
    "# ITSA - Comparing Pre/Post Pandemic Trends\n",
    "linked_df['post_pandemic'] = (linked_df['year_received'] >= 2020).astype(int)\n",
    "linked_df['casesubtype_numeric'] = linked_df['casesubtype'].astype('category').cat.codes\n",
    "model = smf.ols(\"casesubtype_numeric ~ year_received + post_pandemic\", data=linked_df).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Change point detection helps confirm whether the drop was gradual or sudden.\n",
    "# No change points were detected in the time series data, indicating that there \n",
    "# were no significant shifts or structural breaks in the investigation counts over the years.\n",
    "# ITSA helps quantify the impact of the pandemic on investigation trends.\n",
    "# R-squared (0.010): This indicates that only 1% of the variance in casesubtype_numeric is explained by the model. This is quite low, suggesting that the model does not fit the data well.\n",
    "# Intercept (43.8936): This is the expected value of casesubtype_numeric when year_received is zero and post_pandemic is zero. The high t-value (13.594) and low p-value (< 0.0001) indicate that the intercept is statistically significant.\n",
    "# year_received (-0.0213): This negative coefficient suggests that as the year increases, the casesubtype_numeric value slightly decreases. The high t-value (-13.302) and low p-value (< 0.0001) indicate that this effect is statistically significant.\n",
    "# post_pandemic (0.0253): This positive coefficient indicates that cases received post-pandemic have a slightly higher casesubtype_numeric value. The t-value (2.555) and p-value (0.011) suggest that this effect is statistically significant.\n",
    "# Omnibus (10529.441) and Jarque-Bera (54999.577): These tests indicate that the residuals are not \n",
    "# normally distributed, which might affect the validity of the model.\n",
    "# Durbin-Watson (1.960): This value is close to 2, suggesting that there is no significant autocorrelation\n",
    "# in the residuals.\n",
    "# Condition Number (3.05e+06): A high condition number indicates potential multicollinearity issues, \n",
    "# which means that the independent variables might be highly correlated.\n",
    "# Overall, while the model shows some statistically significant relationships, the low R-squared value \n",
    "# and potential multicollinearity suggest that the model may not be very reliable for predicting \n",
    "# casesubtype_numeric. Further investigation and potentially more complex modeling might be needed \n",
    "# to better understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Findings to Look for:\n",
    "Age Analysis: If older donors were investigated less post-pandemic, it may explain some of the decline.\n",
    "\n",
    "Case Type Shifts: A significant drop in certain cases (e.g., Health & Welfare) might suggest policy changes.\n",
    "\n",
    "Trend Analysis: If Finance & Property cases show a step decline while Health & Welfare cases decline gradually, it supports the operational decision hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "3. Investigating Gradual vs. Step Decline in Investigation Cases (2016 Onwards)\n",
    "Refinement: Structural Breaks & Granger Causality\n",
    "To determine if the step reduction aligns with operational decisions, we can:\n",
    "\n",
    "Detect breakpoints in investigation rates using Bayesian Change Point Detection.\n",
    "\n",
    "Apply Granger Causality to test whether operational changes caused case volume reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A. Bayesian Change Point Detection\n",
    "\n",
    "# import numpy as np\n",
    "# import pymc3 as pm\n",
    "\n",
    "# # Convert investigation counts to numpy array\n",
    "# y = investigations_by_year.values\n",
    "\n",
    "# # Define Model\n",
    "# with pm.Model():\n",
    "#     tau = pm.DiscreteUniform(\"tau\", lower=0, upper=len(y)-1)\n",
    "#     mu1 = pm.Normal(\"mu1\", mu=np.mean(y[:len(y)//2]), sigma=np.std(y))\n",
    "#     mu2 = pm.Normal(\"mu2\", mu=np.mean(y[len(y)//2:]), sigma=np.std(y))\n",
    "    \n",
    "#     # Likelihood\n",
    "#     idx = np.arange(len(y))\n",
    "#     mu = pm.math.switch(tau > idx, mu1, mu2)\n",
    "#     obs = pm.Normal(\"obs\", mu=mu, sigma=np.std(y), observed=y)\n",
    "\n",
    "#     trace = pm.sample(2000, return_inferencedata=True)\n",
    "\n",
    "# # Plot Posterior Distribution of Breakpoint\n",
    "# az.plot_posterior(trace, var_names=[\"tau\"])\n",
    "\n",
    "# # If the posterior distribution of tau (change point) aligns with 2016 or the pandemic, it suggests a structural change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_granger_noinx=df_granger.reset_index()\n",
    "df_granger_noinx#.columns\n",
    "df_granger_noinx.index\n",
    "len(df_granger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pandas as pd\n",
    "\n",
    "# Convert casesubtype to numeric codes\n",
    "linked_df['casesubtype_numeric'] = linked_df['casesubtype'].astype('category').cat.codes\n",
    "\n",
    "# Creating a DataFrame with lagged operational decisions\n",
    "df_granger = linked_df[['year_received', 'casesubtype_numeric']].pivot_table(\n",
    "    index='year_received', columns='casesubtype_numeric', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Add operational decision variable\n",
    "df_granger['triage_removed'] = (df_granger.index >= 2016).astype(int)\n",
    "\n",
    "# Create a rolling mean to make 'triage_removed' dynamic\n",
    "df_granger['triage_removed_smooth'] = df_granger['triage_removed'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Check for constant columns and remove them\n",
    "df_granger = df_granger.loc[:, df_granger.nunique() > 1]\n",
    "print(\"Columns after filtering constant values:\", df_granger.columns)\n",
    "\n",
    "# Ensure 'year_received' is the index and formatted correctly\n",
    "df_granger.index = pd.to_datetime(df_granger.index, format='%Y')\n",
    "\n",
    "# Determine maximum allowable lag\n",
    "max_lag = min(2, len(df_granger) - 1)\n",
    "print(\"Max lag:\", max_lag)\n",
    "\n",
    "# Select variables for Granger causality test\n",
    "target_variable = [col for col in df_granger.columns if col not in ['triage_removed', 'triage_removed_smooth']][0]\n",
    "columns_for_test = [target_variable, 'triage_removed_smooth']\n",
    "print(\"Selected columns for Granger test:\", columns_for_test)\n",
    "\n",
    "# Run Granger causality test\n",
    "grangercausalitytests(df_granger[columns_for_test], maxlag=max_lag, verbose=True)\n",
    "\n",
    "# The Granger causality test checks whether past values of one time series (triage_removed_smooth) help predict another time series (target_variable). \n",
    "# Each test result contains the following:\n",
    "# ssr_ftest (F-test for joint significance): Tests if lagged values significantly improve predictions.\n",
    "# ssr_chi2test (Chi-square test): Checks if adding lagged variables reduces residual error.\n",
    "# lrtest (Likelihood ratio test): Compares model fits with and without the lagged predictor.\n",
    "# params_ftest: Similar to ssr_ftest, but tests parameter significance.\n",
    "# Each metric has:\n",
    "# A statistic value (higher suggests stronger causality).\n",
    "# A p-value (lower means stronger statistical significance).\n",
    "# Degrees of freedom (df) used in calculations.\n",
    "\n",
    "# Lag 1 (1-year lag)\n",
    "# F-test (ssr_ftest): F=17.34, p = 0.0059 (statistically significant)\n",
    "# Chi-square test (ssr_chi2test): ùúí2=26.00, p = 3.41 \\times 10^{-7} (highly significant)\n",
    "# Likelihood ratio test (lrtest): ùúí2=12.22, p = 0.00047 (significant)\n",
    "# ‚úÖ At lag 1, triage_removed_smooth Granger-causes target_variable with high confidence.\n",
    "# p-values < 0.01 suggest strong evidence that removing triage affects investigation rates after 1 year.\n",
    "\n",
    "# Lag 2 (2-year lag)\n",
    "# F-test (ssr_ftest): F=7.62, p = 0.0667 (borderline significance)\n",
    "# Chi-square test (ssr_chi2test): ùúí2=40.64, p = 1.50 \\times 10^{-9} (very significant)\n",
    "# Likelihood ratio test (lrtest): œá 2=14.44, p = 0.00073 (significant)\n",
    "# üî∏ At lag 2, triage_removed_smooth still Granger-causes target_variable, but with slightly weaker confidence.\n",
    "# The F-test p-value (0.0667) is above 0.05, suggesting weaker support, but the Chi-square test remains highly significant.\n",
    "\n",
    "# Triage removal strongly influences investigation counts after 1 year (Lag 1, p < 0.01).\n",
    "# Lag 2 also shows a causal effect, but slightly weaker.\n",
    "# Overall, this supports the hypothesis that removing triage led to a step reduction in investigations.\n",
    "\n",
    "# üîé Next Steps\n",
    "# Consider a Vector Autoregression (VAR) model to quantify dynamic relationships.\n",
    "# Test additional lags (3+ years) to see if effects persist.\n",
    "# Explore causal inference techniques (DAGs, synthetic controls) to validate findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "3. Investigating Trends in Investigation Rates\n",
    "To investigate trends in investigation rates for different case types and subtypes, you can use:\n",
    "\n",
    "Techniques:\n",
    "Time Series Analysis: Analyze the trends over time using line plots and statistical tests.\n",
    "Regression Analysis: Use regression models to identify trends and changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Group by year and case type/subtype\n",
    "yearly_counts = linked_df.groupby(['year_concluded', 'case_type']).size().unstack().fillna(0)\n",
    "yearly_counts_subtype = linked_df.groupby(['year_concluded', 'casesubtype']).size().unstack().fillna(0)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_counts.plot(kind='line', marker='o')\n",
    "plt.title('Yearly Investigation Counts by Case Type')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.legend(title='Case Type')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_counts_subtype.plot(kind='line', marker='o')\n",
    "plt.title('Yearly Investigation Counts by Case Subtype')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.legend(title='Case Subtype')\n",
    "plt.show()\n",
    "\n",
    "# Time series decomposition\n",
    "for case_type in yearly_counts.columns:\n",
    "    result = seasonal_decompose(yearly_counts[case_type], model='additive', period=1)\n",
    "    result.plot()\n",
    "    plt.title(f'Time Series Decomposition for {case_type}')\n",
    "    plt.show()\n",
    "\n",
    "# Regression analysis\n",
    "\n",
    "X = yearly_counts.index.values.reshape(-1, 1)\n",
    "for case_type in yearly_counts.columns:\n",
    "    y = yearly_counts[case_type].values\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    print(f'Regression analysis for {case_type}:')\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "Deep Dive into Outliers\n",
    "\n",
    "If we find an anomaly in 2016 or 2020, investigate if it aligns with internal reports or policy changes.\n",
    "\n",
    "Interactive Dashboards\n",
    "\n",
    "Use Plotly or Dash to create an interactive visualization for stakeholders.\n",
    "\n",
    "Predictive Modeling for Future Investigations\n",
    "\n",
    "Use XGBoost or Random Forest to predict the number of investigations based on past trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "A dashboard will help OPG stakeholders visualize the trends, analyze case distributions, and interactively explore insights. I'll create a Plotly Dash application that includes:\n",
    "\n",
    "Age Distribution Analysis ‚Äì A histogram/KDE plot comparing pre- and post-pandemic donor ages.\n",
    "\n",
    "Case Type Trends Over Time ‚Äì A time-series line chart showing case subtypes from 2016 onward.\n",
    "\n",
    "Investigation Volume & Structural Changes ‚Äì A bar chart with annotations marking key events (e.g., triage removal in 2016, pandemic in 2020).\n",
    "\n",
    "Case Type Breakdown (Pie Chart) ‚Äì To compare case distributions over different periods.\n",
    "\n",
    "Changepoint Detection & Forecasting ‚Äì Structural break analysis visualized dynamically.\n",
    "\n",
    "This Plotly Dash app provides an interactive dashboard for OPG stakeholders to explore:\n",
    "\n",
    "Donor Age Distribution before and after the pandemic.\n",
    "\n",
    "Case Type Trends Over Time (line chart).\n",
    "\n",
    "Investigation Volume Changes with key events like triage removal in 2016.\n",
    "\n",
    "Case Type Breakdown (pie chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# !pip install --upgrade dash\n",
    "# !pip install --upgrade plotty\n",
    "\n",
    "# # Uninstall the current version of typing_extensions\n",
    "# !pip uninstall typing-extensions -y\n",
    "\n",
    "# # Install the latest version of typing_extensions\n",
    "# !pip install typing-extensions --upgrade\n",
    "\n",
    "# !pip install jupyter-dash --upgrade\n",
    "\n",
    "# !pip install ipywidgets\n",
    "\n",
    "# !jupyter nbextension list\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# !jupyter labextension install plotlywidget\n",
    "# !jupyter labextension install jupyterlab-dash\n",
    "# !jupyter nbextension install --sys-prefix --py jupyter_dash\n",
    "# !jupyter nbextension enable --sys-prefix --py jupyter_dash\n",
    "\n",
    "!jupyter nbextension install --py jupyter_dash --sys-prefix\n",
    "!jupyter nbextension enable --py jupyter_dash --sys-prefix\n",
    "\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure linked_df exists\n",
    "if 'linked_df' not in locals():\n",
    "    raise ValueError(\"linked_df is not defined. Load the dataset before running the dashboard.\")\n",
    "\n",
    "# Convert to datetime\n",
    "linked_df['date_received_in_opg'] = pd.to_datetime(linked_df['date_received_in_opg'])\n",
    "linked_df['client_donor_dob'] = pd.to_datetime(linked_df['client_donor_dob'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Extract year\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "\n",
    "# Calculate donor age at investigation\n",
    "linked_df['donor_age_at_investigation'] = (linked_df['date_received_in_opg'] - linked_df['client_donor_dob']).dt.days / 365.25\n",
    "\n",
    "# Aggregate data\n",
    "case_type_trend = linked_df.groupby(['year_received', 'casesubtype']).size().reset_index(name='count')\n",
    "\n",
    "# Create Dash App\n",
    "app = JupyterDash(__name__)  # Use JupyterDash for Jupyter Lab compatibility\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"OPG Investigation Dashboard\"),\n",
    "    \n",
    "    # Age Distribution Comparison\n",
    "    dcc.Graph(\n",
    "        figure=px.histogram(\n",
    "            linked_df, \n",
    "            x='donor_age_at_investigation', \n",
    "            color=linked_df['year_received'].apply(lambda x: 'Post-2020' if x >= 2020 else 'Pre-2020'),\n",
    "            nbins=50, \n",
    "            title=\"Donor Age Distribution: Pre vs. Post Pandemic\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Case Type Trends\n",
    "    dcc.Graph(\n",
    "        figure=px.line(\n",
    "            case_type_trend, \n",
    "            x='year_received', \n",
    "            y='count', \n",
    "            color='casesubtype',\n",
    "            title=\"Trends in Case Types Over Time\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Investigation Volume with Key Events\n",
    "    dcc.Graph(\n",
    "        figure=px.bar(\n",
    "            linked_df.groupby('year_received').size().reset_index(name='count'), \n",
    "            x='year_received', \n",
    "            y='count',\n",
    "            title=\"Annual Investigation Volume\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Case Type Breakdown (Pie Chart)\n",
    "    dcc.Graph(\n",
    "        figure=px.pie(\n",
    "            linked_df, \n",
    "            names='casesubtype', \n",
    "            title=\"Case Type Distribution\"\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "# Run app inside Jupyter Lab\n",
    "#app.run(mode='inline')  # Try mode='external' if still not displaying\n",
    "app.run(mode='inline') #, port=8051) # Try different ports (8052, 8053, etc.).\n",
    "#app.run(mode='external')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "additional features, such as changepoint detection visualizations or predictive modeling? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Hello Dash!\"),\n",
    "    dcc.Graph()\n",
    "])\n",
    "\n",
    "app.run(mode='inline')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
