{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "How to explain the drivers and factors that caused a step reduction in the investigation cases for Lasting Power of Attorney (LPA) demands from Office of Public Guardian (OPG)? \n",
    "To achieve this, basically we linked Investigation and LPA dataset which resulted a data frame in Python with the following variables (columns): linked_df[['uid','donor_id','lpa_reg_date','lpa_status','lpa_rec_date','poa_type','unique_id','case_no','client_donor_dob','case_type','concern_type','date_received_in_opg','status','mojap_extract_date','poa_case_type','casesubtype','poa_rec_to_invest_rec','year_concluded','link_id','uid_to_link']].  \n",
    "\n",
    "\n",
    "1. How to investigate the age distribution (based on the dob of donor ('client_donor_dob') at the investigation ('date_received_in_opg') changes before and after pandemic that might be the reason for the step reduction in investigation of LPA cases? \n",
    "2. How to show that different 'casesubtype' and 'case_type' changes influenced this step reduction in the investigation of LPA cases? \n",
    "3. investigate whether: \n",
    "    - It suggests that the downward trend in the investigations rate for Health and Welfare cases or where investigations have included both Health and Welfare AND Property and Finance concerns have been gradual since 2016 rather than a step reduction associated with the pandemic. Having said that the rate of investigations particularly for Health and Welfare cases levelled off after the pandemic.\n",
    "    - There isn‚Äôt any evidence of a gradual decline in Finance and Property cases, but instead the pattern that we have discussed before of a sustained stepped reduction following the pandemic.\n",
    "    - The gradual decline in Health and Welfare and combined concerns from 2016 is interesting because it also coincides with what I believe was an operational decision at that time to remove the triage process for LPA investigations. This had the immediate effect of increasing the number of concerns accepted for investigation, which can be seen in the attached charts, followed by a gradual decline.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install --upgrade pandas\n",
    "import pandas as pd\n",
    "!pip install --upgrade numpy\n",
    "# print(np.__version__)\n",
    "# print(np.__path__)\n",
    "linked_df = pd.read_csv('inv_linked_lpa_data.csv')\n",
    "linked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "1. Investigating Age Distribution Changes Pre- and Post-Pandemic\n",
    "To check whether changes in donor age at the time of investigation contributed to the reduction:\n",
    "\n",
    "Techniques:\n",
    "Descriptive Statistics & Visualization: Calculate mean, median, and IQR of donor age pre- and post-pandemic.\n",
    "\n",
    "Kernel Density Estimation (KDE) & Histograms: Compare the age distributions before and after the pandemic.\n",
    "\n",
    "Kolmogorov-Smirnov (KS) Test: Check if the distribution of ages significantly changed.\n",
    "\n",
    "Causal Inference (Difference-in-Differences - DiD): Compare the mean age before and after the pandemic with a control period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Overall proportion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This updated code includes:\n",
    "# Kernel Density Estimation (KDE) plots to show the age distribution of donors at investigation before and after the pandemic.\n",
    "# Bar charts to visualize the number of investigations by age group before and after the pandemic.\n",
    "# These visualizations should provide a clearer picture of how the investigation demand and age distribution have changed due to the pandemic. \n",
    "\n",
    "!pip install --upgrade scipy matplotlib\n",
    "#!pip install matplotlib\n",
    "# !pip uninstall seaborn\n",
    "# !pip install seaborn\n",
    "!pip install --upgrade seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Convert to datetime\n",
    "linked_df['client_donor_dob'] = pd.to_datetime(linked_df['client_donor_dob'], errors='coerce', dayfirst=True)\n",
    "linked_df['date_received_in_opg'] = pd.to_datetime(linked_df['date_received_in_opg'])\n",
    "\n",
    "# Calculate donor age at investigation\n",
    "linked_df['donor_age_at_investigation'] = (linked_df['date_received_in_opg'] - linked_df['client_donor_dob']).dt.days / 365.25\n",
    "\n",
    "# Define age groups\n",
    "bins = [0, 18, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['0-18', '19-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "linked_df['age_group'] = pd.cut(linked_df['donor_age_at_investigation'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Split pre- and post-pandemic (assuming March 2020 as pandemic start)\n",
    "pre_pandemic = linked_df[linked_df['date_received_in_opg'] < '2020-03-01']\n",
    "post_pandemic = linked_df[linked_df['date_received_in_opg'] >= '2020-03-01']\n",
    "\n",
    "# Calculate overall proportion of investigations in each age group\n",
    "age_group_counts = linked_df['age_group'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Calculate proportion of investigations in each age group for pre- and post-pandemic periods\n",
    "pre_proportion = pre_pandemic['age_group'].value_counts(normalize=True).sort_index()\n",
    "post_proportion = post_pandemic['age_group'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Plot KDE for age distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(pre_pandemic['donor_age_at_investigation'], label='Pre-Pandemic', shade=True)\n",
    "sns.kdeplot(post_pandemic['donor_age_at_investigation'], label='Post-Pandemic', shade=True)\n",
    "plt.legend()\n",
    "plt.title('Age Distribution of Donors at Investigation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for absolute number of investigations by age group\n",
    "plt.figure(figsize=(12, 6))\n",
    "pre_counts = pre_pandemic['age_group'].value_counts().sort_index()\n",
    "post_counts = post_pandemic['age_group'].value_counts().sort_index()\n",
    "bar_width = 0.35\n",
    "index = range(len(labels))\n",
    "\n",
    "plt.bar(index, pre_counts, bar_width, label='Pre-Pandemic')\n",
    "plt.bar([i + bar_width for i in index], post_counts, bar_width, label='Post-Pandemic')\n",
    "\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.title('Investigation Demand by Age Group Before and After Pandemic')\n",
    "plt.xticks([i + bar_width / 2 for i in index], labels)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for overall proportion of investigation demands by age group\n",
    "plt.figure(figsize=(10, 5))\n",
    "age_group_counts.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Proportion of Investigations')\n",
    "plt.title('Overall Proportion of Investigation Demands by Age Group')\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side bar chart for proportion of investigations pre- vs post-pandemic\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(labels))\n",
    "\n",
    "plt.bar(index, pre_proportion, bar_width, label='Pre-Pandemic', color='blue', alpha=0.6)\n",
    "plt.bar([i + bar_width for i in index], post_proportion, bar_width, label='Post-Pandemic', color='red', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Proportion of Investigations')\n",
    "plt.title('Proportion of Investigations by Age Group: Pre vs Post Pandemic')\n",
    "plt.xticks([i + bar_width / 2 for i in index], labels)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform KS test\n",
    "ks_stat, p_value = ks_2samp(pre_pandemic['donor_age_at_investigation'], post_pandemic['donor_age_at_investigation'])\n",
    "print(f\"KS Statistic: {ks_stat}, P-Value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "1. Investigating Age Distribution Changes Before and After the Pandemic\n",
    "To investigate how the age distribution of donors at the time of investigation changed before and after the pandemic, you can use the following steps:\n",
    "\n",
    "Techniques:\n",
    "Descriptive Statistics: Calculate summary statistics (mean, median, standard deviation) for the age of donors before and after the pandemic.\n",
    "Visualization: Use histograms, box plots, and density plots to visualize the age distribution.\n",
    "Hypothesis Testing: Perform statistical tests (e.g., t-test, Mann-Whitney U test) to determine if there are significant differences in age distribution before and after the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, wilcoxon\n",
    "\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "linked_df['date_received_in_opg'] = pd.to_datetime(linked_df['date_received_in_opg'])\n",
    "# Assuming linked_df is your DataFrame\n",
    "linked_df['age_at_investigation'] = (pd.to_datetime(\n",
    "    linked_df['date_received_in_opg']) - pd.to_datetime(\n",
    "    linked_df['client_donor_dob'])).dt.days / 365.25\n",
    "\n",
    "# Split data into before and after pandemic\n",
    "before_pandemic = linked_df[(linked_df['date_received_in_opg'] < '2020-01-01') & (linked_df['date_received_in_opg'] >= '2018-01-01')]\n",
    "after_pandemic = linked_df[(linked_df['date_received_in_opg'] >= '2023-01-01') & (linked_df['date_received_in_opg'] < '2025-01-01')]\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f\"Pre-pandemic: {before_pandemic['age_at_investigation'].describe()}\")\n",
    "print(f\"Post-pandemic: {after_pandemic['age_at_investigation'].describe()}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(before_pandemic['age_at_investigation'], color='blue', label='Before Pandemic', kde=True)\n",
    "sns.histplot(after_pandemic['age_at_investigation'], color='red', label='After Pandemic', kde=True)\n",
    "plt.legend()\n",
    "plt.title('Age Distribution of Donors at Investigation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Hypothesis testing\n",
    "t_stat, p_value = ttest_ind(before_pandemic['age_at_investigation'], after_pandemic['age_at_investigation'])\n",
    "print(f'T-test: t_stat={t_stat}, p_value={p_value}')\n",
    "\n",
    "# Perform Wilcoxon test if data is non-normal\n",
    "if len(before_pandemic) == len(after_pandemic):  # Wilcoxon requires paired samples\n",
    "    w_stat, w_p_value = wilcoxon(before_pandemic['age_at_investigation'], after_pandemic['age_at_investigation'])\n",
    "    print(f\"Wilcoxon Test: w={w_stat}, p={w_p_value}\")\n",
    "\n",
    "    # If p-value < 0.05, the difference is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "2. Analysing Changes in Case Types (‚Äòcasesubtype‚Äô and ‚Äòcase_type‚Äô)\n",
    "To assess whether shifts in case types contributed to the reduction:\n",
    "\n",
    "Techniques:\n",
    "Time Series Analysis: Visualizing case types over time.\n",
    "\n",
    "Chi-Square Test: Checking if case distributions changed pre- and post-pandemic.\n",
    "\n",
    "Logistic Regression: Predicting investigation likelihood based on case type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate case types by year\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "\n",
    "# Assuming linked_df is your DataFrame\n",
    "linked_df['age_at_investigation'] = (pd.to_datetime(\n",
    "    linked_df['date_received_in_opg']) - pd.to_datetime(\n",
    "    linked_df['client_donor_dob'])).dt.days / 365.25\n",
    "\n",
    "case_type_counts = linked_df.groupby(['year_received', 'concern_type']).size().unstack()\n",
    "\n",
    "# Plot trends\n",
    "case_type_counts.plot(kind='line', figsize=(20,10), title=\"Investigation_Concern_Type_for_LPA_Trends_Over_Time\")\n",
    "# Save the figure\n",
    "plt.savefig('images/Investigation_Concern_Type_for_LPA_Trends_Over_Time.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Aggregate case types by month\n",
    "linked_df['month_received'] = linked_df['date_received_in_opg'].dt.to_period('M')\n",
    "\n",
    "case_type_counts_monthly = linked_df.groupby(['month_received', 'concern_type']).size().unstack()\n",
    "\n",
    "# Plot trends\n",
    "case_type_counts_monthly.plot(kind='line', figsize=(20,10), title=\"Investigation Concern Type for LPA Trends Over Time (Monthly)\")\n",
    "# Save the figure\n",
    "plt.savefig('images/Investigation_Concern_Type_for_LPA_Trends_Over_Time_Monthly.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Chi-square test\n",
    "pre_post_pivot = linked_df.pivot_table(\n",
    "    index='concern_type', \n",
    "    columns=linked_df['date_received_in_opg'] >= '2020-03-01', \n",
    "    aggfunc='size', fill_value=0)\n",
    "\n",
    "chi2, p, dof, ex = chi2_contingency(pre_post_pivot)\n",
    "print(f\"Chi-Square Statistic: {chi2}, P-Value: {p}\")\n",
    "\n",
    "# Logistic Regression - Predicting Investigation Likelihood\n",
    "linked_df['post_pandemic'] = (linked_df['date_received_in_opg'] >= '2020-03-01').astype(int)\n",
    "\n",
    "model = smf.logit(\"post_pandemic ~ C(concern_type)\", data=linked_df).fit()\n",
    "# model = smf.logit(\"post_pandemic ~ C(case_type) + C(casesubtype)\", data=linked_df).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# If chi-square p-value is low, case type proportions changed.\n",
    "# The chi-square test is used to determine if there is a significant association between two categorical variables. \n",
    "# In this case, it tests the association between casesubtype and whether the case was received before or \n",
    "# after the pandemic (post_pandemic).\n",
    "# Chi-Square Statistic: A high value (48.08) indicates a strong association between the variables.\n",
    "# P-Value: The extremely low p-value (4.09e-12) suggests that the association is statistically significant. \n",
    "# This means that the distribution of case subtypes before and after the pandemic is significantly different.\n",
    "\n",
    "# Logistic regression shows which case types were more/less likely to be investigated post-pandemic.\n",
    "# The logistic regression model predicts the likelihood of a case being received post-pandemic based on\n",
    "# the case subtype.\n",
    "# Intercept: 2.9569 (highly significant with p-value < 0.0001)\n",
    "# C(casesubtype)[T.pfa]: -0.7498 (also highly significant with p-value < 0.0001)\n",
    "# Intercept: The positive coefficient (2.9569) indicates that, in the absence of other factors, \n",
    "# the likelihood of a case being received post-pandemic is high.\n",
    "# C(casesubtype)[T.pfa]: The negative coefficient (-0.7498) suggests that cases of subtype pfa are less \n",
    "# likely to be received post-pandemic compared to the baseline subtype.\n",
    "\n",
    "# The chi-square test shows a significant change in case subtype distribution post-pandemic.\n",
    "# The logistic regression indicates that the subtype pfa is less likely to be received post-pandemic,\n",
    "# while the overall likelihood of cases being received post-pandemic is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "2. Analyzing Changes in Case Types (‚Äòcasesubtype‚Äô and ‚Äòcase_type‚Äô)\n",
    "Refinement: NLP & Clustering\n",
    "If there are text-based ‚Äòconcern_type‚Äô descriptions, we can use NLP-based topic modeling to group similar concerns over time.\n",
    "\n",
    "Clustering (K-Means, DBSCAN) can group case subtypes to reveal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "2. Showing Influence of 'casesubtype' and 'case_type' Changes\n",
    "To show how different 'casesubtype' and 'case_type' changes influenced the step reduction in investigation cases, you can use:\n",
    "\n",
    "Techniques:\n",
    "Categorical Analysis: Analyze the frequency and distribution of different case subtypes and types before and after the pandemic.\n",
    "Chi-Square Test: Perform chi-square tests to determine if there are significant differences in the distribution of case subtypes and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Split data into before and after pandemic\n",
    "before_pandemic = linked_df[linked_df['date_received_in_opg'] < '2020-03-01']\n",
    "after_pandemic = linked_df[linked_df['date_received_in_opg'] >= '2020-03-01']\n",
    "\n",
    "# Frequency distribution\n",
    "concern_type_counts_before = before_pandemic['concern_type'].value_counts()\n",
    "concern_type_counts_after = after_pandemic['concern_type'].value_counts()\n",
    "\n",
    "case_type_counts_before = before_pandemic['poa_case_type'].value_counts()\n",
    "case_type_counts_after = after_pandemic['poa_case_type'].value_counts()\n",
    "\n",
    "print(f\"Pre-pandemic: {case_subtype_counts_before}\")\n",
    "print(f\"Pre-pandemic (Financial/Health and Welfare): {round(concern_type_counts_before['Financial']/concern_type_counts_before['Health and Welfare'],2)}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=concern_type_counts_before.index, \n",
    "            y=concern_type_counts_before.values, \n",
    "            color='blue', label='Before Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Concern Type Distribution Before Pandemic')\n",
    "plt.xlabel('Concern Type')\n",
    "plt.ylabel('Volume')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Post-pandemic: {case_subtype_counts_after}\")\n",
    "print(f\"Post-pandemic (Financial/Health and Welfare): {round(concern_type_counts_after['pfa']/concern_type_counts_after['hw'],2)}\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=concern_type_counts_after.index, y=concern_type_counts_after.values, \n",
    "            color='red', label='After Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Concern Type Distribution After Pandemic')\n",
    "plt.xlabel('Concern Type')\n",
    "plt.ylabel('Volume')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=case_type_counts_before.index, y=case_type_counts_before.values, \n",
    "            color='blue', label='Before Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Case Type Distribution Before Pandemic')\n",
    "plt.xlabel('Case Type')\n",
    "plt.ylabel('Volume')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=case_type_counts_after.index, y=case_type_counts_after.values, \n",
    "            color='red', label='After Pandemic')\n",
    "plt.legend()\n",
    "plt.title('Case Type Distribution After Pandemic')\n",
    "plt.xlabel('Case Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Chi-square test\n",
    "contingency_table_subtype = pd.crosstab(linked_df['casesubtype'], \n",
    "                                        linked_df['date_received_in_opg'] >= '2020-03-01')\n",
    "chi2_stat_subtype, p_val_subtype, dof_subtype, ex_subtype = chi2_contingency(contingency_table_subtype)\n",
    "print(f'Chi-square test for case subtype: chi2_stat={chi2_stat_subtype}, p_value={p_val_subtype}')\n",
    "\n",
    "contingency_table_type = pd.crosstab(linked_df['poa_case_type'], \n",
    "                                     linked_df['date_received_in_opg'] >= '2020-03-01')\n",
    "chi2_stat_type, p_val_type, dof_type, ex_type = chi2_contingency(contingency_table_type)\n",
    "print(f'Chi-square test for case type: chi2_stat={chi2_stat_type}, p_value={p_val_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggregate case types by year\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "case_type_counts = linked_df.groupby(['year_received', 'casesubtype']).size().unstack()\n",
    "\n",
    "# Plot trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=case_type_counts, palette=\"tab10\")\n",
    "plt.title(\"Case Type Trends Over Time\")\n",
    "plt.xlabel(\"Year Received\")\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.legend(title=\"Case Subtype\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Chi-square test\n",
    "pre_post_pivot = linked_df.pivot_table(\n",
    "    index='casesubtype', \n",
    "    columns=linked_df['date_received_in_opg'] >= '2020-03-01', \n",
    "    aggfunc='size', fill_value=0)\n",
    "\n",
    "chi2, p, dof, ex = chi2_contingency(pre_post_pivot)\n",
    "print(f\"Chi-Square Statistic: {chi2}, P-Value: {p}\")\n",
    "\n",
    "# Bar plot to show differences before and after pandemic\n",
    "pre_post_counts = linked_df.groupby(['casesubtype', linked_df['date_received_in_opg'] >= '2020-03-01']).size().unstack()\n",
    "pre_post_counts.columns = ['Before Pandemic', 'After Pandemic']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pre_post_counts[['Before Pandemic', 'After Pandemic']].plot(kind='bar', color=['skyblue', 'salmon'], edgecolor='black')\n",
    "plt.title(\"Case Subtype Counts Before and After Pandemic\")\n",
    "plt.xlabel(\"Case Subtype\")\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Period\")\n",
    "plt.grid(True)\n",
    "\n",
    "# # Annotate bars\n",
    "# for idx, row in pre_post_counts.loc[['pfa', 'hw']].iterrows():\n",
    "#     for col, value in row.items():\n",
    "#         plt.text(idx, value + 50, f'{value}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression - Predicting Investigation Likelihood\n",
    "linked_df['post_pandemic'] = (linked_df['date_received_in_opg'] >= '2020-03-01').astype(int)\n",
    "model = smf.logit(\"post_pandemic ~ C(casesubtype)\", data=linked_df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "3. Investigating Gradual vs. Step Decline in Investigation Cases (2016 Onwards)\n",
    "To determine if the decline was gradual (since 2016) or a sharp step drop post-pandemic:\n",
    "\n",
    "Techniques:\n",
    "Time Series Decomposition: Breaking down long-term trends and seasonality.\n",
    "\n",
    "Change Point Detection: Identifying structural breaks in investigation rates.\n",
    "\n",
    "Interrupted Time Series Analysis (ITSA): Evaluating the impact of pandemic on investigation trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ruptures\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Aggregate investigation counts by year\n",
    "investigations_by_year = linked_df.groupby('year_received').size()\n",
    "\n",
    "# Time Series Decomposition\n",
    "decomposed = seasonal_decompose(investigations_by_year, model='additive', period=1)\n",
    "decomposed.plot()\n",
    "plt.show()\n",
    "\n",
    "# Change Point Detection\n",
    "algo = rpt.Pelt(model=\"rbf\").fit(investigations_by_year.values.reshape(-1, 1))\n",
    "breakpoints = algo.predict(pen=10)\n",
    "print(\"Change Points Detected at Years:\", \n",
    "      [list(investigations_by_year.index)[bp] for bp in breakpoints[:-1]])\n",
    "\n",
    "# ITSA - Comparing Pre/Post Pandemic Trends\n",
    "linked_df['post_pandemic'] = (linked_df['year_received'] >= 2020).astype(int)\n",
    "linked_df['casesubtype_numeric'] = linked_df['casesubtype'].astype('category').cat.codes\n",
    "model = smf.ols(\"casesubtype_numeric ~ year_received + post_pandemic\", data=linked_df).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Change point detection helps confirm whether the drop was gradual or sudden.\n",
    "# No change points were detected in the time series data, indicating that there \n",
    "# were no significant shifts or structural breaks in the investigation counts over the years.\n",
    "# ITSA helps quantify the impact of the pandemic on investigation trends.\n",
    "# R-squared (0.010): This indicates that only 1% of the variance in casesubtype_numeric is explained by the model. This is quite low, suggesting that the model does not fit the data well.\n",
    "# Intercept (43.8936): This is the expected value of casesubtype_numeric when year_received is zero and post_pandemic is zero. The high t-value (13.594) and low p-value (< 0.0001) indicate that the intercept is statistically significant.\n",
    "# year_received (-0.0213): This negative coefficient suggests that as the year increases, the casesubtype_numeric value slightly decreases. The high t-value (-13.302) and low p-value (< 0.0001) indicate that this effect is statistically significant.\n",
    "# post_pandemic (0.0253): This positive coefficient indicates that cases received post-pandemic have a slightly higher casesubtype_numeric value. The t-value (2.555) and p-value (0.011) suggest that this effect is statistically significant.\n",
    "# Omnibus (10529.441) and Jarque-Bera (54999.577): These tests indicate that the residuals are not \n",
    "# normally distributed, which might affect the validity of the model.\n",
    "# Durbin-Watson (1.960): This value is close to 2, suggesting that there is no significant autocorrelation\n",
    "# in the residuals.\n",
    "# Condition Number (3.05e+06): A high condition number indicates potential multicollinearity issues, \n",
    "# which means that the independent variables might be highly correlated.\n",
    "# Overall, while the model shows some statistically significant relationships, the low R-squared value \n",
    "# and potential multicollinearity suggest that the model may not be very reliable for predicting \n",
    "# casesubtype_numeric. Further investigation and potentially more complex modeling might be needed \n",
    "# to better understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Findings to Look for:\n",
    "Age Analysis: If older donors were investigated less post-pandemic, it may explain some of the decline.\n",
    "\n",
    "Case Type Shifts: A significant drop in certain cases (e.g., Health & Welfare) might suggest policy changes.\n",
    "\n",
    "Trend Analysis: If Finance & Property cases show a step decline while Health & Welfare cases decline gradually, it supports the operational decision hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "3. Investigating Gradual vs. Step Decline in Investigation Cases (2016 Onwards)\n",
    "Refinement: Structural Breaks & Granger Causality\n",
    "To determine if the step reduction aligns with operational decisions, we can:\n",
    "\n",
    "Detect breakpoints in investigation rates using Bayesian Change Point Detection.\n",
    "\n",
    "Apply Granger Causality to test whether operational changes caused case volume reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A. Bayesian Change Point Detection\n",
    "\n",
    "# import numpy as np\n",
    "# import pymc3 as pm\n",
    "\n",
    "# # Convert investigation counts to numpy array\n",
    "# y = investigations_by_year.values\n",
    "\n",
    "# # Define Model\n",
    "# with pm.Model():\n",
    "#     tau = pm.DiscreteUniform(\"tau\", lower=0, upper=len(y)-1)\n",
    "#     mu1 = pm.Normal(\"mu1\", mu=np.mean(y[:len(y)//2]), sigma=np.std(y))\n",
    "#     mu2 = pm.Normal(\"mu2\", mu=np.mean(y[len(y)//2:]), sigma=np.std(y))\n",
    "    \n",
    "#     # Likelihood\n",
    "#     idx = np.arange(len(y))\n",
    "#     mu = pm.math.switch(tau > idx, mu1, mu2)\n",
    "#     obs = pm.Normal(\"obs\", mu=mu, sigma=np.std(y), observed=y)\n",
    "\n",
    "#     trace = pm.sample(2000, return_inferencedata=True)\n",
    "\n",
    "# # Plot Posterior Distribution of Breakpoint\n",
    "# az.plot_posterior(trace, var_names=[\"tau\"])\n",
    "\n",
    "# # If the posterior distribution of tau (change point) aligns with 2016 or the pandemic, it suggests a structural change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_granger_noinx=df_granger.reset_index()\n",
    "df_granger_noinx#.columns\n",
    "df_granger_noinx.index\n",
    "len(df_granger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pandas as pd\n",
    "\n",
    "# Convert casesubtype to numeric codes\n",
    "linked_df['casesubtype_numeric'] = linked_df['casesubtype'].astype('category').cat.codes\n",
    "\n",
    "# Creating a DataFrame with lagged operational decisions\n",
    "df_granger = linked_df[['year_received', 'casesubtype_numeric']].pivot_table(\n",
    "    index='year_received', columns='casesubtype_numeric', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Add operational decision variable\n",
    "df_granger['triage_removed'] = (df_granger.index >= 2016).astype(int)\n",
    "\n",
    "# Create a rolling mean to make 'triage_removed' dynamic\n",
    "df_granger['triage_removed_smooth'] = df_granger['triage_removed'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Check for constant columns and remove them\n",
    "df_granger = df_granger.loc[:, df_granger.nunique() > 1]\n",
    "print(\"Columns after filtering constant values:\", df_granger.columns)\n",
    "\n",
    "# Ensure 'year_received' is the index and formatted correctly\n",
    "df_granger.index = pd.to_datetime(df_granger.index, format='%Y')\n",
    "\n",
    "# Determine maximum allowable lag\n",
    "max_lag = min(2, len(df_granger) - 1)\n",
    "print(\"Max lag:\", max_lag)\n",
    "\n",
    "# Select variables for Granger causality test\n",
    "target_variable = [col for col in df_granger.columns if col not in ['triage_removed', 'triage_removed_smooth']][0]\n",
    "columns_for_test = [target_variable, 'triage_removed_smooth']\n",
    "print(\"Selected columns for Granger test:\", columns_for_test)\n",
    "\n",
    "# Run Granger causality test\n",
    "grangercausalitytests(df_granger[columns_for_test], maxlag=max_lag, verbose=True)\n",
    "\n",
    "# The Granger causality test checks whether past values of one time series (triage_removed_smooth) help predict another time series (target_variable). \n",
    "# Each test result contains the following:\n",
    "# ssr_ftest (F-test for joint significance): Tests if lagged values significantly improve predictions.\n",
    "# ssr_chi2test (Chi-square test): Checks if adding lagged variables reduces residual error.\n",
    "# lrtest (Likelihood ratio test): Compares model fits with and without the lagged predictor.\n",
    "# params_ftest: Similar to ssr_ftest, but tests parameter significance.\n",
    "# Each metric has:\n",
    "# A statistic value (higher suggests stronger causality).\n",
    "# A p-value (lower means stronger statistical significance).\n",
    "# Degrees of freedom (df) used in calculations.\n",
    "\n",
    "# Lag 1 (1-year lag)\n",
    "# F-test (ssr_ftest): F=17.34, p = 0.0059 (statistically significant)\n",
    "# Chi-square test (ssr_chi2test): ùúí2=26.00, p = 3.41 \\times 10^{-7} (highly significant)\n",
    "# Likelihood ratio test (lrtest): ùúí2=12.22, p = 0.00047 (significant)\n",
    "# ‚úÖ At lag 1, triage_removed_smooth Granger-causes target_variable with high confidence.\n",
    "# p-values < 0.01 suggest strong evidence that removing triage affects investigation rates after 1 year.\n",
    "\n",
    "# Lag 2 (2-year lag)\n",
    "# F-test (ssr_ftest): F=7.62, p = 0.0667 (borderline significance)\n",
    "# Chi-square test (ssr_chi2test): ùúí2=40.64, p = 1.50 \\times 10^{-9} (very significant)\n",
    "# Likelihood ratio test (lrtest): œá 2=14.44, p = 0.00073 (significant)\n",
    "# üî∏ At lag 2, triage_removed_smooth still Granger-causes target_variable, but with slightly weaker confidence.\n",
    "# The F-test p-value (0.0667) is above 0.05, suggesting weaker support, but the Chi-square test remains highly significant.\n",
    "\n",
    "# Triage removal strongly influences investigation counts after 1 year (Lag 1, p < 0.01).\n",
    "# Lag 2 also shows a causal effect, but slightly weaker.\n",
    "# Overall, this supports the hypothesis that removing triage led to a step reduction in investigations.\n",
    "\n",
    "# üîé Next Steps\n",
    "# Consider a Vector Autoregression (VAR) model to quantify dynamic relationships.\n",
    "# Test additional lags (3+ years) to see if effects persist.\n",
    "# Explore causal inference techniques (DAGs, synthetic controls) to validate findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "3. Investigating Trends in Investigation Rates\n",
    "To investigate trends in investigation rates for different case types and subtypes, you can use:\n",
    "\n",
    "Techniques:\n",
    "Time Series Analysis: Analyze the trends over time using line plots and statistical tests.\n",
    "Regression Analysis: Use regression models to identify trends and changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Group by year and case type/subtype\n",
    "yearly_counts = linked_df.groupby(['year_concluded', 'case_type']).size().unstack().fillna(0)\n",
    "yearly_counts_subtype = linked_df.groupby(['year_concluded', 'casesubtype']).size().unstack().fillna(0)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_counts.plot(kind='line', marker='o')\n",
    "plt.title('Yearly Investigation Counts by Case Type')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.legend(title='Case Type')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "yearly_counts_subtype.plot(kind='line', marker='o')\n",
    "plt.title('Yearly Investigation Counts by Case Subtype')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Investigations')\n",
    "plt.legend(title='Case Subtype')\n",
    "plt.show()\n",
    "\n",
    "# Time series decomposition\n",
    "for case_type in yearly_counts.columns:\n",
    "    result = seasonal_decompose(yearly_counts[case_type], model='additive', period=1)\n",
    "    result.plot()\n",
    "    plt.title(f'Time Series Decomposition for {case_type}')\n",
    "    plt.show()\n",
    "\n",
    "# Regression analysis\n",
    "\n",
    "X = yearly_counts.index.values.reshape(-1, 1)\n",
    "for case_type in yearly_counts.columns:\n",
    "    y = yearly_counts[case_type].values\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    print(f'Regression analysis for {case_type}:')\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "Deep Dive into Outliers\n",
    "\n",
    "If we find an anomaly in 2016 or 2020, investigate if it aligns with internal reports or policy changes.\n",
    "\n",
    "Interactive Dashboards\n",
    "\n",
    "Use Plotly or Dash to create an interactive visualization for stakeholders.\n",
    "\n",
    "Predictive Modeling for Future Investigations\n",
    "\n",
    "Use XGBoost or Random Forest to predict the number of investigations based on past trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "A dashboard will help OPG stakeholders visualize the trends, analyze case distributions, and interactively explore insights. I'll create a Plotly Dash application that includes:\n",
    "\n",
    "Age Distribution Analysis ‚Äì A histogram/KDE plot comparing pre- and post-pandemic donor ages.\n",
    "\n",
    "Case Type Trends Over Time ‚Äì A time-series line chart showing case subtypes from 2016 onward.\n",
    "\n",
    "Investigation Volume & Structural Changes ‚Äì A bar chart with annotations marking key events (e.g., triage removal in 2016, pandemic in 2020).\n",
    "\n",
    "Case Type Breakdown (Pie Chart) ‚Äì To compare case distributions over different periods.\n",
    "\n",
    "Changepoint Detection & Forecasting ‚Äì Structural break analysis visualized dynamically.\n",
    "\n",
    "This Plotly Dash app provides an interactive dashboard for OPG stakeholders to explore:\n",
    "\n",
    "Donor Age Distribution before and after the pandemic.\n",
    "\n",
    "Case Type Trends Over Time (line chart).\n",
    "\n",
    "Investigation Volume Changes with key events like triage removal in 2016.\n",
    "\n",
    "Case Type Breakdown (pie chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# !pip install --upgrade dash\n",
    "# !pip install --upgrade plotty\n",
    "\n",
    "# # Uninstall the current version of typing_extensions\n",
    "# !pip uninstall typing-extensions -y\n",
    "\n",
    "# # Install the latest version of typing_extensions\n",
    "# !pip install typing-extensions --upgrade\n",
    "\n",
    "# !pip install jupyter-dash --upgrade\n",
    "\n",
    "# !pip install ipywidgets\n",
    "\n",
    "# !jupyter nbextension list\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# !jupyter labextension install plotlywidget\n",
    "# !jupyter labextension install jupyterlab-dash\n",
    "# !jupyter nbextension install --sys-prefix --py jupyter_dash\n",
    "# !jupyter nbextension enable --sys-prefix --py jupyter_dash\n",
    "\n",
    "!jupyter nbextension install --py jupyter_dash --sys-prefix\n",
    "!jupyter nbextension enable --py jupyter_dash --sys-prefix\n",
    "\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure linked_df exists\n",
    "if 'linked_df' not in locals():\n",
    "    raise ValueError(\"linked_df is not defined. Load the dataset before running the dashboard.\")\n",
    "\n",
    "# Convert to datetime\n",
    "linked_df['date_received_in_opg'] = pd.to_datetime(linked_df['date_received_in_opg'])\n",
    "linked_df['client_donor_dob'] = pd.to_datetime(linked_df['client_donor_dob'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Extract year\n",
    "linked_df['year_received'] = linked_df['date_received_in_opg'].dt.year\n",
    "\n",
    "# Calculate donor age at investigation\n",
    "linked_df['donor_age_at_investigation'] = (linked_df['date_received_in_opg'] - linked_df['client_donor_dob']).dt.days / 365.25\n",
    "\n",
    "# Aggregate data\n",
    "case_type_trend = linked_df.groupby(['year_received', 'casesubtype']).size().reset_index(name='count')\n",
    "\n",
    "# Create Dash App\n",
    "app = JupyterDash(__name__)  # Use JupyterDash for Jupyter Lab compatibility\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"OPG Investigation Dashboard\"),\n",
    "    \n",
    "    # Age Distribution Comparison\n",
    "    dcc.Graph(\n",
    "        figure=px.histogram(\n",
    "            linked_df, \n",
    "            x='donor_age_at_investigation', \n",
    "            color=linked_df['year_received'].apply(lambda x: 'Post-2020' if x >= 2020 else 'Pre-2020'),\n",
    "            nbins=50, \n",
    "            title=\"Donor Age Distribution: Pre vs. Post Pandemic\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Case Type Trends\n",
    "    dcc.Graph(\n",
    "        figure=px.line(\n",
    "            case_type_trend, \n",
    "            x='year_received', \n",
    "            y='count', \n",
    "            color='casesubtype',\n",
    "            title=\"Trends in Case Types Over Time\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Investigation Volume with Key Events\n",
    "    dcc.Graph(\n",
    "        figure=px.bar(\n",
    "            linked_df.groupby('year_received').size().reset_index(name='count'), \n",
    "            x='year_received', \n",
    "            y='count',\n",
    "            title=\"Annual Investigation Volume\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Case Type Breakdown (Pie Chart)\n",
    "    dcc.Graph(\n",
    "        figure=px.pie(\n",
    "            linked_df, \n",
    "            names='casesubtype', \n",
    "            title=\"Case Type Distribution\"\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "# Run app inside Jupyter Lab\n",
    "#app.run(mode='inline')  # Try mode='external' if still not displaying\n",
    "app.run(mode='inline') #, port=8051) # Try different ports (8052, 8053, etc.).\n",
    "#app.run(mode='external')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "additional features, such as changepoint detection visualizations or predictive modeling? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Hello Dash!\"),\n",
    "    dcc.Graph()\n",
    "])\n",
    "\n",
    "app.run(mode='inline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "1. Investigating Age Distribution Changes Pre- and Post-Pandemic\n",
    "Refinement: Bayesian Analysis & Causal Inference\n",
    "Rather than just comparing distributions, we can:\n",
    "\n",
    "Use a Bayesian model to estimate how much the mean donor age changed.\n",
    "\n",
    "Apply Causal Impact Analysis (Google‚Äôs CausalImpact package) to check if the change was due to the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A. Topic Modeling for Case Subtypes\n",
    "\n",
    "# Upgrade scikit-learn\n",
    "!pip install --upgrade scikit-learn\n",
    "#import sklearn\n",
    "\n",
    "\n",
    "# Upgrade numpy\n",
    "!pip install --upgrade numpy\n",
    "\n",
    "# Converts text data into a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# Applies Latent Dirichlet Allocation (LDA) for topic modeling.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# TF-IDF Vectorizer: This transforms the text data (casesubtype) into a matrix of TF-IDF features. \n",
    "# TF-IDF stands for Term Frequency-Inverse Document Frequency, which helps in highlighting important words in the documents.\n",
    "# Stop Words: Common words like \"the\", \"and\", etc., are removed to focus on more meaningful words.\n",
    "# Convert case types into TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(linked_df['concern_type'].astype(str))\n",
    "\n",
    "# Creates an LDA model with 5 topics (n_components=5). LDA is a generative probabilistic \n",
    "# model that assumes each document is a mixture of topics and each topic is a mixture of words.\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "# The model is fitted to the TF-IDF matrix and transforms it into topic distributions.\n",
    "topics = lda.fit_transform(X)\n",
    "\n",
    "# For each topic, the top words are displayed. topic.argsort()[-5:] \n",
    "# sorts the words by their importance in the topic and selects the top 5.\n",
    "# Display top words in each topic\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i}: {[vectorizer.get_feature_names_out()[j] for j in topic.argsort()[-5:]]}\")\n",
    "\n",
    "# This topic modeling helps identify patterns in case subtypes that may have changed pre/post-pandemic. \n",
    "# By understanding these patterns, you can gain insights into how different case types have evolved over time.\n",
    "# Helps identify case subtype patterns that may have changed pre/post-pandemic.\n",
    "# The results show the top words for each topic. However, the presence of 'nan' suggests that \n",
    "# there might be missing or improperly formatted data in the casesubtype column. \n",
    "# clean the data to remove or handle 'nan' values appropriately.\n",
    "\n",
    "# B. Clustering Subtypes Over Time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert categorical case subtypes into numeric representations\n",
    "linked_df['concern_type_encoded'] = linked_df['concern_type'].astype('category').cat.codes\n",
    "\n",
    "print(linked_df['concern_type_encoded'])\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "linked_df['cluster'] = kmeans.fit_predict(\n",
    "    linked_df[['concern_type_encoded', 'year_received']])\n",
    "\n",
    "# Visualize Changes in Clusters Over Time\n",
    "sns.lineplot(data=linked_df, x='year_received', y='cluster', hue='concern_type', marker='o')\n",
    "plt.title('Concern Type Investigation Clustering Over Time')\n",
    "plt.show()\n",
    "\n",
    "# If certain clusters disappear or emerge post-pandemic, they could explain the step reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Suppress warnings from statsmodels\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# !pip install --upgrade pymc3\n",
    "# #!python -m pip install --upgrade pip\n",
    "# #!pip uninstall pymc3\n",
    "# #!pip install git+https://github.com/pymc-devs/pymc3\n",
    "# # !git clone https://github.com/pymc-devs/pymc3\n",
    "# # !cd pymc3\n",
    "# # !pip install -r requirements.txt\n",
    "# # !python setup.py install\n",
    "# # !python setup.py develop\n",
    "# # !python -m pip install --upgrade pip\n",
    "# #!pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose\n",
    "# import pymc3 as pm\n",
    "# import arviz as az\n",
    "# from scipy.stats import kstest\n",
    "# print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # A. Bayesian Estimation of Age Differences\n",
    "# #!pip install pymc3\n",
    "\n",
    "# # Define Bayesian Model\n",
    "# with pm.Model():\n",
    "#     mu_pre = pm.Normal(\"mu_pre\", mu=70, sigma=10)  # Prior for Pre-Pandemic Age Mean\n",
    "#     mu_post = pm.Normal(\"mu_post\", mu=70, sigma=10)  # Prior for Post-Pandemic Age Mean\n",
    "#     sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "\n",
    "#     # Likelihood\n",
    "#     age_pre = pm.Normal(\"age_pre\", mu=mu_pre, sigma=sigma, observed=pre_pandemic['donor_age_at_investigation'])\n",
    "#     age_post = pm.Normal(\"age_post\", mu=mu_post, sigma=sigma, observed=post_pandemic['donor_age_at_investigation'])\n",
    "\n",
    "#     # Difference\n",
    "#     diff = pm.Deterministic(\"diff\", mu_post - mu_pre)\n",
    "\n",
    "#     trace = pm.sample(2000, return_inferencedata=True)\n",
    "\n",
    "# # Plot Posterior Distribution of Age Difference\n",
    "# import arviz as az\n",
    "# az.plot_posterior(trace, var_names=[\"diff\"])\n",
    "\n",
    "# # This approach gives a probability distribution of the change in age, rather than a simple p-value.\n",
    "\n",
    "# # If most of the posterior distribution is negative, it means donor age at investigation decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install causalimpact\n",
    "# from causalimpact import CausalImpact\n",
    "# import pandas as pd\n",
    "\n",
    "# # Ensure proper datetime conversion\n",
    "# linked_df['year_received'] = pd.to_datetime(linked_df['date_received_in_opg']).dt.year\n",
    "\n",
    "# # Compute mean donor age per year\n",
    "# ts_data = linked_df.groupby('year_received')['donor_age_at_investigation'].mean()\n",
    "\n",
    "# # Ensure DataFrame format\n",
    "# ts_data = ts_data.to_frame(name=\"value\")\n",
    "\n",
    "# # Convert index to datetime format\n",
    "# ts_data.index = pd.to_datetime(ts_data.index, format='%Y')\n",
    "\n",
    "# # Drop missing values\n",
    "# ts_data.dropna(inplace=True)\n",
    "\n",
    "# # Define pre/post intervention periods based on actual years in index\n",
    "# pre_period = [ts_data.index.min().year, 2019]\n",
    "# post_period = [2020, ts_data.index.max().year]\n",
    "\n",
    "# print(pre_period)\n",
    "# print(post_period)\n",
    "# print(len(ts_data))\n",
    "# # Ensure there are enough data points\n",
    "# if len(ts_data) < 12:\n",
    "#     print(\"üö® Not enough data points for CausalImpact. Try alternative methods.\")\n",
    "# else:\n",
    "#     # Run CausalImpact\n",
    "#     impact = CausalImpact(data=ts_data, pre_period=pre_period, post_period=post_period)\n",
    "\n",
    "#     # Check if the model ran successfully\n",
    "#     if impact.inferences is not None:\n",
    "#         impact.plot()\n",
    "#         print(impact.summary())\n",
    "#         print(impact.summary(output='report'))\n",
    "#     else:\n",
    "#         print(\"üö® CausalImpact failed. Check data formatting.\")\n",
    "\n",
    "# # If you only have 6 data points, CausalImpact won‚Äôt work reliably because:\n",
    "\n",
    "# # Bayesian structural time series (BSTS) models need at least 12+ observations.\n",
    "\n",
    "# # With just 6 years, there's not enough variance to estimate a meaningful impact.\n",
    "\n",
    "# from causalimpact import CausalImpact\n",
    "# import pandas as pd\n",
    "\n",
    "# # Ensure proper datetime conversion\n",
    "# linked_df['date_received'] = pd.to_datetime(linked_df['date_received_in_opg'])#.dt.strftime('%Y-%m').astype(str)\n",
    "\n",
    "# # Compute mean donor age per month\n",
    "# ts_data = linked_df.groupby(linked_df['date_received'].dt.to_period('M'))['donor_age_at_investigation'].mean()\n",
    "# # ts_data = linked_df.groupby(linked_df['date_received'])['donor_age_at_investigation'].mean()\n",
    "\n",
    "# # Ensure DataFrame format\n",
    "# ts_data = ts_data.to_frame(name=\"value\")\n",
    "\n",
    "# # Convert index to datetime format\n",
    "# ts_data.index = ts_data.index.to_timestamp()\n",
    "\n",
    "# # Drop missing values\n",
    "# ts_data.dropna(inplace=True)\n",
    "\n",
    "# #linked_df['date_received'].loc[1:1][1]\n",
    "\n",
    "# # Define pre/post intervention periods based on actual months in index\n",
    "# # pre_period = [ts_data.index.min().strftime('%Y-%m'), '2019-12']\n",
    "# # post_period = ['2020-01', ts_data.index.max().strftime('%Y-%m')]\n",
    "# # pre_period = [ts_data.index.min(), '2019-12']\n",
    "# # post_period = ['2020-01', ts_data.index.max()]\n",
    "# pre_period = [ts_data.index.min(), pd.Timestamp('2019-12-01')]\n",
    "# post_period = [pd.Timestamp('2020-01-01'), ts_data.index.max()]\n",
    "\n",
    "# print(pre_period)\n",
    "# print(post_period)\n",
    "# print(len(ts_data))\n",
    "# print(ts_data.head(10))\n",
    "# data = ts_data.reset_index()\n",
    "\n",
    "# # Ensure there are enough data points\n",
    "# if len(ts_data) < 12:\n",
    "#     print(\"üö® Not enough data points for CausalImpact. Try alternative methods.\")\n",
    "# else:\n",
    "#     # Run CausalImpact\n",
    "#     impact = CausalImpact(data=data, pre_period=pre_period, post_period=post_period)\n",
    "\n",
    "#     # Check if the model ran successfully\n",
    "#     if impact.inferences is not None:\n",
    "#         impact.plot()\n",
    "#         print(impact.summary())\n",
    "#         print(impact.summary(output='report'))\n",
    "#     else:\n",
    "#         print(\"üö® CausalImpact failed. Check data formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# First, uninstall the current versions\n",
    "!pip uninstall numpy statsmodels -y\n",
    "\n",
    "# Then, install compatible versions\n",
    "!pip install numpy==1.26.4 statsmodels\n",
    "#!pip install --upgrade numpy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Ensure proper datetime conversion\n",
    "linked_df['year_received'] = pd.to_datetime(linked_df['date_received_in_opg']).dt.year\n",
    "\n",
    "# Compute mean donor age per year\n",
    "ts_data = linked_df.groupby('year_received')['donor_age_at_investigation'].mean()\n",
    "\n",
    "# Create a dummy variable: 1 if post-2020, 0 if before\n",
    "linked_df['post_policy'] = (linked_df['year_received'] >= 2020).astype(int)\n",
    "\n",
    "# Dummy variable for treatment (e.g., Health & Welfare cases vs. Finance cases)\n",
    "linked_df['treated'] = (linked_df['casesubtype'] == 'hw').astype(int)\n",
    "\n",
    "# Interaction term (DiD effect)\n",
    "linked_df['post_treated'] = linked_df['post_policy'] * linked_df['treated']\n",
    "\n",
    "# Run a DiD regression\n",
    "model = smf.ols(\"donor_age_at_investigation ~ post_policy + treated + post_treated\", data=linked_df).fit()\n",
    "print(model.summary())\n",
    "\n",
    " # If the post_treated coefficient is statistically significant, \n",
    " #    it suggests that the intervention (policy change) affected investigations.\n",
    "# Dependent Variable: donor_age_at_investigation\n",
    "# R-squared: 0.006\n",
    "# This indicates that the model explains only 0.6% of the variance in the dependent variable. \n",
    "# Essentially, the model has very low explanatory power.\n",
    "# Adjusted R-squared: 0.006\n",
    "# Similar to R-squared, it adjusts for the number of predictors in the model. Here, it also indicates very low explanatory power.\n",
    "# F-statistic: 39.50\n",
    "# This tests the overall significance of the model. A higher F-statistic suggests that the model is statistically significant.\n",
    "# Prob (F-statistic): 1.96e-25\n",
    "# The p-value associated with the F-statistic. Since it is much less than 0.05, the model is statistically significant.\n",
    "# Coefficients:\n",
    "# Intercept: 81.6097\n",
    "# This is the average donor age at investigation when all other variables are zero.\n",
    "# post_policy: 0.3935\n",
    "# This coefficient represents the change in donor age at investigation post-2020. The p-value (0.239) indicates that this\n",
    "# change is not statistically significant.\n",
    "# treated: -4.7951\n",
    "# This coefficient represents the difference in donor age at investigation between Health & Welfare cases and Finance cases. \n",
    "# The p-value (0.000) indicates that this difference is statistically significant.\n",
    "# post_treated: 1.6825\n",
    "# This interaction term represents the combined effect of post-2020 and treatment type. The p-value (0.182) indicates that this interaction effect is not statistically significant.\n",
    "# Diagnostic Tests:\n",
    "# Omnibus: 8707.441\n",
    "# This tests for the normality of residuals. A high value indicates non-normality.\n",
    "# Durbin-Watson: 2.006\n",
    "# This tests for autocorrelation in residuals. A value close to 2 suggests no autocorrelation.\n",
    "# Jarque-Bera (JB): 51834.606\n",
    "# This also tests for normality. A high value indicates non-normality.\n",
    "# Skew: -2.133\n",
    "# This indicates the distribution of residuals is left-skewed.\n",
    "# Kurtosis: 9.839\n",
    "# This indicates the distribution of residuals has heavy tails (leptokurtic).\n",
    "# Conclusion:\n",
    "# The model is statistically significant overall, but it explains very little of the variance in donor age at investigation. \n",
    "# The coefficient for treated is significant, suggesting that Health & Welfare cases have a lower average donor age at \n",
    "# investigation compared to Finance cases. However, the coefficients for post_policy and post_treated are not significant, \n",
    "# indicating no substantial change in donor age at investigation post-2020 or due to the interaction effect.\n",
    "# The diagnostics suggest non-normality in residuals, which might affect the reliability of the model. \n",
    "#You might want to consider additional variables or different model specifications to improve explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pydbtools\n",
    "# !pip install awswrangler\n",
    "# !pip install pandas --upgrade\n",
    "# !pip install time\n",
    "# !pip install numpy --upgrade\n",
    "# !pip install jinja2\n",
    "# !pip install lovely_logger \n",
    "\n",
    "# import pydbtools\n",
    "# import awswrangler as wr\n",
    "# import model_stages.stage1_get_invest_data as stage1\n",
    "# import model_stages.stage2_prepare_invests_for_merging as stage2\n",
    "# import model_stages.stage3_merge_with_POA_data as stage3\n",
    "\n",
    "\n",
    "# ## Config\n",
    "\n",
    "# date_cols = ['date_received_in_investigations', 'date_allocated_to_team',\n",
    "#                   'date_allocated_to_current_investigator', 'pg_sign_off_date',\n",
    "#                   'closure_date',\n",
    "#                   'date_received_in_opg', 'legal_approval_date_clean',\n",
    "#                  'receiptdate', 'registrationdate']\n",
    "\n",
    "# cols_to_keep = ['unique_id', \n",
    "#                 # 'client_donor_title',\n",
    "#                 # 'client_donor_forename', 'client_donor_surname', 'client_donor_dob',\n",
    "#                 'case_type', 'concern_type', 'status', 'sub_status',\n",
    "#                 'date_received_in_opg', 'multiple_id', 'lead_case',\n",
    "#                 'days_to_pg_sign_off', 'closure_date', 'case_uid', 'receiptdate', 'registrationdate',\n",
    "#                 'case_status', 'poa_case_type', 'casesubtype', 'poas_involved',\n",
    "#                 'order_involved', 'legal_approval_date_clean', 'invest_concluded',\n",
    "#                 'year_concluded', 'case_type_agg']\n",
    "# first_date = '01/01/2019'\n",
    "# last_date = '31/12/2024'\n",
    "\n",
    "# min_LPA_receiptdate = '2017-01-01'\n",
    "# max_LPA_receiptdate = '2024-12-31'\n",
    "# sample_size = '1000000'\n",
    "\n",
    "\n",
    "# ## Read/Link Data\n",
    "\n",
    "# # Read Investigations Data\n",
    "# df_inv_data = stage1.main(date_cols, first_date, last_date, cols_to_keep)\n",
    "\n",
    "# # Expand Investigations Data\n",
    "# expanded_df = stage2.main(df_inv_data)\n",
    "\n",
    "# # Link to LPA Data\n",
    "# linked_df = stage3.main(expanded_df, \n",
    "#                         min_LPA_receiptdate, \n",
    "#                         max_LPA_receiptdate, \n",
    "#                         sample_size)\n",
    "\n",
    "# # print(linked_df)\n",
    "\n",
    "# linked_Investigation_LPA_data = linked_df\n",
    "# linked_Investigation_LPA_data.to_csv('linked_Investigation_LPA_data.csv', index=False)\n",
    "\n",
    "# linked_df\n",
    "\n",
    "# inv = linked_df.loc[linked_df['investigator'].notnull()]\n",
    "# inv.to_csv('linked_Investigation_LPA_inv.csv', index=False)\n",
    "\n",
    "# inv_linked_lpa = linked_df[['uid','donor_id','lpa_reg_date','lpa_status','lpa_rec_date','poa_type','unique_id','case_no','client_donor_dob','case_type','concern_type','date_received_in_opg','status','mojap_extract_date','poa_case_type','casesubtype','poa_rec_to_invest_rec','year_concluded','link_id','uid_to_link']]\n",
    "# inv_linked_lpa\n",
    "# inv_linked_lpa_data = inv_linked_lpa\n",
    "# inv_linked_lpa_data.to_csv('inv_linked_lpa_data.csv', index=False)\n",
    "\n",
    "# pydbtools.read_sql_query(\"SELECT DISTINCT(mojap_extract_date) FROM opg_investigations_prod.investigations ORDER BY mojap_extract_date DESC\")\n",
    "# lpa_dashboard = pydbtools.read_sql_query(\"SELECT * FROM sirius_derived.opg_lpa_dashboard LIMIT 5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
