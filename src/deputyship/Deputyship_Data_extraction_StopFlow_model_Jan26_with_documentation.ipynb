{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Deputyship data extraction and stop flow forecast notebook\n",
    "\n",
    "This notebook pulls monthly snapshots of deputyship cases from the Sirius database, turns those snapshots into a set of flow measures, then produces a forecast of future active caseload by age group. The forecast method used here is a stop flow recurrence, which updates the number of active cases using entries and terminations observed one year earlier.\n",
    "\n",
    "The notebook is written so a new analyst can run it from top to bottom. The first code cell installs a small set of dependencies into an isolated location so the rest of the notebook can import them without changing the shared environment. The later run cells write outputs into the output folder, including raw monthly extracts, summary tables, a combined historical and forecast table, charts, and an insights markdown file.\n",
    "\n",
    "Before you run the extraction steps you need access to the opg_sirius_prod database through pydbtools. If you do not have that access, the notebook will fail at the first query and you will need to request the relevant permissions.\n",
    "\n",
    "A practical note about outputs: the run sections call a helper that deletes all files inside the output folder before writing new results. If you want to keep earlier runs, copy the folder elsewhere first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Environment and dependency set up\n",
    "\n",
    "This cell installs the extra Python libraries that the rest of the notebook relies on. It does it in a way that avoids breaking the shared Python environment.\n",
    "\n",
    "The approach is to create a fresh folder on disk and tell pip to install packages into that folder. The folder is then added to Python import search path so imports work immediately.\n",
    "\n",
    "This is useful on managed notebook platforms because a normal install can try to upgrade or remove packages that other projects depend on. When that happens you can get errors about locked files or dependency conflicts. By isolating installs into a new folder, we avoid touching the base environment.\n",
    "\n",
    "The helper functions in this cell do three things.\n",
    "\n",
    "First they choose a base directory for installs. You can override it with the NB_PIP_TARGET environment variable. If you do not set that variable the code creates a unique folder under tmp using the current time.\n",
    "\n",
    "Second they activate a chosen install folder by inserting it at the front of sys.path. Python imports from the first matching entry on sys.path, so putting the new folder first means the newly installed packages are the ones that get imported.\n",
    "\n",
    "Third they run pip in a mode that installs only the named packages into the target folder and avoids pulling in and upgrading large shared dependencies. This reduces the chance of conflicts with core libraries such as numpy and botocore.\n",
    "\n",
    "At the end of the cell, the notebook imports each installed package and prints a version check so you can see that the environment is ready for the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fresh, isolated installs to avoid \"device or resource busy\" + avoid dependency conflicts ---\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def _new_target_base():\n",
    "    # prefer env override; else unique temp dir\n",
    "    base = os.environ.get(\"NB_PIP_TARGET\")\n",
    "    if not base:\n",
    "        base = f\"/tmp/pythonlibs_{int(time.time())}\"\n",
    "    os.makedirs(base, exist_ok=True)\n",
    "    return base\n",
    "\n",
    "_ISO_BASE = _new_target_base()      # e.g., /tmp/pythonlibs_1737500000\n",
    "_SYS_PATH_SEEN = set(sys.path)\n",
    "\n",
    "def activate_site(path: str):\n",
    "    \"\"\"Put a site-packages dir at the very front of sys.path (once).\"\"\"\n",
    "    if path in _SYS_PATH_SEEN:\n",
    "        return path\n",
    "    sys.path.insert(0, path)\n",
    "    _SYS_PATH_SEEN.add(path)\n",
    "    return path\n",
    "\n",
    "def pip_install_isolated(*pkgs, no_deps: bool = False, target: str | None = None):\n",
    "    \"\"\"\n",
    "    Install pkgs into a brand-new site dir so nothing gets removed/overwritten.\n",
    "    Use no_deps=True to avoid pulling conflicting core deps (botocore/wrapt/numpy).\n",
    "    Returns the target path.\n",
    "    \"\"\"\n",
    "    target = target or os.path.join(_ISO_BASE, f\"site_{len(_SYS_PATH_SEEN)}\")\n",
    "    os.makedirs(target, exist_ok=True)\n",
    "    activate_site(target)\n",
    "\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\",\n",
    "           \"--upgrade\", \"--no-cache-dir\", \"--disable-pip-version-check\",\n",
    "           \"--target\", target]\n",
    "    if no_deps:\n",
    "        cmd.append(\"--no-deps\")\n",
    "    cmd.extend(pkgs)\n",
    "    print(\">\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "    importlib.invalidate_caches()\n",
    "    print(\"Installed into:\", target)\n",
    "    return target\n",
    "\n",
    "print(\"Isolated install base:\", _ISO_BASE)\n",
    "\n",
    "# Example: install pydbtools without dependencies (use base env for boto/numpy/etc.)\n",
    "\n",
    "pip_install_isolated(\"arrow_pd_parser\", no_deps=True)\n",
    "pip_install_isolated(\"mojap_metadata\", no_deps=True)\n",
    "pip_install_isolated(\"sql_metadata\", no_deps=True)\n",
    "pip_install_isolated(\"sqlparse\", no_deps=True)\n",
    "pip_install_isolated(\"pydbtools\", no_deps=True)\n",
    "pip_install_isolated(\"awswrangler\", no_deps=True)\n",
    "pip_install_isolated(\"dataengineeringutils3\", no_deps=True)\n",
    "\n",
    "# Sanity check\n",
    "import sql_metadata\n",
    "print(\"sql_metadata:\", getattr(sql_metadata, \"__version__\", \"unknown\"))\n",
    "\n",
    "import sqlparse\n",
    "print(\"sqlparse:\", getattr(sqlparse, \"__version__\", \"unknown\"))\n",
    "\n",
    "import pydbtools\n",
    "print(\"pydbtools:\", getattr(pydbtools, \"__version__\", \"unknown\"))\n",
    "\n",
    "import arrow_pd_parser\n",
    "print(\"arrow_pd_parser:\", getattr(arrow_pd_parser, \"__version__\", \"unknown\"))\n",
    "\n",
    "import mojap_metadata\n",
    "print(\"mojap_metadata:\", getattr(mojap_metadata, \"__version__\", \"unknown\"))\n",
    "\n",
    "import awswrangler\n",
    "print(\"awswrangler:\", getattr(awswrangler, \"__version__\", \"unknown\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Core imports and small utilities used throughout the notebook\n",
    "\n",
    "This cell loads the Python libraries used for data access, data manipulation, forecasting, and plotting. Most of the work later in the notebook uses pandas for tabular data, numpy for numeric work, matplotlib for charts, and pydbtools for running SQL and returning results as a DataFrame.\n",
    "\n",
    "It also sets notebook level behaviour.\n",
    "\n",
    "Warnings from statsmodels about divide by zero in information criteria are suppressed because they can appear when a time series is flat and they distract from the main output. Logging is disabled so that the notebook prints only the outputs we care about when running end to end.\n",
    "\n",
    "After imports, the cell defines a few helper functions that keep the rest of the notebook readable.\n",
    "\n",
    "parse_month converts a month string into a datetime object representing the first day of that month. It is used so that functions receive consistent date inputs.\n",
    "\n",
    "clear_directory removes everything inside a given folder. It checks that the path is a directory first. This is used to ensure the output folder contains only the results from the current run.\n",
    "\n",
    "generate_month_list creates a list of monthly datetime points from a start month to an end month. It uses relativedelta to move forward one month at a time. This gives a simple and explicit loop over reporting months.\n",
    "\n",
    "last_day_of_month converts a datetime to the last calendar day of its month, formatted as a string. The data extraction queries work on month end snapshots, so we use the last day as the snapshot date.\n",
    "\n",
    "There is also a small placeholder definition of fetch_cases_for_date. It illustrates the intended pattern of running a SQL query through pydbtools. A full version of that function is defined later and will replace this placeholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydbtools\n",
    "import calendar\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# Configure logging\n",
    "import warnings\n",
    "# Suppress statsmodels AIC/BIC divide-by-zero runtime warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*divide by zero encountered in log.*\")\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Disable logging output\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "def parse_month(month_str: str) -> datetime:\n",
    "    \"\"\"Parse 'YYYY-MM' to datetime.\"\"\"\n",
    "    result = datetime.strptime(month_str.strip().strip(\"'\\\"\"), \"%Y-%m\")\n",
    "    logging.debug(f\"parse_month: parsed '{month_str}' to {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# def clear_directory(path):\n",
    "#     logging.info(f\"clear_directory: clearing path {path}\")\n",
    "#     for filename in os.listdir(path):\n",
    "#         file_path = os.path.join(path, filename)\n",
    "#         try:\n",
    "#             if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "#                 os.unlink(file_path)\n",
    "#                 logging.debug(f\"Deleted file {file_path}\")\n",
    "#             elif os.path.isdir(file_path):\n",
    "#                 shutil.rmtree(file_path)\n",
    "#                 logging.debug(f\"Deleted directory {file_path}\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "def clear_directory(path: str):\n",
    "    \"\"\"Safely delete all files in `path` if it’s a directory.\"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        logging.debug(f\"clear_directory: '{path}' is not a directory, skipping.\")\n",
    "        return\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "def fetch_cases_for_date(run_date: str) -> pd.DataFrame:\n",
    "    logging.info(f\"fetch_cases_for_date: fetching data for {run_date}\")\n",
    "    query = \"...\"  # trimmed for brevity\n",
    "    df = pydbtools.read_sql_query(query)\n",
    "    logging.info(f\"fetch_cases_for_date: returned {len(df)} rows for {run_date}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_month_list(start_month: str, end_month: str):\n",
    "    logging.info(f\"generate_month_list: from {start_month} to {end_month}\")\n",
    "    start_dt = parse_month(start_month)\n",
    "    end_dt = parse_month(end_month)\n",
    "    if start_dt > end_dt:\n",
    "        raise ValueError(f\"Start month ({start_month}) is after end month ({end_month})\")\n",
    "    months = []\n",
    "    current = start_dt\n",
    "    while current <= end_dt:\n",
    "        months.append(current)\n",
    "        logging.debug(f\"Added month {current}\")\n",
    "        current += relativedelta(months=1)\n",
    "    logging.info(f\"Generated {len(months)} months\")\n",
    "    return months\n",
    "\n",
    "\n",
    "def last_day_of_month(dt: datetime) -> str:\n",
    "    day = calendar.monthrange(dt.year, dt.month)[1]\n",
    "    result = dt.replace(day=day).strftime(\"%Y-%m-%d\")\n",
    "    logging.debug(f\"last_day_of_month: for {dt} result {result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data extraction from Sirius and export of monthly snapshots\n",
    "\n",
    "This cell defines the functions that actually pull deputyship case data from the database and write it to disk as month end snapshots.\n",
    "\n",
    "The central function is fetch_cases_for_date. It takes a snapshot date as a string in year month day format and runs a SQL query through pydbtools. The query is designed to be a point in time extract, meaning every table reference is filtered to the same glueexporteddate. That is important because Sirius data in this environment is stored as daily exports. Using the same export date everywhere makes the extract internally consistent.\n",
    "\n",
    "Inside the query, the common table expression active_fee_reductions finds fee reduction records that are active on the snapshot date. It does two filters that matter.\n",
    "\n",
    "It keeps only fee reductions whose start date is on or before the snapshot date and whose end date is on or after the snapshot date. That gives the reductions that apply on that day.\n",
    "\n",
    "It also chooses the latest record per finance client by taking the maximum id within the active window. That avoids duplicate fee reduction rows if there have been updates.\n",
    "\n",
    "The main SELECT then joins persons to cases for the snapshot date. It pulls fields that are used later for grouping and for age based modelling, including case number, supervision level, risk score, order subtype, order status, and date of birth.\n",
    "\n",
    "Age is calculated as an approximate integer number of years. The logic is:\n",
    "\n",
    "age in years equals the number of days between date of birth and created date, divided by 365.25, then rounded to the nearest integer. The 365.25 factor accounts for leap years on average. If the computed value is negative, it is set to zero.\n",
    "\n",
    "The cell also defines export_monthly_reports. This function takes a first month and a last month, builds the list of months in between, then for each month:\n",
    "\n",
    "1. It converts the month to its last calendar day and uses that as the snapshot date.\n",
    "2. It calls fetch_cases_for_date to pull the data for that snapshot.\n",
    "3. It adds a month tag column so we can later combine months into one table.\n",
    "4. It writes a csv file into a month specific folder under the output directory.\n",
    "5. It writes the same data into an Excel workbook as one sheet per month.\n",
    "\n",
    "After looping over all months, it concatenates the monthly DataFrames into one combined table and writes a combined csv for the full date range. It also produces a small summary table that counts total rows as a proxy for orders, and unique case numbers as a proxy for unique clients, grouped by year plus an overall row for the whole period.\n",
    "\n",
    "These exports give you an auditable trail: you can inspect individual month extracts, and you can also work with the combined table for modelling steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_cases_for_date(run_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch all cases & their fee reductions for the given run_date (YYYY-MM-DD)\n",
    "    using pydbtools.read_sql_query, which returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    WITH active_fee_reductions AS (\n",
    "      SELECT\n",
    "        fc.client_id,\n",
    "        SUBSTRING(fr.type,1,1) || LOWER(SUBSTRING(fr.type,2)) AS type,\n",
    "        DATE(fr.startdate) AS startdate,\n",
    "        DATE(fr.enddate)   AS enddate,\n",
    "        fc.payment_method\n",
    "      FROM opg_sirius_prod.fee_reduction fr\n",
    "      JOIN opg_sirius_prod.finance_client fc\n",
    "        ON fc.id = fr.finance_client_id\n",
    "       AND fc.glueexporteddate = DATE('{run_date}')\n",
    "      JOIN (\n",
    "        SELECT\n",
    "          MAX(id)           AS id,\n",
    "          finance_client_id\n",
    "        FROM opg_sirius_prod.fee_reduction\n",
    "        WHERE enddate           >= DATE('{run_date}')\n",
    "          AND startdate         <= DATE('{run_date}')\n",
    "          AND deleted            = FALSE\n",
    "          AND glueexporteddate   = DATE('{run_date}')\n",
    "        GROUP BY finance_client_id\n",
    "      ) latest ON latest.id = fr.id\n",
    "      WHERE fr.glueexporteddate = DATE('{run_date}')\n",
    "    )\n",
    "    SELECT\n",
    "      c.glueexporteddate,\n",
    "      c.caserecnumber            AS casenumber,\n",
    "      c.uid                      AS siriusid,\n",
    "      (\n",
    "        SELECT supervisionlevel\n",
    "        FROM opg_sirius_prod.supervision_level_log sll\n",
    "        WHERE sll.order_id         = c.id\n",
    "          AND sll.glueexporteddate = DATE('{run_date}')\n",
    "        ORDER BY sll.appliesfrom DESC\n",
    "        LIMIT 1\n",
    "      ) AS casesupervisionlevel,\n",
    "      p.risk_score               AS CREC,\n",
    "      c.casesubtype              AS orderType,\n",
    "      c.orderdate                AS ordermadedate,\n",
    "      c.orderstatus              AS orderStatus,\n",
    "      afr.type                   AS feereductiontype,\n",
    "      p.dob,\n",
    "      CASE\n",
    "        WHEN FLOOR(DATE_DIFF('day', p.dob, p.createddate) / 365.25) < 0 THEN 0\n",
    "        ELSE ROUND(DATE_DIFF('day', p.dob, p.createddate) / 365.25)\n",
    "      END AS age_in_years\n",
    "    FROM opg_sirius_prod.persons p\n",
    "    JOIN opg_sirius_prod.cases c\n",
    "      ON p.id                   = c.client_id\n",
    "     AND c.glueexporteddate     = DATE('{run_date}')\n",
    "    LEFT JOIN active_fee_reductions afr\n",
    "      ON afr.client_id          = p.id\n",
    "    WHERE c.orderstatus IN ('OPEN','ACTIVE','DUPLICATE')\n",
    "      AND p.glueexporteddate     = DATE('{run_date}')\n",
    "    ORDER BY c.orderdate;\n",
    "    \"\"\"\n",
    "    return pydbtools.read_sql_query(query)\n",
    "\n",
    "\n",
    "def generate_month_list(start_month: str, end_month: str):\n",
    "    \"\"\"\n",
    "    Return a list of datetime objects for each month-start\n",
    "    from start_month to end_month inclusive.\n",
    "    \"\"\"\n",
    "    start_dt = parse_month(start_month)\n",
    "    end_dt = parse_month(end_month)\n",
    "    if start_dt > end_dt:\n",
    "        raise ValueError(f\"Start month ({start_month}) is after end month ({end_month})\")\n",
    "\n",
    "    months = []\n",
    "    current = start_dt\n",
    "    while current <= end_dt:\n",
    "        months.append(current)\n",
    "        current += relativedelta(months=1)\n",
    "    return months\n",
    "\n",
    "def last_day_of_month(dt: datetime) -> str:\n",
    "    \"\"\"Return the last day of dt's month as 'YYYY-MM-DD'.\"\"\"\n",
    "    day = calendar.monthrange(dt.year, dt.month)[1]\n",
    "    return dt.replace(day=day).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "def export_monthly_reports(first_month: str, last_month: str, output_base=\"output\") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Clean inputs\n",
    "    clean_first = first_month.strip().strip(\"'\\\"\")\n",
    "    clean_last = last_month.strip().strip(\"'\\\"\")\n",
    "\n",
    "    # Generate all months\n",
    "    months = generate_month_list(clean_first, clean_last)\n",
    "    if not months:\n",
    "        print(\"No months in range; nothing to do.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Prepare output directory\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    # **Clear the output directory, not the Excel filepath**\n",
    "    clear_directory(output_base)\n",
    "\n",
    "    excel_filename = f\"cases_{clean_first}_to_{clean_last}.xlsx\"\n",
    "    excel_path = os.path.join(output_base, excel_filename)\n",
    "\n",
    "    # List to accumulate each month's DataFrame\n",
    "    all_months = []\n",
    "\n",
    "    # Create Excel workbook and write each month's sheet\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "        for dt in months:\n",
    "            month_tag = dt.strftime(\"%Y-%m\")\n",
    "            run_date = last_day_of_month(dt)\n",
    "\n",
    "            # Fetch data for this month-end\n",
    "            df = fetch_cases_for_date(run_date)\n",
    "\n",
    "            # Tag the DataFrame with its month, then collect it\n",
    "            df[\"month\"] = month_tag\n",
    "            all_months.append(df)\n",
    "\n",
    "            # Save CSV for this month\n",
    "            month_folder = os.path.join(output_base, month_tag)\n",
    "            os.makedirs(month_folder, exist_ok=True)\n",
    "            csv_path = os.path.join(month_folder, f\"cases_{month_tag}.csv\")\n",
    "            df.to_csv(csv_path, index=False)\n",
    "\n",
    "            # Add to Excel workbook\n",
    "            df.to_excel(writer, sheet_name=month_tag, index=False)\n",
    "\n",
    "            print(f\"→ Saved CSV for {month_tag}: {csv_path}\")\n",
    "        pass\n",
    "        print(f\"→ Combined Excel workbook saved at: {excel_path}\")\n",
    "\n",
    "    # After all sheets are written, concatenate & export one big CSV\n",
    "    if all_months:\n",
    "        combined_df = pd.concat(all_months, ignore_index=True)\n",
    "        combined_csv_path = os.path.join(\n",
    "            output_base,\n",
    "            f\"all_cases_{clean_first}_to_{clean_last}.csv\"\n",
    "        )\n",
    "        combined_df.to_csv(combined_csv_path, index=False)\n",
    "        print(f\"→ Combined CSV for all months saved at: {combined_csv_path}\")\n",
    "    else:\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "    # ---- NEW SUMMARY SECTION ----\n",
    "    if not combined_df.empty:\n",
    "        # extract year from the month tag\n",
    "        combined_df[\"year\"] = pd.to_datetime(combined_df[\"month\"], format=\"%Y-%m\").dt.year\n",
    "\n",
    "        # annual summary\n",
    "        annual = (\n",
    "            combined_df\n",
    "            .groupby(\"year\")\n",
    "            .agg(\n",
    "                total_orders=(\"casenumber\", \"size\"),\n",
    "                total_people=(\"casenumber\", \"nunique\")\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # overall summary across entire period\n",
    "        overall = pd.DataFrame([{\n",
    "            \"year\": \"all\",\n",
    "            \"total_orders\": combined_df.shape[0],\n",
    "            \"total_people\": combined_df[\"casenumber\"].nunique()\n",
    "        }])\n",
    "\n",
    "        # combine for easy comparison\n",
    "        summary_df = pd.concat([annual, overall], ignore_index=True)\n",
    "\n",
    "        # print to console\n",
    "        print(\"\\n=== Orders & Unique-People Summary ===\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "    else:\n",
    "        summary_df = pd.DataFrame()\n",
    "        print(\"No data to summarise.\")\n",
    "\n",
    "    return combined_df, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Monthly active cases by order type\n",
    "\n",
    "This cell defines calculate_monthly_active_cases. The intention is to create a month by month view of how many active cases exist, broken down by order type.\n",
    "\n",
    "The function takes four inputs.\n",
    "\n",
    "df is the source DataFrame containing case level rows.\n",
    "first_month and last_month define the month range to report.\n",
    "output_base is the folder where the function writes its csv output.\n",
    "\n",
    "The function builds a list of months, then loops through that list. For each month it creates a month tag and prepares a DataFrame called df_active. The aggregation step groups by month and order type and counts unique case numbers. Using nunique here means a person with multiple rows is counted once within the group.\n",
    "\n",
    "After all months are processed, the per month summaries are concatenated into a single result table and written to a csv file. The function then creates a yearly summary from the collected active data. It reports two measures per year.\n",
    "\n",
    "order_count is the number of rows, which is a proxy for number of orders.\n",
    "unique_cases is the number of unique case numbers, which is a proxy for number of distinct clients.\n",
    "\n",
    "Important note for anyone maintaining this function. The current implementation sets df_active equal to df without filtering df down to the relevant month. The commented code shows an earlier idea of fetching or filtering per month, but that logic is not active. For the monthly outputs to be meaningful, df should already be filtered to the current month inside the loop, or the commented fetch should be re enabled in a future refactor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_active_cases(\n",
    "    df: pd.DataFrame,\n",
    "    first_month: str,\n",
    "    last_month: str,\n",
    "    output_base=\"output\"\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each month between first_month and last_month (inclusive), fetch the data,\n",
    "    filter to ACTIVE cases, then aggregate unique casenumber counts by orderType.\n",
    "    Writes a CSV 'monthly_active_cases_<first>_to_<last>.csv' under output_base,\n",
    "    prints yearly order & people counts, and returns:\n",
    "      - result_df: monthly/orderType aggregates\n",
    "      - summary_df: yearly order & unique-case counts\n",
    "    \"\"\"\n",
    "    # Clean inputs\n",
    "    clean_first = first_month.strip().strip(\"'\\\"\")\n",
    "    clean_last = last_month.strip().strip(\"'\\\"\")\n",
    "\n",
    "    # Generate all months\n",
    "    months = generate_month_list(clean_first, clean_last)\n",
    "    if not months:\n",
    "        print(\"No months in range; nothing to do.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Prepare output directory\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "    # Lists to collect each month's summary and raw active data\n",
    "    summaries = []\n",
    "    active_data = []\n",
    "\n",
    "    for dt in months:\n",
    "        month_tag = dt.strftime(\"%Y-%m\")\n",
    "        run_date = last_day_of_month(dt)\n",
    "\n",
    "        \n",
    "        # Fetch and filter to ACTIVE\n",
    "        #df = fetch_cases_for_date(run_date)\n",
    "        #df = combined_df\n",
    "        \n",
    "        #df_active = df[df[\"orderstatus\"] == \"ACTIVE\"].copy()\n",
    "        df_active = df\n",
    "        df_active[\"month\"] = month_tag\n",
    "\n",
    "        # Accumulate raw active rows for yearly summary\n",
    "        active_data.append(df_active)\n",
    "\n",
    "        # Aggregate unique casenumbers per orderType\n",
    "        if df_active.empty:\n",
    "            summaries.append(\n",
    "                pd.DataFrame([{\n",
    "                    \"month\": month_tag,\n",
    "                    \"orderType\": None,\n",
    "                    \"active_case_count\": 0\n",
    "                }])\n",
    "            )\n",
    "        else:\n",
    "            summary = (\n",
    "                df_active\n",
    "                .groupby([\"month\", \"ordertype\"], observed=False)[\"casenumber\"]\n",
    "                .nunique()\n",
    "                .reset_index(name=\"active_case_count\")\n",
    "            )\n",
    "            summaries.append(summary)\n",
    "\n",
    "        print(f\"→ Aggregated ACTIVE cases for {month_tag}\")\n",
    "\n",
    "    # Combine monthly summaries\n",
    "    result_df = pd.concat(summaries, ignore_index=True)\n",
    "\n",
    "    # Write out CSV\n",
    "    out_csv = os.path.join(\n",
    "        output_base,\n",
    "        f\"monthly_active_cases_{clean_first}_to_{clean_last}.csv\"\n",
    "    )\n",
    "    result_df.to_csv(out_csv, index=False)\n",
    "    print(f\"→ Monthly ACTIVE cases CSV saved at: {out_csv}\")\n",
    "\n",
    "    # ---- NEW YEARLY SUMMARY ----\n",
    "    if active_data:\n",
    "        combined_active = pd.concat(active_data, ignore_index=True)\n",
    "        combined_active[\"year\"] = pd.to_datetime(\n",
    "            combined_active[\"month\"], format=\"%Y-%m\"\n",
    "        ).dt.year\n",
    "\n",
    "        # Total orders per year\n",
    "        orders_year = (\n",
    "            combined_active\n",
    "            .groupby(\"year\")\n",
    "            .size()\n",
    "            .reset_index(name=\"order_count\")\n",
    "        )\n",
    "\n",
    "        # Unique cases (people) per year\n",
    "        people_year = (\n",
    "            combined_active\n",
    "            .groupby(\"year\")[\"casenumber\"]\n",
    "            .nunique()\n",
    "            .reset_index(name=\"unique_cases\")\n",
    "        )\n",
    "\n",
    "        summary_df = orders_year.merge(people_year, on=\"year\")\n",
    "\n",
    "        print(\"\\n=== Yearly Active Orders & Unique Cases ===\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "    else:\n",
    "        summary_df = pd.DataFrame()\n",
    "        print(\"No ACTIVE data to summarise.\")\n",
    "\n",
    "    return active_data, result_df, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Monthly flow of active cases\n",
    "\n",
    "This cell defines calculate_monthly_flow. It creates a simple movement table that answers, for each month, how many people are active, how many appear for the first time compared with the previous month, and how many disappear compared with the previous month.\n",
    "\n",
    "The logic relies on treating the set of active case numbers in each month as a mathematical set.\n",
    "\n",
    "For a given month m, let A(m) be the set of active case numbers in that month.\n",
    "\n",
    "The number active is |A(m)|, meaning the size of the set.\n",
    "\n",
    "The people who entered in month m are A(m) minus A(m−1). In set notation that is A(m) ∖ A(m−1).\n",
    "\n",
    "The people who exited in month m are A(m−1) minus A(m). In set notation that is A(m−1) ∖ A(m).\n",
    "\n",
    "The function builds A(m) for each month by querying the database at month end, then performs the set arithmetic above. This approach is fast and avoids duplicate counting.\n",
    "\n",
    "The first month in the range has no prior month in the snapshots, so its entered value would equal the full active set and its exited value would be zero. The code removes that first record before saving outputs, so the csv focuses on genuine month to month movement.\n",
    "\n",
    "After building the monthly table, the function creates a yearly summary. It unions the monthly entered sets and exited sets within a year to count unique people who entered and exited at any point in that year, and it unions the monthly active sets to estimate unique active clients within the year.\n",
    "\n",
    "The outputs are written to a csv file in the output folder and also printed for quick inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_flow(\n",
    "    #df: pd.DataFrame,\n",
    "    first_month: str,\n",
    "    last_month: str,\n",
    "    output_base=\"output\"\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each month from first_month to last_month (inclusive),\n",
    "    snapshot the set of active casenumbers, then compare to the prior month\n",
    "    to count how many entered, exited, and how many were active.\n",
    "    Writes 'monthly_flow_<first>_to_<last>.csv' under output_base,\n",
    "    prints yearly summaries, and returns (flow_df, summary_df).\n",
    "    \"\"\"\n",
    "    # Clean inputs\n",
    "    clean_first = first_month.strip().strip(\"'\\\"\")\n",
    "    clean_last  = last_month.strip().strip(\"'\\\"\")\n",
    "\n",
    "    # Generate month list\n",
    "    months = generate_month_list(clean_first, clean_last)\n",
    "    if not months:\n",
    "        print(\"No months in range; nothing to do.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Snapshot active casenumbers per month\n",
    "    snapshots = {}\n",
    "    for dt in months:\n",
    "        tag = dt.strftime(\"%Y-%m\")\n",
    "\n",
    "        # load data frame\n",
    "        df = fetch_cases_for_date(last_day_of_month(dt))\n",
    "        \n",
    "        snapshots[tag] = set(df[\"casenumber\"].unique())\n",
    "        print(f\"→ Snapshot for {tag}: {len(snapshots[tag])} active cases\")\n",
    "\n",
    "    # Build flow records\n",
    "    flow_records = []\n",
    "    entered_sets = {}\n",
    "    exited_sets  = {}\n",
    "    prev_tag = None\n",
    "\n",
    "    for tag in sorted(snapshots):\n",
    "        current = snapshots[tag]\n",
    "        active_cnt = len(current)\n",
    "\n",
    "        if prev_tag is None:\n",
    "            entered = current\n",
    "            exited  = set()\n",
    "        else:\n",
    "            prev = snapshots[prev_tag]\n",
    "            entered = current - prev\n",
    "            exited  = prev - current\n",
    "\n",
    "        entered_sets[tag] = entered\n",
    "        exited_sets[tag]  = exited\n",
    "\n",
    "        flow_records.append({\n",
    "            \"month\":        tag,\n",
    "            \"active_count\": active_cnt,\n",
    "            \"entered\":      len(entered),\n",
    "            \"exited\":       len(exited)\n",
    "        })\n",
    "        prev_tag = tag\n",
    "\n",
    "    # Create DataFrame & save CSV\n",
    "    flow_df = pd.DataFrame(flow_records)\n",
    "\n",
    "    # Remove the forst record as it only shows the whole count of active cases for entered and shows exited = 0\n",
    "    flow_df = flow_df[1:]\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    out_csv = os.path.join(output_base, f\"monthly_flow_{clean_first}_to_{clean_last}.csv\")\n",
    "    flow_df.to_csv(out_csv, index=False)\n",
    "    print(f\"→ Monthly flow CSV saved at: {out_csv}\")\n",
    "\n",
    "    # Yearly summary\n",
    "    flow_df[\"year\"] = pd.to_datetime(flow_df[\"month\"], format=\"%Y-%m\").dt.year\n",
    "    summary_records = []\n",
    "\n",
    "    for year, group in flow_df.groupby(\"year\"):\n",
    "        months_in_year = group[\"month\"].tolist()\n",
    "        total_entered = group[\"entered\"].sum()\n",
    "        total_exited  = group[\"exited\"].sum()\n",
    "        total_active  = group[\"active_count\"].sum()\n",
    "\n",
    "        unique_entered = len(set().union(*(entered_sets[m] for m in months_in_year)))\n",
    "        unique_exited  = len(set().union(*(exited_sets[m]  for m in months_in_year)))\n",
    "        unique_active  = len(set().union(*(snapshots[m]     for m in months_in_year)))\n",
    "\n",
    "        summary_records.append({\n",
    "            \"year\":            year,\n",
    "            #\"entered_orders\":  total_entered,\n",
    "            \"entered_people\":  unique_entered,\n",
    "            #\"exited_orders\":   total_exited,\n",
    "            \"exited_people\":   unique_exited,\n",
    "            #\"active_orders\":   total_active,\n",
    "            \"active_clients\":  unique_active\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "    print(\"\\n=== Yearly Flow & Active Summary ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    return flow_df, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Combining historical and forecast tables\n",
    "\n",
    "This small helper function exists to stitch two tables with the same column structure into one combined table.\n",
    "\n",
    "In this project we produce a historical table of age specific metrics from the database, and a forecast table of the same metrics from the stop flow model. Concatenating them with pandas concat creates one continuous time series that can be saved, plotted, and analysed in one go.\n",
    "\n",
    "ignore_index is set to True so the combined table gets a fresh sequential index rather than keeping the original indexes from the two inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append current and forecasted tables\n",
    "def get_combined_age_deputyship_table(tbl1, tbl2):\n",
    "    combined = pd.concat(\n",
    "        [tbl1, tbl2],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Year on year flows and age specific entry and termination rates\n",
    "\n",
    "This cell defines calculate_yearonyear_flows_and_age_rates. It is the main feature engineering step that turns raw case extracts into the quantities used by the forecasting model.\n",
    "\n",
    "There are two connected ideas in this function.\n",
    "\n",
    "The first is a year on year view of flow. Instead of comparing month m with month m−1, the code compares month m with the same month one year earlier, m−12. That gives a seasonal comparison, which is useful when you expect strong calendar effects.\n",
    "\n",
    "For each month tag, the function builds two snapshots.\n",
    "\n",
    "One snapshot is the set of active case numbers at the end of the current month.\n",
    "The other snapshot is the set of active case numbers at the end of the same month one year earlier.\n",
    "\n",
    "From these it calculates:\n",
    "\n",
    "active_count_current equals the size of the current set.\n",
    "active_count_previous equals the size of the previous year set.\n",
    "entered equals the number of case numbers that are in the current set but not in the previous year set.\n",
    "exited equals the number of case numbers that are in the previous year set but not in the current set.\n",
    "\n",
    "Mathematically, with A(m) as the current set and A(m−12) as the previous year set:\n",
    "\n",
    "entered(m) = |A(m) ∖ A(m−12)|\n",
    "exited(m)  = |A(m−12) ∖ A(m)|\n",
    "\n",
    "The second idea is age specific rates. The code takes the people who entered, the people who exited, and the base population from the previous year snapshot, and assigns them to age groups. Age groups are built as single year bins from 0 up to 106. Any missing or out of range ages are labelled as Unknown.\n",
    "\n",
    "For each month and each age group g it then computes:\n",
    "\n",
    "active(g) is the number of unique case numbers in the base population for that age group.\n",
    "entered(g) is the number of unique case numbers that entered for that age group.\n",
    "terminations(g) is the number of unique case numbers that exited for that age group.\n",
    "\n",
    "Termination rate is defined as terminations(g) divided by active(g) when active(g) is greater than zero. Retention rate is defined as 1 minus termination rate.\n",
    "\n",
    "These rates are the core inputs to the stop flow forecast later, because the stop flow recurrence needs a view of how many people typically enter and leave by age.\n",
    "\n",
    "Unknown age handling is optional. If redistribute_unknown_age is True, the function redistributes Unknown rows across the concrete age groups. The redistribution uses a proportional integer allocation method.\n",
    "\n",
    "It first counts known cases by age group and converts those counts into proportions.\n",
    "It multiplies those proportions by the total number of Unknown rows to get target fractional allocations.\n",
    "It takes the integer floor of each target, then distributes the remaining units to the groups with the largest fractional remainders. This is often called Hamilton apportionment.\n",
    "Finally it assigns the Unknown rows deterministically by sorted row index so that repeated runs give the same result.\n",
    "\n",
    "The function saves two detailed csv outputs into the output folder: one for the month level flow table and one for the month by age table of rates. It also returns four DataFrames so downstream cells can work in memory.\n",
    "\n",
    "flows_df contains the month level year on year flow measures.\n",
    "ages_df contains the month by age metrics including termination and retention rates.\n",
    "summary_df contains year level unique people counts based on unions of the monthly sets.\n",
    "monthly_summary_df contains month level people counts derived from the cached sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearonyear_flows_and_age_rates(\n",
    "    first_month: str,\n",
    "    last_month: str,\n",
    "    output_base: str = \"output\",\n",
    "    redistribute_unknown_age: bool = False,\n",
    "    age_bins: tuple = None,\n",
    "    age_labels: tuple = None\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculate year-on-year flows (entries/exits) and age-specific rates, with optional\n",
    "    imputation of missing ages (\"Unknown\") via proportional redistribution into\n",
    "    integer age groups. When redistribution is enabled, each Unknown row is reassigned\n",
    "    to a concrete integer age group using integer allocations (Hamilton apportionment),\n",
    "    and the 'age_group' value for those rows is updated accordingly.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Local imports for numerical helpers (keeps dependency scope clear)\n",
    "    import numpy as np\n",
    "\n",
    "    # --- Logging the operation and key parameters for traceability\n",
    "    logging.info(\n",
    "        f\"Calculating year-on-year flows and age rates from {first_month} to {last_month}, \"\n",
    "        f\"redistribute_unknown_age={redistribute_unknown_age}\"\n",
    "    )\n",
    "\n",
    "    # --- Ensure the output directory exists to avoid file write errors later\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "    # --- Establish age bins and labels (default is 0–106 inclusive as integer groups)\n",
    "    # If caller didn't supply custom bins/labels, build the defaults.\n",
    "    if age_bins is None or age_labels is None:\n",
    "        age_bins = list(range(0, 107))                     # Edges for [0,1), [1,2), ..., [106,107)\n",
    "        age_labels = [str(a) for a in age_bins[:-1]]       # String labels '0'...'106' (no \"Unknown\")\n",
    "\n",
    "    # --- Make sure we work with labels that do NOT include 'Unknown' for final outputs\n",
    "    # If a user passed labels that contain 'Unknown', remove it from the cut-labels.\n",
    "    labels_for_cut = [lbl for lbl in age_labels if lbl != \"Unknown\"]  # Used by pd.cut\n",
    "    final_age_labels = labels_for_cut[:]                               # Final index order for outputs\n",
    "\n",
    "    # --- Helper: cut ages into groups and set 'Unknown' for out-of-range/missing ages\n",
    "    def _assign_age_groups_inplace(df: pd.DataFrame) -> None:\n",
    "        \"\"\"Add/overwrite df['age_group'] using bins/labels; out-of-range/missing -> 'Unknown'.\"\"\"\n",
    "        # Compute categorical age groups for known ages using left-closed bins\n",
    "        df[\"age_group\"] = pd.cut(\n",
    "            df[\"age_in_years\"],               # Source age column (assumed present)\n",
    "            bins=age_bins,                    # Integer bin edges (e.g., [0,1), [1,2), ...)\n",
    "            labels=labels_for_cut,            # Only concrete integer labels\n",
    "            right=False,                      # Include left edge, exclude right edge\n",
    "            include_lowest=True               # Include the lowest bound\n",
    "        )\n",
    "        # Convert NaN categories (missing/out-of-range) into the literal string \"Unknown\"\n",
    "        df[\"age_group\"] = df[\"age_group\"].astype(object).where(df[\"age_group\"].notna(), \"Unknown\")\n",
    "\n",
    "    # --- Helper: proportional integer allocation (Hamilton apportionment)\n",
    "    def _proportional_integer_allocation(known_counts: pd.Series, unknown_total: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Allocate 'unknown_total' integer units across index labels of 'known_counts'\n",
    "        proportionally to known_counts (or uniformly if all zeros).\n",
    "        Returns a Series of integer allocations indexed like known_counts.\n",
    "        \"\"\"\n",
    "        # Ensure index order matches final labels and fill missing with zero\n",
    "        known_counts = known_counts.reindex(final_age_labels, fill_value=0)\n",
    "\n",
    "        # Sum the known counts to derive proportions\n",
    "        total_known = known_counts.sum()\n",
    "\n",
    "        # If no known info exists, split uniformly; else use proportional shares\n",
    "        if total_known == 0:\n",
    "            # Uniform shares across all age groups\n",
    "            raw = pd.Series(np.full(len(final_age_labels), unknown_total / max(len(final_age_labels), 1.0)),\n",
    "                            index=final_age_labels, dtype=float)\n",
    "        else:\n",
    "            # Proportional shares: each group's fraction times unknown_total\n",
    "            raw = (known_counts / total_known) * unknown_total\n",
    "\n",
    "        # Base integer allocations via floor\n",
    "        base = np.floor(raw).astype(int)\n",
    "\n",
    "        # Remaining units to distribute due to flooring\n",
    "        remainder = int(unknown_total - base.sum())\n",
    "\n",
    "        # Fractional remainders for Hamilton method\n",
    "        frac = raw - base\n",
    "\n",
    "        # Deterministic tie-break: sort by fractional part desc, then by label asc\n",
    "        order = sorted(final_age_labels, key=lambda x: (-frac.loc[x], x))\n",
    "\n",
    "        # Distribute one-by-one to the top 'remainder' labels\n",
    "        for i in range(remainder):\n",
    "            base.loc[order[i]] += 1\n",
    "\n",
    "        # Return allocations as a Series aligned to final_age_labels\n",
    "        return base.reindex(final_age_labels).astype(int)\n",
    "\n",
    "    # --- Helper: impute unknown ages row-wise, deterministically, in-place\n",
    "    def _impute_unknowns_inplace(df: pd.DataFrame, id_col: str = \"casenumber\") -> None:\n",
    "        \"\"\"\n",
    "        For a given df that already has 'age_group' with some 'Unknown',\n",
    "        reassign 'Unknown' rows to concrete integer age groups.\n",
    "\n",
    "        Allocation weights are derived from the df's own composition using\n",
    "        unique {id_col} counts by known age group. The assignment to rows is\n",
    "        deterministic (sorted index order) to ensure reproducibility.\n",
    "        \"\"\"\n",
    "        # Identify which rows are currently Unknown\n",
    "        unknown_idx = df.index[df[\"age_group\"] == \"Unknown\"]\n",
    "        # If nothing to impute, bail early\n",
    "        if len(unknown_idx) == 0:\n",
    "            return\n",
    "\n",
    "        # Build weights using unique case counts by known age group\n",
    "        # (drop Unknown to avoid circularity)\n",
    "        known_unique = (\n",
    "            df.loc[df[\"age_group\"] != \"Unknown\", [\"age_group\", id_col]]\n",
    "              .drop_duplicates()\n",
    "              .groupby(\"age_group\", observed=False)[id_col]\n",
    "              .nunique()\n",
    "              .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "\n",
    "        # Compute integer allocations across age groups for the Unknown total\n",
    "        allocations = _proportional_integer_allocation(known_unique, len(unknown_idx))\n",
    "\n",
    "        # Deterministic row assignment: sort unknown indices so results are stable\n",
    "        unknown_idx_sorted = sorted(unknown_idx.tolist())\n",
    "\n",
    "        # Pointer into the unknown index list as we assign chunks\n",
    "        cursor = 0\n",
    "\n",
    "        # Assign each block of Unknown rows to its allocated age group\n",
    "        for lbl in final_age_labels:\n",
    "            k = int(allocations.get(lbl, 0))     # How many Unknown rows to assign to this label\n",
    "            if k > 0:\n",
    "                take = unknown_idx_sorted[cursor: cursor + k]  # Slice next k rows\n",
    "                df.loc[take, \"age_group\"] = lbl                # Set their age_group to the label\n",
    "                cursor += k                                    # Advance the pointer\n",
    "\n",
    "        # Safety: if any Unknowns remain due to edge cases, place them in the smallest label\n",
    "        if (df[\"age_group\"] == \"Unknown\").any():\n",
    "            leftovers = df.index[df[\"age_group\"] == \"Unknown\"]\n",
    "            fallback = final_age_labels[0] if final_age_labels else \"0\"\n",
    "            df.loc[leftovers, \"age_group\"] = fallback\n",
    "\n",
    "    # --- Containers to accumulate per-month analytics (flows and age-rate details)\n",
    "    flow_records = []         # List of dicts: overall monthly counts (active, entered, exited)\n",
    "    age_rate_records = []     # List of dicts: per-month x age-group counts and rates\n",
    "    snapshots_cur = {}        # Dict: month tag -> set of active casenumbers (current month)\n",
    "    snapshots_prev = {}       # Dict: month tag -> set of active casenumbers (prev-year same month)\n",
    "    entered_sets = {}         # Dict: month tag -> set of casenumbers entered this month\n",
    "    exited_sets = {}          # Dict: month tag -> set of casenumbers exited this month\n",
    "\n",
    "    # --- Iterate each month in the requested window\n",
    "    for dt in generate_month_list(first_month, last_month):\n",
    "        # Compute the month exactly one year earlier for YoY comparisons\n",
    "        prev_dt = dt - relativedelta(years=1)\n",
    "\n",
    "        # Skip early months that don't have a prior-year comparison within window\n",
    "        if prev_dt < parse_month(first_month):\n",
    "            continue\n",
    "\n",
    "        # Create a YYYY-MM tag for logging and indexing\n",
    "        tag = dt.strftime(\"%Y-%m\")\n",
    "        logging.info(f\"Processing month {tag}\")\n",
    "\n",
    "        # Fetch snapshots of active cases at month-end for current and prior-year month\n",
    "        df_cur = fetch_cases_for_date(last_day_of_month(dt))\n",
    "        df_prev = fetch_cases_for_date(last_day_of_month(prev_dt))\n",
    "\n",
    "        # Convert to sets of IDs for fast set arithmetic\n",
    "        set_cur = set(df_cur[\"casenumber\"])\n",
    "        set_prev = set(df_prev[\"casenumber\"])\n",
    "\n",
    "        # Persist these snapshots for later summaries\n",
    "        snapshots_cur[tag] = set_cur\n",
    "        snapshots_prev[tag] = set_prev\n",
    "\n",
    "        # Entrants are in current but not in previous; exits are the opposite\n",
    "        entered = set_cur - set_prev\n",
    "        exited = set_prev - set_cur\n",
    "\n",
    "        # Cache the entrant/exit sets for people-level yearly/monthly summaries\n",
    "        entered_sets[tag] = entered\n",
    "        exited_sets[tag] = exited\n",
    "\n",
    "        # Record high-level flow counts for this month\n",
    "        flow_records.append({\n",
    "            \"month\":                 tag,\n",
    "            \"active_count_current\":  len(set_cur),\n",
    "            \"active_count_previous\": len(set_prev),\n",
    "            \"entered\":               len(entered),\n",
    "            \"exited\":                len(exited)\n",
    "        })\n",
    "\n",
    "        # Build three DataFrames for age analysis:\n",
    "        #  - df_term: those who exited (from last year's snapshot)\n",
    "        #  - df_in:   those who entered (into this year's snapshot)\n",
    "        #  - df_base: the base population (last year's snapshot)\n",
    "        df_term = df_prev[df_prev[\"casenumber\"].isin(exited)].copy()\n",
    "        df_in   = df_cur[df_cur[\"casenumber\"].isin(entered)].copy()\n",
    "        df_base = df_prev.copy()\n",
    "\n",
    "        # Assign initial age groups with \"Unknown\" for missing/out-of-range\n",
    "        _assign_age_groups_inplace(df_term)\n",
    "        _assign_age_groups_inplace(df_in)\n",
    "        _assign_age_groups_inplace(df_base)\n",
    "\n",
    "        # Optionally impute Unknown ages by redistributing them into integer groups\n",
    "        if redistribute_unknown_age:\n",
    "            _impute_unknowns_inplace(df_term)   # Replace 'Unknown' with concrete age_group\n",
    "            _impute_unknowns_inplace(df_in)     # Replace 'Unknown' with concrete age_group\n",
    "            _impute_unknowns_inplace(df_base)   # Replace 'Unknown' with concrete age_group\n",
    "\n",
    "        # --- Diagnostics (can be converted to logging.debug if preferred)\n",
    "        # print(\"All records in base:\", len(df_base))\n",
    "        # print(\"Records with age_group assigned (incl. imputed):\", df_base['age_group'].notna().sum())\n",
    "\n",
    "        # --- Compute counts by integer age group (final_age_labels), filling missing with zeros\n",
    "        # People entered per age group (unique casenumbers)\n",
    "        in_counts = (\n",
    "            df_in.groupby(\"age_group\", observed=False)[\"casenumber\"]\n",
    "                 .nunique()\n",
    "                 .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "        # People exited per age group (unique casenumbers)\n",
    "        term_counts = (\n",
    "            df_term.groupby(\"age_group\", observed=False)[\"casenumber\"]\n",
    "                  .nunique()\n",
    "                  .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "        # Active people in base per age group (unique casenumbers)\n",
    "        base_counts = (\n",
    "            df_base.groupby(\"age_group\", observed=False)[\"casenumber\"]\n",
    "                  .nunique()\n",
    "                  .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "\n",
    "        # Orders (row counts) by age group — useful if multiple rows per person exist\n",
    "        in_order_counts = (\n",
    "            df_in.groupby(\"age_group\", observed=False)[\"casenumber\"]\n",
    "                 .count()\n",
    "                 .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "        term_order_counts = (\n",
    "            df_term.groupby(\"age_group\", observed=False)[\"casenumber\"]\n",
    "                  .count()\n",
    "                  .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "        order_counts = (\n",
    "            df_base.groupby(\"age_group\", observed=False)[\"casenumber\"]\n",
    "                  .count()\n",
    "                  .reindex(final_age_labels, fill_value=0)\n",
    "        )\n",
    "\n",
    "        # --- Build age-rate rows for this month across all integer age groups\n",
    "        for grp in final_age_labels:\n",
    "            active      = int(base_counts[grp])                 # Active unique people in base\n",
    "            orders_age  = int(order_counts[grp])                # Active orders (rows) in base\n",
    "            clients_age = active                                # Alias kept for continuity\n",
    "            term        = int(term_counts[grp])                 # Exits (unique people)\n",
    "            ent         = int(in_counts[grp])                   # Entries (unique people)\n",
    "            rate        = round(term / active, 4) if active else 0.0  # Termination rate\n",
    "            retention   = 1 - rate if rate >= 0 else 1.0        # Retention (1 - termination)\n",
    "\n",
    "            # Append a fully specified record for this (month, age_group)\n",
    "            age_rate_records.append({\n",
    "                \"month\":              tag,\n",
    "                \"age_group\":          grp,\n",
    "                \"active_count\":       active,\n",
    "                \"active_orders_age\":  orders_age,\n",
    "                \"active_clients_age\": clients_age,\n",
    "                \"entered\":            ent,\n",
    "                \"terminations\":       term,\n",
    "                \"termination_rate\":   rate,\n",
    "                \"retention_rate\":     retention\n",
    "            })\n",
    "\n",
    "    # --- Convert accumulated lists to DataFrames for downstream use\n",
    "    flows_df = pd.DataFrame(flow_records)       # Month-level flows\n",
    "    ages_df  = pd.DataFrame(age_rate_records)   # Month x age-group metrics\n",
    "\n",
    "    # --- Persist the outputs for reproducibility/auditing\n",
    "    flows_df.to_csv(\n",
    "        os.path.join(output_base, f\"yearonyear_flows_{first_month}_to_{last_month}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "    ages_df.to_csv(\n",
    "        os.path.join(output_base, f\"termination_and_entry_rates_by_age_{first_month}_to_{last_month}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # --- Yearly people-level summary (unique people across months per year)\n",
    "    flows_df[\"year\"] = pd.to_datetime(flows_df[\"month\"], format=\"%Y-%m\").dt.year  # Extract calendar year\n",
    "    summary_records = []                                                           # Collector for yearly rows\n",
    "\n",
    "    # Iterate each year and union people across the year's months (entered/exited/active)\n",
    "    for year, grp in flows_df.groupby(\"year\"):\n",
    "        months_in_year = grp[\"month\"].tolist()                                     # Months in this year\n",
    "        entered_people = len(set().union(*(entered_sets[m] for m in months_in_year)))  # Unique entrants\n",
    "        exited_people  = len(set().union(*(exited_sets[m]  for m in months_in_year)))  # Unique exits\n",
    "        active_clients = len(set().union(*(snapshots_cur[m] for m in months_in_year))) # Unique active\n",
    "\n",
    "        # Append the yearly summary row\n",
    "        summary_records.append({\n",
    "            \"year\":           year,\n",
    "            \"entered_people\": entered_people,\n",
    "            \"exited_people\":  exited_people,\n",
    "            \"active_clients\": active_clients\n",
    "        })\n",
    "\n",
    "    # Materialize yearly summary table\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "\n",
    "    # --- Console print for a quick glance (can swap to logging.info if preferred)\n",
    "    print(\"\\n=== Yearly Summary: Orders & Clients ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # --- Monthly people-level summary using the cached sets\n",
    "    monthly_records = []  # Collector for monthly rows\n",
    "\n",
    "    for _, row in flows_df.iterrows():\n",
    "        m = row[\"month\"]                                # Month tag\n",
    "        monthly_records.append({\n",
    "            \"month\":           m,\n",
    "            \"entered_people\":  len(entered_sets[m]),    # Unique entrants that month\n",
    "            \"exited_people\":   len(exited_sets[m]),     # Unique exits that month\n",
    "            \"active_clients\":  len(snapshots_cur[m])    # Unique active that month\n",
    "        })\n",
    "\n",
    "    # Materialise monthly summary table\n",
    "    monthly_summary_df = pd.DataFrame(monthly_records)\n",
    "\n",
    "    # Console print for a quick glance\n",
    "    print(\"\\n=== Monthly Summary: Orders & Clients ===\")\n",
    "    print(monthly_summary_df.to_string(index=False))\n",
    "\n",
    "    # --- Final log to indicate successful completion\n",
    "    logging.info(\"Completed calculation of year-on-year flows and age rates\")\n",
    "\n",
    "    # --- Return the four primary outputs: flows, age metrics, yearly and monthly people summaries\n",
    "    return flows_df, ages_df, summary_df, monthly_summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Run block for extraction and feature engineering\n",
    "\n",
    "This cell is a runnable orchestration block. It sets the date window for the extraction, creates or clears the output folder, then calls the main functions defined earlier to produce the datasets used for forecasting.\n",
    "\n",
    "The start_month and end_month values define the month end snapshots that will be pulled from Sirius. The output_base variable controls where all files are written.\n",
    "\n",
    "The cell runs three main steps.\n",
    "\n",
    "First it calls export_monthly_reports to pull raw data for each month end, save a per month csv, and build a combined table across the full period.\n",
    "\n",
    "Second it calls calculate_monthly_active_cases to create an aggregated view of active cases by order type. This provides a quick check on volumes and composition.\n",
    "\n",
    "Third it calls calculate_yearonyear_flows_and_age_rates with redistribute_unknown_age set to True. This produces final_df, which holds the year on year flow measures, and ages_df, which holds the age specific active counts, entry counts, termination counts, and rates.\n",
    "\n",
    "These two DataFrames are the main hand off into the forecasting step. Later cells rely on final_df and ages_df being present in memory.\n",
    "\n",
    "Because this is a notebook, __name__ is usually \"__main__\" when the cell runs. That means this block will execute when you run the cell, and it will also execute if you run all cells in order. Keep in mind that it clears the output directory, so do not point output_base at a folder that contains anything you need to keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Deputyship forecasting model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_year = 2022\n",
    "    end_year = 2025\n",
    "    start_month = \"2024-12\"\n",
    "    end_month = \"2025-12\"\n",
    "    output_base=\"output\"\n",
    "\n",
    "    # Prepare output directory\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    # **Clear the output directory, not the Excel filepath**\n",
    "    clear_directory(output_base)\n",
    "    \n",
    "    combined_df, summary_df = export_monthly_reports(start_month, end_month)\n",
    "    summary_df\n",
    "    print(combined_df)\n",
    "    \n",
    "    active_df, monthly_df, yearly_summary = calculate_monthly_active_cases(combined_df, start_month, end_month, output_base=\"output\")\n",
    "    yearly_summary\n",
    "    print(monthly_df)\n",
    "    print(active_df)\n",
    "    \n",
    "\n",
    "    # Calculate historical flows and age rates\n",
    "    final_df, ages_df, summary_df, monthly_summary_df = calculate_yearonyear_flows_and_age_rates(\n",
    "         start_month, end_month,\n",
    "         redistribute_unknown_age=True)\n",
    "    \n",
    "    print(summary_df)\n",
    "    print(monthly_summary_df)\n",
    "    print(ages_df)\n",
    "    print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Quick inspection of the flow output\n",
    "\n",
    "This cell simply displays final_df in the notebook output.\n",
    "\n",
    "final_df is the month level table produced by calculate_yearonyear_flows_and_age_rates. Looking at it here is a simple sanity check that the extraction ran successfully and that month tags and counts look reasonable before moving on to forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Quick inspection of the age specific rate table\n",
    "\n",
    "This cell displays ages_df.\n",
    "\n",
    "ages_df is the detailed month by age table that contains active counts, entry counts, termination counts, and derived rates such as termination_rate and retention_rate. The stop flow forecast uses these values, so it is worth checking that the month column parsed correctly and that age groups look as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Stop flow forecast function\n",
    "\n",
    "This cell defines stop_flow_forecast, the forecasting method used in this notebook.\n",
    "\n",
    "The stop flow idea is to update the number of active cases using a simple accounting identity:\n",
    "\n",
    "active(t) = active(t−1) + entered(t−12) − terminations(t−12)\n",
    "\n",
    "The identity says that the active caseload at time t equals last month active caseload plus new people entering, minus people leaving. The seasonal assumption in this implementation is that entries and terminations for month t behave like the entries and terminations observed in the same calendar month one year earlier. That is why the function looks up flows at a 12 month lag.\n",
    "\n",
    "How the function uses the historical data.\n",
    "\n",
    "It takes ages_df as input. ages_df contains active_count for each age group and month, plus entered and terminations for each age group and month.\n",
    "\n",
    "It identifies the last historical month in the input, then uses the active counts in that month as the starting state for the forecast.\n",
    "\n",
    "It creates a list of forecast months, then loops over them. For each forecast month and each age group it:\n",
    "\n",
    "1. Reads the previous month forecast active count for that age group.\n",
    "2. Looks up the entered and terminations values from the lag month one year earlier in the historical table.\n",
    "3. Applies the recurrence active(t) = active(t−1) + entered − terminations, with a floor at zero so the forecast never goes negative.\n",
    "4. Stores the result and updates the previous month state ready for the next month.\n",
    "\n",
    "The function returns four tables.\n",
    "\n",
    "per_age_df contains the forecast by month and age group.\n",
    "monthly is a month level total across age groups.\n",
    "yearly is a year level total across the forecast horizon.\n",
    "df is a copy of the input ages_df for reference.\n",
    "\n",
    "This function is intentionally simple. Its value is in producing a transparent baseline forecast that preserves seasonality, rather than in fitting a complex statistical model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_flow_forecast(\n",
    "    ages_df: pd.DataFrame,\n",
    "    periods: int = 12\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    “Stop‐flow” forecast:\n",
    "      active_t    = active_{t–1} + entered_{t–12} – terminated_{t–12}\n",
    "\n",
    "    Returns:\n",
    "      - per_age_df          : month, age_group, active_forecast,\n",
    "                              active_orders_age_fc, active_clients_age_fc\n",
    "      - monthly_summary_df  : month, total_active_orders, total_active_clients\n",
    "      - yearly_summary_df   : year, yearly_active_orders, yearly_active_clients\n",
    "      - base_df             : the input ages_df (for reference)\n",
    "    \"\"\"\n",
    "    # Make a copy of the input data so we don't change the original\n",
    "    df = ages_df.copy()\n",
    "\n",
    "    # Ensure 'month' is a column; if it's an index, reset it, otherwise error\n",
    "    if 'month' not in df.columns:\n",
    "        if 'month' in df.index.names:\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            raise ValueError(\"Input ages_df must have a 'month' column or index level\")\n",
    "\n",
    "    # Convert 'month' column to date format, just to be sure\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "\n",
    "    # Find the last (most recent) month in the historical data\n",
    "    last_hist = df['month'].max()\n",
    "\n",
    "    # Make a list of months to forecast (e.g. next 12 months)\n",
    "    fc_months = [last_hist + relativedelta(months=i) for i in range(1, periods+1)]\n",
    "\n",
    "    # Get the data for the last historical month\n",
    "    base = df[df['month']==last_hist]\n",
    "    \n",
    "    \n",
    "    # Store the active counts for each age group from the last month as a starting point\n",
    "    prev_active      = base.set_index('age_group')['active_count'].to_dict()\n",
    "    prev_orders      = base.set_index('age_group')['active_orders_age'].to_dict()  # (Commented out)\n",
    "    prev_clients     = base.set_index('age_group')['active_clients_age'].to_dict()\n",
    "\n",
    "    records = []  # This will store forecast results for each month and age group\n",
    "\n",
    "    # For each month in the forecast period\n",
    "    for m in fc_months:\n",
    "        print(f\"month: {m}\")\n",
    "        # Load all active cases at the end of the current and previous year’s month\n",
    "        #df_cur  = fetch_cases_for_date(last_day_of_month(pd.to_datetime(m)))\n",
    "        #print(df_cur)\n",
    "        \n",
    "        # Find the matching month from 12 months ago (for stop-flow calculation)\n",
    "        lag = m - relativedelta(years=1)\n",
    "        # For each age group\n",
    "        for age in df['age_group'].unique():\n",
    "            #print(f\"age: {age}\")\n",
    "            # Get previous forecasted counts, or 0 if not found\n",
    "            a_prev     = prev_active.get(age, 0)\n",
    "            #print(f\"1.a_prev: {a_prev}\")\n",
    "            o_prev     = prev_orders .get(age, 0)   # (Commented out)\n",
    "            c_prev     = prev_clients.get(age, 0)\n",
    "            #print(f\"c_prev: {c_prev}\")\n",
    "            \n",
    "            # Find the data for this age group from 12 months ago (if it exists)\n",
    "            row        = df[(df['month']==lag)&(df['age_group']==age)]\n",
    "            # Get 'entered' and 'terminations' values; use 0 if missing\n",
    "            entered    = int(row['entered'     ].iloc[0]) if not row.empty else 0\n",
    "            #print(f\"entered: {entered}\")\n",
    "            \n",
    "            term       = int(row['terminations'].iloc[0]) if not row.empty else 0\n",
    "            #print(f\"term: {term}\")\n",
    "            \n",
    "            # Calculate new forecast: previous + entered - terminated (but not below zero)\n",
    "            a_fc = max(0, a_prev + entered - term)\n",
    "            #print(f\"a_fc: {a_fc}\")\n",
    "            o_fc = max(0, o_prev + entered - term)   # (Commented out)\n",
    "            c_fc = max(0, c_prev + entered - term)\n",
    "            #print(f\"c_fc: {c_fc}\")\n",
    "            \n",
    "            # Store the result for this month and age group\n",
    "            records.append({\n",
    "                'month':                 m,\n",
    "                'age_group':             age,\n",
    "                'active_forecast':       a_fc, \n",
    "                'active_orders_age_fc':  o_fc,    # (Commented out)\n",
    "                'active_clients_age_fc': c_fc\n",
    "            }) #active_clients_age_fc\n",
    "\n",
    "            # Update the previous values for the next month in the loop\n",
    "            prev_active[age]  = a_fc\n",
    "            prev_orders[age]  = o_fc   # (Commented out)\n",
    "            prev_clients[age] = c_fc\n",
    "\n",
    "    # Convert all forecast records into a DataFrame (table)\n",
    "    per_age_df = pd.DataFrame(records)\n",
    "\n",
    "    # Make a summary table for each forecast month (total across ages)\n",
    "    monthly = (\n",
    "        per_age_df\n",
    "        .groupby('month')\n",
    "        .agg(\n",
    "            total_active_orders=('active_orders_age_fc' , 'sum'),    # (Commented out)\n",
    "            total_active_clients=('active_clients_age_fc', 'sum')\n",
    "            #total_active=('active_forecast', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    # print(\"\\n=== Monthly Stop‐Flow Summary ===\")\n",
    "    # print(monthly.to_string(index=False))\n",
    "\n",
    "    # Add a 'year' column for yearly summary\n",
    "    monthly['year'] = monthly['month'].dt.year\n",
    "\n",
    "    # Make a summary table for each year (totals across months)\n",
    "    yearly = (\n",
    "        monthly\n",
    "        .groupby('year')\n",
    "        .agg(\n",
    "            #yearly_active_orders = ('total_active_orders', 'sum'),   # (Commented out)\n",
    "            yearly_active_clients= ('total_active_clients','sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    # print(\"\\n=== Yearly Stop‐Flow Summary ===\")\n",
    "    # print(yearly.to_string(index=False))\n",
    "\n",
    "    # Return all results and the original input data\n",
    "    return per_age_df, monthly, yearly, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Visualisation and insight generation\n",
    "\n",
    "The next code cell defines a helper function that turns the combined historical and forecast table into charts and a short insights note.\n",
    "\n",
    "The input to the function is a DataFrame that contains at least a month column, an age column, and columns that represent active caseload counts for clients and orders. The function is defensive about naming. If the exact column names are not present it looks for common alternatives and renames them into a standard form so the plotting code can stay simple.\n",
    "\n",
    "The function also standardises dates. Month values are converted to pandas datetime so the x axis in charts behaves correctly. If you pass hist_last_month, the function draws a vertical marker at that month to show where the historical data ends and the forecast begins.\n",
    "\n",
    "Several visual outputs are produced in the output folder.\n",
    "\n",
    "A totals line chart shows total active clients and total active orders over time. It also adds a rough uncertainty band using a Poisson style approximation for count data. For a monthly count x the function computes a 95 percent interval as:\n",
    "\n",
    "lower = max(0, x − z √(phi x))\n",
    "upper = x + z √(phi x)\n",
    "\n",
    "where z is 1.96 and phi is a dispersion factor. When phi is 1 this matches the basic Poisson square root variance assumption. Larger values of phi widen the band to reflect over dispersion.\n",
    "\n",
    "A stacked area chart groups ages into five year bands and shows how the composition of the caseload changes over time.\n",
    "\n",
    "A heatmap shows active clients by five year age band and month, which helps to spot cohort patterns and seasonal effects.\n",
    "\n",
    "A top movers chart compares the first and last month in the horizon and highlights which ages contribute most to the change in caseload.\n",
    "\n",
    "A ratio chart shows orders per 100 clients over time, which can flag shifts in how many orders exist per person.\n",
    "\n",
    "In addition to charts, the function writes an insights markdown file summarising horizon start and end, total change in clients and orders, peak months, and the biggest age group increases and decreases.\n",
    "\n",
    "The function returns a small dictionary of headline metrics so the caller can log or store the key results programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Visualisation & Insight Analysis (append below)\n",
    "# -----------------------------------------------\n",
    "def visualize_and_analyze_deputyship_forecasts(\n",
    "    combined_df: pd.DataFrame,\n",
    "    output_dir: str = \"output\",\n",
    "    hist_last_month: \"pd.Timestamp|str|None\" = None,\n",
    "    top_k: int = 8,\n",
    "    axis_start_month: \"pd.Timestamp|str|None\" = None,   # <-- add this\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Visualize historical + forecasted active caseloads (clients & orders) and produce key insights.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    combined_df : DataFrame with at least:\n",
    "        - 'month' (datetime or string)\n",
    "        - 'age' (string or int age group)\n",
    "        - 'active_caseloads_clients' (or fallbacks: 'active_clients_age', 'active_clients_age_fc', 'active_forecast')\n",
    "        - 'active_caseloads_orders'  (or fallbacks: 'active_orders_age',  'active_orders_age_fc',  'active_forecast')\n",
    "    output_dir : where to save PNGs and insights markdown.\n",
    "    hist_last_month : last historical month (for a vertical cutoff line). If None, no cutoff line is drawn.\n",
    "    top_k : how many age groups to highlight in stacked area and top-movers charts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A small dict of computed summary metrics for programmatic use.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = combined_df.copy()\n",
    "\n",
    "    # --- Ensure 'month' is datetime (handles '2025-07' and 'Jul-25' style strings)\n",
    "    if not np.issubdtype(df['month'].dtype, np.datetime64):\n",
    "        # try robust parsing; attempt two common formats\n",
    "        try:\n",
    "            df['month'] = pd.to_datetime(df['month'])\n",
    "        except Exception:\n",
    "            df['month'] = pd.to_datetime(df['month'], format=\"%b-%y\")\n",
    "\n",
    "    # --- Standardise column names via safe coalescing (guards against earlier rename differences)\n",
    "    def coalesce_col(frame, candidates, new_name):\n",
    "        for c in candidates:\n",
    "            if c in frame.columns:\n",
    "                frame.rename(columns={c: new_name}, inplace=True)\n",
    "                return new_name\n",
    "        # if none exist, create an empty numeric column\n",
    "        frame[new_name] = 0\n",
    "        return new_name\n",
    "\n",
    "    clients_col = coalesce_col(\n",
    "        df,\n",
    "        [\"active_caseloads_clients\", \"active_clients_age\", \"active_clients_age_fc\"],\n",
    "        \"active_caseloads_clients\"\n",
    "    )\n",
    "    orders_col = coalesce_col(\n",
    "        df,\n",
    "        [\"active_caseloads_orders\", \"active_orders_age\", \"active_orders_age_fc\"],\n",
    "        \"active_caseloads_orders\"\n",
    "    )\n",
    "\n",
    "    # --- NEW: parse / infer axis_start_month safely ---\n",
    "    if axis_start_month is not None and not isinstance(axis_start_month, pd.Timestamp):\n",
    "        axis_start_month = pd.to_datetime(axis_start_month)\n",
    "\n",
    "    if axis_start_month is None:\n",
    "        # Auto-start at first non-zero month to avoid leading zeros\n",
    "        totals_all = (\n",
    "            df.groupby('month')[[clients_col, orders_col]]\n",
    "              .sum(min_count=1)\n",
    "              .fillna(0)\n",
    "        )\n",
    "        nz = totals_all.sum(axis=1) > 0\n",
    "        axis_start_month = nz.index[nz.argmax()] if nz.any() else df['month'].min()\n",
    "\n",
    "    \n",
    "    # --- Tidy 'age' to string and keep a stable order\n",
    "    if 'age' not in df.columns:\n",
    "        # Fallback if upstream still calls the column 'age_group'\n",
    "        if 'age_group' in df.columns:\n",
    "            df.rename(columns={'age_group': 'age'}, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"Expected an 'age' (or 'age_group') column in combined_df.\")\n",
    "    df['age'] = df['age'].astype(str)\n",
    "\n",
    "    # --- Sort for plotting\n",
    "    df = df.sort_values(['month', 'age']).reset_index(drop=True)\n",
    "\n",
    "    # --- Identify last historical month (line on charts) if provided as str\n",
    "    if hist_last_month is not None and not isinstance(hist_last_month, pd.Timestamp):\n",
    "        hist_last_month = pd.to_datetime(hist_last_month)\n",
    "\n",
    "    # # ========== 1) Total caseloads over time (line) ==========\n",
    "    # totals = (\n",
    "    #     df.groupby('month', as_index=False)\n",
    "    #       .agg(total_clients=(clients_col, 'sum'),\n",
    "    #            total_orders=(orders_col, 'sum'))\n",
    "    # )\n",
    "\n",
    "    # # Plot\n",
    "    # plt.figure(figsize=(11, 6))\n",
    "    # plt.plot(totals['month'], totals['total_clients'], label='Total active clients')\n",
    "    # if df[orders_col].sum() > 0:\n",
    "    #     plt.plot(totals['month'], totals['total_orders'], label='Total active orders')\n",
    "    # if hist_last_month is not None:\n",
    "    #     plt.axvline(hist_last_month, linestyle='--', linewidth=1, label='Last historical month')\n",
    "    # plt.title('Active caseloads over time: clients vs orders')\n",
    "    # plt.xlabel('Month'); plt.ylabel('Count'); plt.legend(); plt.tight_layout()\n",
    "    # path_total = os.path.join(output_dir, \"01_totals_clients_orders.png\")\n",
    "    # plt.savefig(path_total, dpi=180); plt.close()\n",
    "\n",
    "    # ========== 7) Total caseloads over time (line + 95% CIs) ==========\n",
    "    \n",
    "    totals = (\n",
    "        df.groupby('month', as_index=False)\n",
    "          .agg(total_clients=(clients_col, 'sum'),\n",
    "               total_orders=(orders_col, 'sum'))\n",
    "    )\n",
    "    \n",
    "    # Optional: start x-axis at axis_start_month if your function has that param\n",
    "    if 'axis_start_month' in locals() and axis_start_month is not None:\n",
    "        if not isinstance(axis_start_month, pd.Timestamp):\n",
    "            axis_start_month = pd.to_datetime(axis_start_month)\n",
    "        totals = totals[totals['month'] >= axis_start_month]\n",
    "    \n",
    "    # def _poisson_ci(series: pd.Series, z: float = 1.96):\n",
    "    #     \"\"\"95% CI for counts via Poisson approx: x ± z*sqrt(x), floored at 0.\"\"\"\n",
    "    #     x = series.to_numpy(dtype=float)\n",
    "    #     sd = np.sqrt(np.clip(x, 0, None))\n",
    "    #     lower = np.maximum(0, x - z * sd)\n",
    "    #     upper = x + z * sd\n",
    "    #     return lower, upper\n",
    "\n",
    "    def _poisson_ci(series: pd.Series, z: float = 1.96, phi: float = 1.0):\n",
    "        \"\"\"95% CI for counts via Poisson approx: x ± z*sqrt(x), floored at 0.\"\"\"\n",
    "        x = series.to_numpy(dtype=float)\n",
    "        sd = np.sqrt(phi * np.clip(x, 0, None))  # phi=1 → Poisson; phi>1 → over-dispersed\n",
    "        lower = np.maximum(0, x - z * sd)\n",
    "        upper = x + z * sd\n",
    "        return lower, upper\n",
    "\n",
    "    # Clients CI\n",
    "    c_lo, c_hi = _poisson_ci(totals['total_clients'])\n",
    "    \n",
    "    # Orders CI (only if any orders exist)\n",
    "    has_orders = (df[orders_col].sum() > 0)\n",
    "    if has_orders:\n",
    "        o_lo, o_hi = _poisson_ci(totals['total_orders'])\n",
    "    \n",
    "    plt.figure(figsize=(11, 6))\n",
    "    \n",
    "    # Plot CIs first so lines sit on top\n",
    "    plt.fill_between(totals['month'], c_lo, c_hi, alpha=0.2, label='95% CI (clients)', zorder=1)\n",
    "    if has_orders:\n",
    "        plt.fill_between(totals['month'], o_lo, o_hi, alpha=0.15, label='95% CI (orders)', zorder=1)\n",
    "    \n",
    "    # Now the lines\n",
    "    plt.plot(totals['month'], totals['total_clients'], label='Total active clients', zorder=2)\n",
    "    if has_orders:\n",
    "        plt.plot(totals['month'], totals['total_orders'], label='Total active orders', zorder=2)\n",
    "    \n",
    "    # Historical cutoff marker\n",
    "    if hist_last_month is not None:\n",
    "        plt.axvline(hist_last_month, linestyle='--', linewidth=1, label='Last historical month')\n",
    "    \n",
    "    plt.title('Active caseloads over time: clients vs orders (with 95% CIs)')\n",
    "    plt.xlabel('Month'); plt.ylabel('Count'); plt.legend(); plt.tight_layout()\n",
    "    \n",
    "    path_total = os.path.join(output_dir, \"01_totals_clients_orders.png\")\n",
    "    plt.savefig(path_total, dpi=180); plt.close()\n",
    "\n",
    "    # # ========== 2) Stacked area by age (clients, top_k) ==========\n",
    "    # # pick top_k age groups by average presence across period\n",
    "    # top_ages = (\n",
    "    #     df.groupby('age', as_index=False)[clients_col].mean()\n",
    "    #       .sort_values(clients_col, ascending=False)['age']\n",
    "    #       .head(top_k)\n",
    "    #       .tolist()\n",
    "    # )\n",
    "    # df_area = df.copy()\n",
    "    # df_area['age_area'] = np.where(df_area['age'].isin(top_ages), df_area['age'], 'Other')\n",
    "\n",
    "    # area_wide = (\n",
    "    #     df_area.groupby(['month', 'age_area'], as_index=False)[clients_col].sum()\n",
    "    #            .pivot(index='month', columns='age_area', values=clients_col)\n",
    "    #            .fillna(0)\n",
    "    # )\n",
    "\n",
    "    # plt.figure(figsize=(11, 6))\n",
    "    # # stack in deterministic order: top ages (descending by latest), then Other if present\n",
    "    # ordered_cols = [c for c in top_ages if c in area_wide.columns]\n",
    "    # if 'Other' in area_wide.columns:\n",
    "    #     ordered_cols = ordered_cols + ['Other']\n",
    "    # plt.stackplot(area_wide.index, area_wide[ordered_cols].T, labels=ordered_cols)\n",
    "    # if hist_last_month is not None:\n",
    "    #     plt.axvline(hist_last_month, linestyle='--', linewidth=1, label='Last historical month')\n",
    "    # plt.title(f'Active clients by age (stacked), top {top_k} groups')\n",
    "    # plt.xlabel('Month'); plt.ylabel('Active clients'); plt.legend(loc='upper left'); plt.tight_layout()\n",
    "    # path_area = os.path.join(output_dir, \"02_clients_stacked_area_by_age.png\")\n",
    "    # plt.savefig(path_area, dpi=180); plt.close()\n",
    "\n",
    "\n",
    "    # ===== 2) Stacked area by age (clients) — 5-year bands from 0, axis starts at start_month, CI band =====\n",
    "    # axis_start_month can be a string (e.g., \"2022-07\") or Timestamp; if None, keep all months\n",
    "    if axis_start_month is not None and not isinstance(axis_start_month, pd.Timestamp):\n",
    "        axis_start_month = pd.to_datetime(axis_start_month)\n",
    "    \n",
    "    # Filter from axis_start_month forward to avoid plotting pre-start zeros\n",
    "    df_band = df.copy()\n",
    "    if axis_start_month is not None:\n",
    "        df_band = df_band[df_band['month'] >= axis_start_month]\n",
    "    \n",
    "    # Parse ages to integers (robust to labels like \"70-74\" by taking the first number)\n",
    "    def _to_int_age(x):\n",
    "        s = str(x)\n",
    "        num = ''.join(ch for ch in s.split('-')[0] if ch.isdigit())\n",
    "        try:\n",
    "            return int(num)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    \n",
    "    df_band['age_int'] = df_band['age'].apply(_to_int_age)\n",
    "    df_band = df_band[df_band['age_int'].notna()].copy()\n",
    "    df_band['age_int'] = df_band['age_int'].astype(int)\n",
    "    \n",
    "    # Map to 5-year bands (START AT 0), cap high ages so the final band is 105–109 (covers 105+)\n",
    "    def _band_label(a, width=5, cap=109):\n",
    "        a = max(0, min(int(a), cap))\n",
    "        lo = (a // width) * width\n",
    "        hi = lo + width - 1\n",
    "        return f\"{lo:02d}-{hi:02d}\"\n",
    "    \n",
    "    df_band['age_band'] = df_band['age_int'].apply(_band_label)\n",
    "    \n",
    "    # Build a COMPLETE ordered list of bands from 0 up to the cap (ensures we start at 00-04)\n",
    "    _width = 5\n",
    "    _cap   = 109\n",
    "    full_bands = [f\"{lo:02d}-{lo+_width-1:02d}\" for lo in range(0, _cap + 1, _width)]  # 00-04, 05-09, ..., 105-109\n",
    "    \n",
    "    # Wide table: month x 5-year band (sum clients per band and month), then reindex to include all bands from 0\n",
    "    area_wide = (\n",
    "        df_band.groupby(['month', 'age_band'], as_index=False)[clients_col].sum()\n",
    "               .pivot(index='month', columns='age_band', values=clients_col)\n",
    "               .reindex(columns=full_bands, fill_value=0)   # <-- force presence from 00-04 upward\n",
    "               .fillna(0)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(11, 6))\n",
    "    # Plot in strict ascending band order from 00-04 upwards\n",
    "    plt.stackplot(area_wide.index, area_wide[full_bands].T, labels=full_bands)\n",
    "    \n",
    "    # Vertical line for last historical month if provided\n",
    "    if hist_last_month is not None:\n",
    "        plt.axvline(hist_last_month, linestyle='--', linewidth=1, label='Last historical month')\n",
    "    \n",
    "    # ---- Uncertainty band (95% Poisson CI) around the total clients series ----\n",
    "    totals_band = area_wide.sum(axis=1)\n",
    "    lower = np.maximum(0, totals_band - 1.96 * np.sqrt(np.clip(totals_band, a_min=0, a_max=None)))\n",
    "    upper = totals_band + 1.96 * np.sqrt(np.clip(totals_band, a_min=0, a_max=None))\n",
    "    plt.fill_between(area_wide.index, lower, upper, alpha=0.2, label='95% CI (total)')\n",
    "    \n",
    "    plt.title('Active clients by age (5-year bands from 0)')\n",
    "    plt.xlabel('Month'); plt.ylabel('Active clients')\n",
    "    plt.legend(loc='upper left', ncol=3)  # more columns so the legend fits many bands\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    path_area = os.path.join(output_dir, \"02_clients_stacked_area_5yr_bands_from0.png\")\n",
    "    plt.savefig(path_area, dpi=180); plt.close()\n",
    "\n",
    "\n",
    "    # ========== 3) Heatmap (clients) — 5-year age bands x month ==========\n",
    "\n",
    "    # 1) Parse 'age' to an integer (handles \"70\" or \"70-74\" by taking the left number)\n",
    "    def _to_int_age(x):\n",
    "        s = str(x)\n",
    "        left = s.split('-')[0]\n",
    "        num = ''.join(ch for ch in left if ch.isdigit())\n",
    "        try:\n",
    "            return int(num)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    \n",
    "    df_heat = df.copy()\n",
    "    df_heat['age_int'] = df_heat['age'].apply(_to_int_age)\n",
    "    df_heat = df_heat[df_heat['age_int'].notna()].copy()\n",
    "    df_heat['age_int'] = df_heat['age_int'].astype(int)\n",
    "    \n",
    "    # 2) Map to 5-year bands (cap at 109 so final band is 105–109 catching 105+)\n",
    "    def _band_label(a, width=5, cap=109):\n",
    "        a = max(0, min(int(a), cap))\n",
    "        lo = (a // width) * width\n",
    "        hi = lo + width - 1\n",
    "        return f\"{lo:02d}-{hi:02d}\"\n",
    "    \n",
    "    df_heat['age_band'] = df_heat['age_int'].apply(_band_label)\n",
    "    \n",
    "    # 3) Build band x month matrix (sum of clients per band/month)\n",
    "    heat = (\n",
    "        df_heat.pivot_table(index='age_band', columns='month', values=clients_col, aggfunc='sum')\n",
    "               .fillna(0)\n",
    "    )\n",
    "    \n",
    "    # 4) Sort age bands numerically by their lower bound\n",
    "    heat = heat.reindex(sorted(heat.index, key=lambda s: int(s.split('-')[0])))\n",
    "    \n",
    "    # 5) Plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.imshow(heat.values, aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar(label='Active clients')\n",
    "    \n",
    "    # y-axis: band labels\n",
    "    plt.yticks(ticks=np.arange(len(heat.index)), labels=heat.index)\n",
    "    \n",
    "    # x-axis: month labels (downsample to ~12 ticks for readability)\n",
    "    x_idx = np.arange(len(heat.columns))\n",
    "    step = max(1, len(heat.columns)//12)\n",
    "    plt.xticks(\n",
    "        ticks=x_idx[::step],\n",
    "        labels=[m.strftime('%Y-%m') for m in heat.columns][::step],\n",
    "        rotation=45, ha='right'\n",
    "    )\n",
    "    \n",
    "    plt.title('Heatmap: Active clients by 5-year age band and month')\n",
    "    plt.tight_layout()\n",
    "    path_heat = os.path.join(output_dir, \"03_heatmap_clients_ageband_month.png\")\n",
    "    plt.savefig(path_heat, dpi=180); plt.close()\n",
    "\n",
    "\n",
    "    # ========== 4) Top movers across forecast horizon (delta by age) ==========\n",
    "    # Determine anchor months for delta\n",
    "    # If we have a historical cutoff, compare last hist vs last overall; else earliest vs latest.\n",
    "    if hist_last_month is not None and (df['month'] <= hist_last_month).any():\n",
    "        m0 = df.loc[df['month'] <= hist_last_month, 'month'].max()\n",
    "    else:\n",
    "        m0 = df['month'].min()\n",
    "    m1 = df['month'].max()\n",
    "\n",
    "    # snap0 = df[df['month'] == m0].groupby('age', as_index=False)[clients_col, orders_col].sum()\n",
    "    # snap1 = df[df['month'] == m1].groupby('age', as_index=False)[clients_col, orders_col].sum()\n",
    "\n",
    "    cols_for_delta = list(dict.fromkeys([clients_col, orders_col]))  # de-dup, keep order\n",
    "    snap0 = df[df['month'] == m0].groupby('age', as_index=False)[cols_for_delta].sum()\n",
    "    snap1 = df[df['month'] == m1].groupby('age', as_index=False)[cols_for_delta].sum()\n",
    "\n",
    "    delta = snap1.merge(snap0, on='age', suffixes=('_end', '_start'), how='outer').fillna(0)\n",
    "    delta['delta_clients'] = delta[f'{clients_col}_end'] - delta[f'{clients_col}_start']\n",
    "    delta['delta_orders']  = delta[f'{orders_col}_end']  - delta[f'{orders_col}_start']\n",
    "\n",
    "    # Top increases & decreases for clients\n",
    "    inc_clients = delta.sort_values('delta_clients', ascending=False).head(top_k)\n",
    "    dec_clients = delta.sort_values('delta_clients', ascending=True).head(top_k)\n",
    "\n",
    "    # Plot clients movers (bar)\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    plt.bar(inc_clients['age'], inc_clients['delta_clients'], label='Increases')\n",
    "    plt.bar(dec_clients['age'], dec_clients['delta_clients'], label='Decreases')\n",
    "    plt.title(f'Top movers by age (clients): {m0:%Y-%m} → {m1:%Y-%m}')\n",
    "    plt.xlabel('Age'); plt.ylabel('Δ Active clients'); plt.legend(); plt.tight_layout()\n",
    "    path_movers_clients = os.path.join(output_dir, \"04_top_movers_clients.png\")\n",
    "    plt.savefig(path_movers_clients, dpi=180); plt.close()\n",
    "\n",
    "    # If orders present, do the same\n",
    "    if df[orders_col].sum() > 0:\n",
    "        inc_orders = delta.sort_values('delta_orders', ascending=False).head(top_k)\n",
    "        dec_orders = delta.sort_values('delta_orders', ascending=True).head(top_k)\n",
    "\n",
    "        plt.figure(figsize=(11, 6))\n",
    "        plt.bar(inc_orders['age'], inc_orders['delta_orders'], label='Increases')\n",
    "        plt.bar(dec_orders['age'], dec_orders['delta_orders'], label='Decreases')\n",
    "        plt.title(f'Top movers by age (orders): {m0:%Y-%m} → {m1:%Y-%m}')\n",
    "        plt.xlabel('Age'); plt.ylabel('Δ Active orders'); plt.legend(); plt.tight_layout()\n",
    "        path_movers_orders = os.path.join(output_dir, \"05_top_movers_orders.png\")\n",
    "        plt.savefig(path_movers_orders, dpi=180); plt.close()\n",
    "    else:\n",
    "        path_movers_orders = None\n",
    "\n",
    "    # ========== 5) Ratios & peaks ==========\n",
    "    # Ratio: orders per 100 clients (where clients > 0)\n",
    "    totals['orders_per_100_clients'] = np.where(\n",
    "        totals['total_clients'] > 0,\n",
    "        totals['total_orders'] * 100.0 / totals['total_clients'],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(11, 5))\n",
    "    plt.plot(totals['month'], totals['orders_per_100_clients'])\n",
    "    if hist_last_month is not None:\n",
    "        plt.axvline(hist_last_month, linestyle='--', linewidth=1, label='Last historical month')\n",
    "    plt.title('Orders per 100 clients (level & trend)')\n",
    "    plt.xlabel('Month'); plt.ylabel('Orders per 100 clients') \n",
    "    if hist_last_month is not None: plt.legend()\n",
    "    plt.tight_layout()\n",
    "    path_ratio = os.path.join(output_dir, \"06_orders_per_100_clients.png\")\n",
    "    plt.savefig(path_ratio, dpi=180); plt.close()\n",
    "\n",
    "    # Peak months\n",
    "    peak_clients_idx = totals['total_clients'].idxmax()\n",
    "    peak_orders_idx  = totals['total_orders'].idxmax() if df[orders_col].sum() > 0 else None\n",
    "    peak_clients_month = totals.loc[peak_clients_idx, 'month']\n",
    "    peak_orders_month  = totals.loc[peak_orders_idx,  'month'] if peak_orders_idx is not None else None\n",
    "\n",
    "    # ========== 6) Insights (markdown) ==========\n",
    "    hist_total_clients = totals.loc[totals['month'] == m0, 'total_clients'].sum() if (totals['month'] == m0).any() else np.nan\n",
    "    end_total_clients  = totals.loc[totals['month'] == m1, 'total_clients'].sum()\n",
    "    hist_total_orders  = totals.loc[totals['month'] == m0, 'total_orders' ].sum() if (totals['month'] == m0).any() else np.nan\n",
    "    end_total_orders   = totals.loc[totals['month'] == m1, 'total_orders' ].sum()\n",
    "\n",
    "    abs_change_clients = end_total_clients - hist_total_clients if pd.notna(hist_total_clients) else np.nan\n",
    "    pct_change_clients = (abs_change_clients / hist_total_clients * 100.0) if pd.notna(hist_total_clients) and hist_total_clients else np.nan\n",
    "\n",
    "    abs_change_orders  = end_total_orders - hist_total_orders if pd.notna(hist_total_orders) else np.nan\n",
    "    pct_change_orders  = (abs_change_orders / hist_total_orders * 100.0) if pd.notna(hist_total_orders) and hist_total_orders else np.nan\n",
    "\n",
    "    # Contribution of top movers (clients)\n",
    "    movers_clients = delta[['age', 'delta_clients']].sort_values('delta_clients', ascending=False)\n",
    "    pos_sum = movers_clients[movers_clients['delta_clients'] > 0]['delta_clients'].sum()\n",
    "    top_contrib = movers_clients.head(top_k)['delta_clients'].sum()\n",
    "    share_topk = (top_contrib / pos_sum * 100.0) if pos_sum else np.nan\n",
    "\n",
    "    insights_lines = [\n",
    "        \"# Deputyship caseload forecast — key insights\",\n",
    "        f\"- **Horizon compared:** {m0:%Y-%m} → {m1:%Y-%m}\",\n",
    "        f\"- **Total clients:** {int(end_total_clients):,} at end; change = {int(abs_change_clients):,} ({pct_change_clients:0.1f}%)\" if pd.notna(pct_change_clients) else f\"- **Total clients (end):** {int(end_total_clients):,}\",\n",
    "        f\"- **Total orders:** {int(end_total_orders):,} at end; change = {int(abs_change_orders):,} ({pct_change_orders:0.1f}%)\" if pd.notna(pct_change_orders) else f\"- **Total orders (end):** {int(end_total_orders):,}\",\n",
    "        f\"- **Peak clients month:** {peak_clients_month:%Y-%m} (value: {int(totals.loc[peak_clients_idx,'total_clients']):,})\",\n",
    "    ]\n",
    "    if peak_orders_month is not None:\n",
    "        insights_lines.append(f\"- **Peak orders month:** {peak_orders_month:%Y-%m} (value: {int(totals.loc[peak_orders_idx,'total_orders']):,})\")\n",
    "    if pd.notna(share_topk):\n",
    "        insights_lines.append(f\"- **Top {top_k} age groups account for ~{share_topk:0.1f}% of the positive change in clients.**\")\n",
    "\n",
    "    # List the top 5 client growers & decliners\n",
    "    top_incr = movers_clients.head(5)\n",
    "    top_decl = movers_clients.tail(5).sort_values('delta_clients')\n",
    "    insights_lines.append(\"\\n**Top 5 age increases (clients):** \" + \", \".join(f\"{a} (+{int(d):,})\" for a, d in zip(top_incr['age'], top_incr['delta_clients'])))\n",
    "    insights_lines.append(\"**Top 5 age decreases (clients):** \" + \", \".join(f\"{a} ({int(d):,})\" for a, d in zip(top_decl['age'], top_decl['delta_clients'])))\n",
    "\n",
    "    # Write markdown\n",
    "    insights_path = os.path.join(output_dir, \"00_forecast_insights.md\")\n",
    "    with open(insights_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(insights_lines))\n",
    "\n",
    "    # Console echo for quick read\n",
    "    print(\"\\n\".join(insights_lines))\n",
    "    print(f\"\\nSaved charts:\\n- {path_total}\\n- {path_area}\\n- {path_heat}\\n- {path_movers_clients}\\n\"\n",
    "          f\"{'- ' + path_movers_orders if path_movers_orders else ''}\\n- {path_ratio}\\nInsights → {insights_path}\")\n",
    "\n",
    "    # Return a small metrics dict if you want to log/store programmatically\n",
    "    return {\n",
    "        \"horizon_start\": m0,\n",
    "        \"horizon_end\": m1,\n",
    "        \"end_total_clients\": int(end_total_clients),\n",
    "        \"end_total_orders\":  int(end_total_orders),\n",
    "        \"abs_change_clients\": int(abs_change_clients) if pd.notna(abs_change_clients) else None,\n",
    "        \"pct_change_clients\": float(pct_change_clients) if pd.notna(pct_change_clients) else None,\n",
    "        \"abs_change_orders\":  int(abs_change_orders) if pd.notna(abs_change_orders) else None,\n",
    "        \"pct_change_orders\":  float(pct_change_orders) if pd.notna(pct_change_orders) else None,\n",
    "        \"peak_clients_month\": peak_clients_month,\n",
    "        \"peak_orders_month\":  peak_orders_month\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Run block for forecasting, final outputs, and charts\n",
    "\n",
    "The next code cell runs the forecasting and reporting steps that sit on top of the extracted data.\n",
    "\n",
    "It assumes that ages_df has already been created by the earlier extraction and feature engineering run. If you have not run that section yet, the commented extraction lines show what needs to be executed first.\n",
    "\n",
    "The first action is to run stop_flow_forecast. That produces a forecast by month and age group for the chosen number of periods.\n",
    "\n",
    "The cell then prepares two tables with a consistent schema.\n",
    "\n",
    "One table is the historical age specific table built from ages_df. Columns are renamed into a reporting friendly naming convention such as active_caseloads_clients and new_deputyships.\n",
    "\n",
    "The other table is the forecast table built from per_age_df. Its columns are renamed into the same reporting convention.\n",
    "\n",
    "These two tables are concatenated into one combined table, then a subset of the most important columns is selected. Month is formatted into a short label like Jan 25 for readability in charts and outputs.\n",
    "\n",
    "The combined historical and forecast table is saved as a csv file in the output folder. This file is the main handover artifact for downstream users.\n",
    "\n",
    "Finally, the cell calls the visualisation and insight function. It saves charts and an insights markdown file into the output folder. The hist_last_month argument controls where the function draws the boundary between historical and forecast data, and axis_start_month is used to avoid leading empty months on the x axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Deputyship forecasting model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_year = 2022\n",
    "    end_year = 2025\n",
    "    start_month = \"2024-12\"\n",
    "    end_month = \"2025-12\"\n",
    "    output_base=\"output\"\n",
    "\n",
    "    # We have already ran the section below earlier for data extraction step,\n",
    "    # if neccessary, un-comment the lines below:\n",
    "    \n",
    "    ## ------------------------------ Data Extraction and Engineeing----------------------------------##\n",
    "    # # Prepare output directory\n",
    "    # os.makedirs(output_base, exist_ok=True)\n",
    "    # # **Clear the output directory, not the Excel filepath**\n",
    "    # clear_directory(output_base)\n",
    "    \n",
    "    # combined_df, summary_df = export_monthly_reports(start_month, end_month)\n",
    "    # summary_df\n",
    "    # print(combined_df)\n",
    "    \n",
    "    # active_df, monthly_df, yearly_summary = calculate_monthly_active_cases(combined_df, start_month, end_month, output_base=\"output\")\n",
    "    # yearly_summary\n",
    "    # print(monthly_df)\n",
    "    # print(active_df)\n",
    "    \n",
    "\n",
    "    # # Calculate historical flows and age rates\n",
    "    # final_df, ages_df, summary_df, monthly_summary_df = calculate_yearonyear_flows_and_age_rates(\n",
    "    #      start_month, end_month,\n",
    "    #      redistribute_unknown_age=True)\n",
    "    \n",
    "    # print(summary_df)\n",
    "    # print(monthly_summary_df)\n",
    "    # print(ages_df)\n",
    "    # print(final_df)\n",
    "    ## --------------------------End of Data Extraction and Engineeing-------------------------------------## \n",
    "\n",
    "    \n",
    "    # Compute 2-year stop-flow forecast\n",
    "    per_age_df, monthly, yearly, df = stop_flow_forecast(ages_df, periods=12)    \n",
    "    \n",
    "    print(per_age_df)\n",
    "    print(monthly)\n",
    "    print(yearly)\n",
    "    print(df)\n",
    "\n",
    "    # Combine the historical data and forecasts\n",
    "    current_age_specific_deputyship_agg = ages_df.copy()\n",
    "    current_age_specific_deputyship_agg = current_age_specific_deputyship_agg.rename(\n",
    "        columns={\n",
    "            'age_group': 'age',\n",
    "            #'active_count': 'active_caseloads',\n",
    "            'entered': 'new_deputyships',\n",
    "            'terminations': 'terminated',\n",
    "            'active_clients_age': 'active_caseloads_clients',\n",
    "            'active_orders_age':  'active_caseloads_orders'\n",
    "\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    forecasted_age_specific_deputyship_agg = per_age_df.copy()\n",
    "    forecasted_age_specific_deputyship_agg = forecasted_age_specific_deputyship_agg.rename(\n",
    "        columns={\n",
    "            'age_group': 'age',\n",
    "            #'active_forecast': 'active_caseloads',\n",
    "            'active_clients_age_fc': 'active_caseloads_clients',\n",
    "            'active_orders_age_fc': 'active_caseloads_orders'\n",
    "        }\n",
    "    )\n",
    "    #forecasted_age_specific_deputyship_agg['month'] = pd.to_datetime(forecasted_age_specific_deputyship_agg['month'], format='%Y-%m')\n",
    "    #current_age_specific_deputyship_agg['month'] = pd.to_datetime(forecasted_age_specific_deputyship_agg['month'], format='%Y-%m')\n",
    "    # Final tforcast and actuals\n",
    "    combined_table = get_combined_age_deputyship_table(current_age_specific_deputyship_agg, forecasted_age_specific_deputyship_agg)\n",
    "    # Ensure 'month' is datetime\n",
    "    #combined_table['month'] = pd.to_datetime(combined_table['month'], format='%Y-%m')\n",
    "    final_deputyship_historical_forecasts = combined_table[['month', 'age', 'active_caseloads_clients', 'active_caseloads_orders', 'new_deputyships', 'terminated']]\n",
    "    final_deputyship_historical_forecasts['month'] = pd.to_datetime(final_deputyship_historical_forecasts['month']).dt.strftime(\"%b-%y\")\n",
    "    \n",
    "    # Save in CSV\n",
    "    final_deputyship_historical_forecasts.to_csv(f\"output/final_deputyship_historical_forecasts_{start_year}_{end_year}.csv\")\n",
    "    print(final_deputyship_historical_forecasts)\n",
    "\n",
    "\n",
    "    # Use end_month as the last historical month to draw a vertical line on charts\n",
    "    _ = visualize_and_analyze_deputyship_forecasts(\n",
    "        final_deputyship_historical_forecasts,\n",
    "        output_dir=output_base,\n",
    "        hist_last_month=pd.to_datetime(end_month),\n",
    "        axis_start_month=start_month,   # <-- important\n",
    "        top_k=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Optional plotting snippet kept as a reference\n",
    "\n",
    "This cell contains an older plotting approach that is currently commented out. It shows how to pivot the per age forecast table into a wide format with one column per age group and then plot each age group as its own line.\n",
    "\n",
    "It can be useful as a quick exploratory plot when you want to focus on a small number of ages, but it is not part of the main reporting pipeline because the dedicated visualisation function produces a fuller set of charts and a consistent set of saved files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtualisation: Plotting age-specific active caseloads, termination rate, and new deputyships over time\n",
    "\n",
    "# # Active Caseloads by Age Group\n",
    "# active_pivot = combined_table.pivot(index='month', columns='age', values='active_caseloads')\n",
    "    \n",
    "# # Pivot for plotting: month on x‐axis, each age a line\n",
    "# pivot = per_age_df.pivot(index='month', columns='age_group', values='active_forecast')\n",
    "# pivot.index = pd.to_datetime(pivot.index)\n",
    "    \n",
    "# # Plot\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# for age, series in pivot.items():\n",
    "#     ax.plot(\n",
    "#         series.index, series.values,\n",
    "#         label=f\"{int(age)} yrs\",\n",
    "#         marker='o',\n",
    "#         linewidth=2,\n",
    "#         alpha=0.8\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Placeholder cell\n",
    "\n",
    "This final cell is empty. It can be used for ad hoc checks, quick plots, or scratch calculations when you are validating a run. It is safe to leave it empty for normal notebook execution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
