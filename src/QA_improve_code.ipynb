{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# raw = pd.read_csv(\"data.csv\")  # dates are strings, no schema checks\n",
    "# dim_region = pd.read_csv(\"region_lookup.csv\")\n",
    "\n",
    "# 1) Reading data with no checks\n",
    "\n",
    "# Messy: pd.read_csv(\"data.csv\") and dates left as plain text.\n",
    "# Why it’s a problem: If columns change type (e.g., numbers come in as strings) or the date format shifts, the code happily carries on and gives you wrong results.\n",
    "# Fix: Parse dates strictly and enforce a schema so bad feeds fail fast.\n",
    "# What I did: Used pd.to_datetime(..., errors=\"raise\") and a Pandera schema with type, range, and allowed-values checks.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- AQUA: Data & Evidence — schema & joins ---\n",
    "import pandera as pa\n",
    "from pandera import Column, Check\n",
    "\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"id\": Column(int, Check.gt(0), nullable=False),\n",
    "    \"event_time\": Column(object, nullable=False),\n",
    "    \"age\": Column(float, Check.in_range(0,120)),\n",
    "    \"bmi\": Column(float, Check.gt(0), nullable=True),\n",
    "    \"region\": Column(str, nullable=True),\n",
    "    \"spend\": Column(float, Check.ge(0), nullable=False)\n",
    "})\n",
    "\n",
    "raw = schema.validate(\n",
    "    pd.read_csv(\"data.csv\").assign(\n",
    "        event_time=lambda d: pd.to_datetime(d[\"event_time\"], errors=\"raise\")\n",
    "    ),\n",
    "    lazy=True\n",
    ")\n",
    "dim_region = pd.read_csv(\"region_lookup.csv\")\n",
    "\n",
    "#--\n",
    "\n",
    "# # quick join\n",
    "# df = raw.merge(dim_region, on=\"region\", how=\"left\")  # could explode rows\n",
    "\n",
    "# 2) Blind join that might duplicate rows\n",
    "\n",
    "# Messy: merge(... how=\"left\") with no cardinality check.\n",
    "# Why it’s a problem: If the lookup has duplicates, your dataset silently grows and skews counts and model training.\n",
    "# Fix: Prove the join is one row per key.\n",
    "# What I did: validate=\"m:1\" and an assert on row counts so we catch row explosions immediately.\n",
    "# assert  m:1 join to avoid row explosion\n",
    "before = len(raw)\n",
    "df = raw.merge(dim_region, on=\"region\", how=\"left\", validate=\"m:1\")\n",
    "assert len(df) == before, \"Row explosion after join\"\n",
    "\n",
    "#--\n",
    "# target made from future info (leakage risk)\n",
    "df[\"high_spend_next_week\"] = df.groupby(\"id\")[\"spend\"].shift(-1) > 100\n",
    "y = df[\"high_spend_next_week\"].astype(int)\n",
    "\n",
    "# 3) Creating a “future” target then doing a random split\n",
    "\n",
    "# Messy: shift(-1) to label “next week” behaviour, then train_test_split randomly.\n",
    "# Why it’s a problem: You peek into the future to build the label but evaluate with a random split, letting future info leak into training. That inflates performance.\n",
    "# Fix: Respect time.\n",
    "# What I did: Sort by person and time, keep the “next week” target, then split by date (train on earlier period, test on later period).\n",
    "# --- AQUA: Methods & Assumptions — time-aware target & split ---\n",
    "df = df.sort_values([\"id\",\"event_time\"])\n",
    "df[\"y\"] = (df.groupby(\"id\")[\"spend\"].shift(-1) > 100).astype(int)\n",
    "\n",
    "#--\n",
    "# # drop NAs\n",
    "# df = df.dropna()\n",
    "\n",
    "# 4) Dropping all missing values\n",
    "\n",
    "# Messy: df.dropna() wipes any row with any missing field.\n",
    "# Why it’s a problem: You can throw away lots of useful data and introduce bias. Also you haven’t said what to do when new data has gaps.\n",
    "# Fix: Be surgical about missing data.\n",
    "# What I did: Only drop rows that can’t have a target, and impute feature gaps inside the modelling pipeline so the imputer learns from the training folds only.\n",
    "# drop rows without next-week target (end-of-series), but *log* the decision\n",
    "df = df.dropna(subset=[\"y\"])\n",
    "\n",
    "#--\n",
    "# # simple features\n",
    "# X = df[[\"age\", \"bmi\", \"region\", \"spend\"]]\n",
    "\n",
    "# # scale and encode BEFORE split (leakage)\n",
    "# scaler = StandardScaler()\n",
    "# X[[\"age\", \"bmi\", \"spend\"]] = scaler.fit_transform(X[[\"age\", \"bmi\", \"spend\"]])\n",
    "\n",
    "\n",
    "# 5) Preprocessing before the split\n",
    "\n",
    "# Messy: scaler.fit_transform and OneHotEncoder().fit_transform on the full dataset.\n",
    "# Why it’s a problem: The scaler and encoder learn from the test set too, which is another form of leakage.\n",
    "# Fix: Put all preprocessing in a pipeline.\n",
    "# What I did: Wrapped imputation, scaling and one-hot encoding in a ColumnTransformer inside a Pipeline, so they are fit only on the training folds during cross-validation.\n",
    "\n",
    "\n",
    "# enc = OneHotEncoder()\n",
    "# region_ohe = enc.fit_transform(X[[\"region\"]]).toarray()\n",
    "# X = pd.concat([X.drop(columns=[\"region\"]), pd.DataFrame(region_ohe)], axis=1)\n",
    "\n",
    "\n",
    "# 6) One-hot encoding that breaks on new categories\n",
    "\n",
    "# Messy: Default OneHotEncoder with manual toarray() and concat.\n",
    "# Why it’s a problem: New categories at prediction time can crash the model, and manual concatenation is brittle and error-prone.\n",
    "# Fix: Make encoding robust and automated.\n",
    "# What I did: OneHotEncoder(handle_unknown=\"ignore\") inside the pipeline; no manual concatenation.\n",
    "\n",
    "\n",
    "# # random split (not time-aware), no stratification\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "# 7) Random split on time-based data\n",
    "\n",
    "# Messy: train_test_split with default settings.\n",
    "# Why it’s a problem: For temporal problems, random splits overstate performance because the model sees patterns from the future.\n",
    "# Fix: Use time-aware validation.\n",
    "# What I did: Train/test split by a date cutoff and used TimeSeriesSplit for cross-validation on the training period.\n",
    "\n",
    "features = [\"age\",\"bmi\",\"region\",\"spend\"]\n",
    "X = df[features]\n",
    "y = df[\"y\"].astype(int)\n",
    "\n",
    "# create a time-aware split: last 20% of timeline as test\n",
    "cutoff = df[\"event_time\"].quantile(0.8)\n",
    "X_train, y_train = X[df[\"event_time\"] <= cutoff], y[df[\"event_time\"] <= cutoff]\n",
    "X_test,  y_test  = X[df[\"event_time\"] >  cutoff], y[df[\"event_time\"] >  cutoff]\n",
    "\n",
    "# --- AQUA: Leakage control — pipeline & proper CV ---\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "num = [\"age\",\"bmi\",\"spend\"]\n",
    "cat = [\"region\"]\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                          (\"sc\", StandardScaler())]), num),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "# 8) Default classifier with no imbalance handling\n",
    "\n",
    "# Messy: LogisticRegression() defaults and no seed.\n",
    "# Why it’s a problem: If positives are rare, the model can ignore them and still look “good.” Results may also vary run-to-run.\n",
    "# Fix: Make it stable and fairer to the minority class.\n",
    "# What I did: class_weight=\"balanced\", higher max_iter, and random_state=42.\n",
    "\n",
    "# # default logistic regression\n",
    "# clf = LogisticRegression()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # evaluate with accuracy on imbalanced data\n",
    "# pred = clf.predict(X_test)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "\n",
    "\n",
    "# 9) Using accuracy on an imbalanced target\n",
    "\n",
    "# Messy: accuracy_score as the only metric.\n",
    "# Why it’s a problem: With a 95/5 split, predicting “always negative” gives 95% accuracy but zero value.\n",
    "# Fix: Use the right metrics and show more than one view.\n",
    "# What I did: Report F1 and PR-AUC (precision-recall AUC), plus a classification report and confusion matrix.\n",
    "\n",
    "# time-aware CV on training only\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scorers = {\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"prauc\": make_scorer(average_precision_score, needs_proba=True)\n",
    "}\n",
    "cv_res = cross_validate(pipe, X_train, y_train, cv=tscv, scoring=scorers, return_train_score=False)\n",
    "print(\"CV F1 mean±sd:\", np.mean(cv_res[\"test_f1\"]), np.std(cv_res[\"test_f1\"]))\n",
    "print(\"CV PR-AUC mean±sd:\", np.mean(cv_res[\"test_prauc\"]), np.std(cv_res[\"test_prauc\"]))\n",
    "\n",
    "# fit on full training period and calibrate\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "cal = CalibratedClassifierCV(pipe, cv=3)\n",
    "cal.fit(X_train, y_train)\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "proba = cal.predict_proba(X_test)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "# choose threshold that maximises F1 (or cost-weighted objective)\n",
    "best_idx = np.argmax(2*prec*rec/(prec+rec+1e-9))\n",
    "y_pred = (proba >= thr[best_idx]).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# 10) No cross-validation or uncertainty\n",
    "\n",
    "# Messy: Single train/test split, one number.\n",
    "# Why it’s a problem: One split can be lucky. You don’t know how variable performance is.\n",
    "# Fix: Cross-validate and show variability.\n",
    "# What I did: cross_validate with TimeSeriesSplit, printing mean and standard deviation across folds.\n",
    "\n",
    "# 11) Untuned threshold and uncalibrated probabilities\n",
    "\n",
    "# Messy: Use default 0.5 threshold and raw probabilities.\n",
    "# Why it’s a problem: The probability scale might be off, and 0.5 rarely matches business costs.\n",
    "# Fix: Calibrate and pick a threshold that suits the goal.\n",
    "# What I did: CalibratedClassifierCV for well-scaled probabilities, then chose a threshold from the precision-recall curve to maximise F1 (or align to business costs).\n",
    "\n",
    "# 12) Quiet assumptions and no audit trail\n",
    "\n",
    "# Messy: Magic numbers, silent data decisions, no logging or checks.\n",
    "# Why it’s a problem: Future you (or an auditor) cannot tell why numbers changed or whether data feeds broke.\n",
    "# Fix: Make assumptions explicit and testable.\n",
    "# What I did: Added a schema, explicit join validation, deterministic seeds, and clear, reproducible steps. In practice you’d also keep a short Data Quality log and a model card.\n",
    "\n",
    "# 13) Performance and maintainability nits\n",
    "\n",
    "# Messy: Manual dense one-hot arrays and possible pandas “SettingWithCopy” assignments.\n",
    "# Why it’s a problem: It’s easy to create silent bugs and memory overhead.\n",
    "# Fix: Let the pipeline manage sparse features and transformations cleanly, avoiding copy warnings and keeping code compact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
