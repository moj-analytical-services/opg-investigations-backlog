{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One collective end-to-end demo\n",
    "\n",
    "def demo_all():\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    \n",
    "    from preprocessing import load_raw, engineer\n",
    "    from time_series import build_backlog_series, build_daily_panel\n",
    "    from interval_analysis import IntervalAnalysis, plot_pg_signoff_monthly_trends\n",
    "    from eda_opg import EDAConfig, OPGInvestigationEDA\n",
    "\n",
    "    raw, colmap = load_raw(Path(\"data/raw/raw.csv\"))\n",
    "    typed = engineer(raw, colmap)\n",
    "    \n",
    "    backlog = build_backlog_series(typed)\n",
    "\n",
    "    if \"backlog_available\" in backlog.columns and \"backlog\" not in backlog.columns:\n",
    "        backlog = backlog.rename(columns={\"backlog_available\": \"backlog\"})\n",
    "\n",
    "    daily, backlog_ts, events = build_daily_panel(typed)\n",
    "\n",
    "    di = IntervalAnalysis.build_interval_frame(typed, backlog_series=backlog_ts)\n",
    "    trend = IntervalAnalysis.monthly_trend(\n",
    "        di, metric=\"days_to_pg_signoff\", agg=\"median\", by=[\"case_type\"]\n",
    "    ).copy()\n",
    "    trend[\"month\"] = pd.to_datetime(trend[\"yyyymm\"] + \"-01\")\n",
    "    \n",
    "    cfg = EDAConfig(\n",
    "        id_col=\"case_id\",\n",
    "        date_received=\"dt_received_inv\",\n",
    "        date_allocated=\"dt_alloc_invest\",\n",
    "        date_signed_off=\"dt_pg_signoff\",\n",
    "    )\n",
    "    eda = OPGInvestigationEDA(typed, cfg)\n",
    "    overview = eda.quick_overview()\n",
    "\n",
    "    print(\"=== EDA OVERVIEW ===\")\n",
    "    print(overview)\n",
    "    print(\"\\n=== INTERVAL TREND HEAD ===\")\n",
    "    print(trend.head())\n",
    "\n",
    "    # Call your plotting function for the interval and trends\n",
    "    results = plot_pg_signoff_monthly_trends(di,\"data/out/plot/plots\")\n",
    "    \n",
    "    # Extract for returning\n",
    "    trend_all = results[\"trend_all\"]\n",
    "    \n",
    "    plot_paths = results[\"plots\"]\n",
    "    \n",
    "    # # Inspect returned objects if you want\n",
    "    # results[\"trend\"].tail()\n",
    "    # results[\"trend_all\"].tail()\n",
    "    # results[\"plots\"]\n",
    "    \n",
    "    return {\n",
    "        \"raw\": raw,\n",
    "        \"typed\": typed,\n",
    "        \"daily\": daily,\n",
    "        \"backlog\": backlog_ts,\n",
    "        \"events\": events,\n",
    "        \"di\": di,\n",
    "        \"trend\": trend,\n",
    "        \"overview\": overview,\n",
    "        \"trend_all\": trend_all,\n",
    "        \"plots\": plot_paths,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "from demo_pipeline import demo_all\n",
    "outputs = demo_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# demo_eda.py\n",
    "# Small, self-contained demo that exercises key methods on synthetic OPG-like data.\n",
    "\n",
    "import numpy as np  # numerical work (corr, quantiles)\n",
    "import pandas as pd  # core dataframe operations\n",
    "from eda_opg import EDAConfig, OPGInvestigationEDA\n",
    "\n",
    "# ----- 1) Create a small synthetic dataset for demonstration -----\n",
    "rng = np.random.default_rng(42)\n",
    "n = 2000\n",
    "\n",
    "# Base dates\n",
    "start = pd.Timestamp(\"2024-01-01\")\n",
    "recv_dates = start + pd.to_timedelta(rng.integers(0, 300, size=n), unit=\"D\")\n",
    "\n",
    "# Allocation occurs for ~85% within 1-30 days; else censored (NaT)\n",
    "alloc_delays = rng.integers(1, 31, size=n)\n",
    "allocated_mask = rng.random(size=n) < 0.85\n",
    "alloc_dates = pd.Series(recv_dates) + pd.to_timedelta(alloc_delays, unit=\"D\")\n",
    "alloc_dates = alloc_dates.where(allocated_mask, pd.NaT)\n",
    "\n",
    "# Sign-off for ~70% within 20-120 days from received; else censored\n",
    "signoff_delays = rng.integers(20, 121, size=n)\n",
    "so_mask = rng.random(size=n) < 0.70\n",
    "signoff_dates = pd.Series(recv_dates) + pd.to_timedelta(signoff_delays, unit=\"D\")\n",
    "signoff_dates = signoff_dates.where(so_mask, pd.NaT)\n",
    "\n",
    "# Categorical fields\n",
    "case_types = rng.choice([\"LPA\", \"Deputyship\", \"Other\"], size=n, p=[0.6, 0.3, 0.1])\n",
    "risk_band = rng.choice([\"Low\", \"Medium\", \"High\"], size=n, p=[0.5, 0.35, 0.15])\n",
    "teams = rng.choice([\"Team A\", \"Team B\", \"Team C\"], size=n, p=[0.4, 0.4, 0.2])\n",
    "region = rng.choice([\"North\", \"Midlands\", \"South\"], size=n)\n",
    "\n",
    "# Daily ops fields\n",
    "investigators_on_duty = rng.integers(8, 20, size=n)  # rough proxy\n",
    "allocations = rng.integers(0, 25, size=n)  # allocated on that day\n",
    "backlog = np.maximum(\n",
    "    0, 500 + rng.normal(0, 60, size=n).astype(int)\n",
    ")  # evolving backlog proxy\n",
    "\n",
    "# Target: legal review ~5%, with higher odds for High risk and longer allocation delay\n",
    "# We'll simulate it based on logits to mimic a real signal\n",
    "base_logit = -3.0 + 0.02 * np.nan_to_num(alloc_dates - recv_dates).astype(\n",
    "    \"timedelta64[D]\"\n",
    ").astype(float)\n",
    "risk_bump = np.select(\n",
    "    [risk_band == \"High\", risk_band == \"Medium\"], [1.2, 0.4], default=0.0\n",
    ")\n",
    "logit = base_logit + risk_bump\n",
    "prob = 1 / (1 + np.exp(-logit))\n",
    "legal_review = (rng.random(size=n) < prob).astype(int)\n",
    "\n",
    "# Assemble DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": np.arange(1, n + 1),\n",
    "        \"date_received_opg\": recv_dates,\n",
    "        \"date_allocated_investigator\": alloc_dates,\n",
    "        \"date_pg_signoff\": signoff_dates,\n",
    "        \"case_type\": case_types,\n",
    "        \"risk_band\": risk_band,\n",
    "        \"team\": teams,\n",
    "        \"region\": region,\n",
    "        \"investigators_on_duty\": investigators_on_duty,\n",
    "        \"allocations\": allocations,\n",
    "        \"backlog\": backlog,\n",
    "        \"legal_review\": legal_review,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ----- 2) Configure columns and instantiate the EDA toolkit -----\n",
    "cfg = EDAConfig(\n",
    "    id_col=\"id\",\n",
    "    date_received=\"date_received_opg\",\n",
    "    date_allocated=\"date_allocated_investigator\",\n",
    "    date_signed_off=\"date_pg_signoff\",\n",
    "    target_col=\"legal_review\",\n",
    "    numeric_cols=[\n",
    "        \"days_to_alloc\",\n",
    "        \"days_to_signoff\",\n",
    "        \"investigators_on_duty\",\n",
    "        \"allocations\",\n",
    "        \"backlog\",\n",
    "    ],\n",
    "    categorical_cols=[\"case_type\", \"risk_band\", \"team\", \"region\"],\n",
    "    time_index_col=\"date_received_opg\",\n",
    "    team_col=\"team\",\n",
    "    risk_col=\"risk_band\",\n",
    "    case_type_col=\"case_type\",\n",
    ")\n",
    "\n",
    "eda = OPGInvestigationEDA(df, cfg)\n",
    "\n",
    "# ----- 3) Run a few core EDA tasks (print or log these in practice) -----\n",
    "print(\"\\n== QUICK OVERVIEW ==\")\n",
    "print(eda.quick_overview())\n",
    "\n",
    "print(\"\\n== MISSINGNESS ==\")\n",
    "print(eda.missingness_matrix().head(10))\n",
    "print(\n",
    "    \"Missing 'days_to_signoff' vs target:\\n\", eda.missing_vs_target(\"days_to_signoff\")\n",
    ")\n",
    "\n",
    "print(\"\\n== OUTLIERS (days_to_signoff) ==\")\n",
    "print(eda.iqr_outliers(\"days_to_signoff\"))\n",
    "\n",
    "print(\"\\n== CATEGORICAL SUMMARY (case_type × risk_band) ==\")\n",
    "summary = eda.group_summary(\n",
    "    by=[\"case_type\", \"risk_band\"],\n",
    "    metrics={\n",
    "        \"n\": (\"id\", \"count\"),\n",
    "        \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "        \"med_alloc\": (\"days_to_signoff\", \"median\"),\n",
    "    },\n",
    ")\n",
    "print(summary.head(12))\n",
    "\n",
    "print(\"\\n== NUMERIC CORRELATIONS (Spearman) ==\")\n",
    "print(eda.numeric_correlations(\"spearman\"))\n",
    "\n",
    "print(\"\\n== REDUNDANCY DROP LIST (|r|>0.9) ==\")\n",
    "print(eda.redundancy_drop_list())\n",
    "\n",
    "print(\"\\n== CLASS IMBALANCE ==\")\n",
    "print(eda.imbalance_summary())\n",
    "\n",
    "print(\"\\n== LEAKAGE SCAN ==\")\n",
    "print(eda.leakage_scan([\"post\", \"signed\", \"decision\", \"outcome\"]))\n",
    "\n",
    "print(\"\\n== INTERACTION: risk_band × binned days_to_signoff -> legal_review rate ==\")\n",
    "print(eda.binned_interaction_rate(\"days_to_signoff\", \"risk_band\"))\n",
    "\n",
    "print(\"\\n== RESAMPLED TIME SERIES (daily) ==\")\n",
    "ts = eda.resample_time_series(\n",
    "    {\n",
    "        \"backlog\": (\"backlog\", \"last\"),\n",
    "        \"inv_mean\": (\"investigators_on_duty\", \"mean\"),\n",
    "    }\n",
    ")\n",
    "print(ts.tail())\n",
    "\n",
    "print(\"\\n== LAG CORRELATIONS: backlog vs inv_mean ==\")\n",
    "print(eda.lag_correlations(ts[\"backlog\"], ts[\"inv_mean\"]))\n",
    "\n",
    "print(\"\\n== KM QUANTILES by risk_band (signoff) ==\")\n",
    "print(eda.km_quantiles_by_group(\"days_to_signoff\", \"event_signed_off\", \"risk_band\"))\n",
    "\n",
    "print(\"\\n== MONTHLY KPIs by team ==\")\n",
    "print(eda.monthly_kpis().head(12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "!python -m demo_eda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_pipline\n",
    "# run a preprocessing + time-series demo\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from preprocessing import load_raw, engineer\n",
    "from time_series import (\n",
    "    build_event_log,\n",
    "    build_wip_series,\n",
    "    build_backlog_series,\n",
    "    build_daily_panel,\n",
    "    summarise_daily_panel,\n",
    ")\n",
    "\n",
    "# 1) Load raw data and engineer typed table\n",
    "raw_path = Path(\"data/raw/raw.csv\")  # adjust if needed\n",
    "raw, colmap = load_raw(raw_path)\n",
    "typed = engineer(raw, colmap)\n",
    "\n",
    "# 2) Build core time-series artefacts\n",
    "events = build_event_log(typed)\n",
    "wip = build_wip_series(typed)\n",
    "backlog = build_backlog_series(typed)\n",
    "\n",
    "daily, backlog_ts, events_ts = build_daily_panel(\n",
    "    typed,\n",
    "    start=None,\n",
    "    end=None,\n",
    "    exclude_weekends=True,\n",
    "    holidays=None,\n",
    "    pad_days=14,\n",
    "    backlog_freq=\"W-FRI\",\n",
    ")\n",
    "\n",
    "# 3) Aggregate to team-level daily and weekly\n",
    "team_daily = summarise_daily_panel(daily, by=[\"date\", \"team\"])\n",
    "team_weekly = summarise_daily_panel(daily, by=[\"date\", \"team\"], freq=\"W-FRI\")\n",
    "\n",
    "print(team_daily.head())\n",
    "print(team_weekly.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m demo_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IntervalAnalysis demo\n",
    "\n",
    "from interval_analysis import IntervalAnalysis\n",
    "\n",
    "# 1) Build interval frame (one row per staff × date with backlog, events, flags)\n",
    "di = IntervalAnalysis.build_interval_frame(\n",
    "    typed,\n",
    "    backlog_series=backlog_ts,   # expects ['date', 'backlog'] style columns\n",
    "    bank_holidays=None,\n",
    ")\n",
    "\n",
    "print(di.head())\n",
    "\n",
    "# 2) Monthly trend in days to PG signoff by case_type\n",
    "trend = IntervalAnalysis.monthly_trend(\n",
    "    di,\n",
    "    metric=\"days_to_pg_signoff\",\n",
    "    agg=\"median\",\n",
    "    by=[\"case_type\"],\n",
    ")\n",
    "print(trend.head())\n",
    "\n",
    "# 3) Volatility score (e.g., week-on-week variation by team)\n",
    "vol = IntervalAnalysis.volatility_score(\n",
    "    di,\n",
    "    metric=\"days_to_pg_signoff\",\n",
    "    freq=\"W\",\n",
    "    by=[\"team\"],\n",
    ")\n",
    "print(vol.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an EDA demo on real data\n",
    "\n",
    "from eda_opg import EDAConfig, OPGInvestigationEDA\n",
    "\n",
    "cfg = EDAConfig(\n",
    "    id_col=\"case_id\",\n",
    "    date_received=\"dt_received_inv\",   # or dt_received_opg, depending on your engineered columns\n",
    "    date_allocated=\"dt_alloc_invest\",\n",
    "    date_signed_off=\"dt_pg_signoff\",\n",
    "    target_col=None,                  # or \"legal_review\" if you have it\n",
    "    numeric_cols=[\n",
    "        \"days_to_pg_signoff\",\n",
    "        \"wip_load\",\n",
    "        \"backlog\",\n",
    "    ],\n",
    "    categorical_cols=[\"team\", \"case_type\", \"concern_type\"],\n",
    "    time_index_col=\"date\",            # 'date' from IntervalAnalysis.build_interval_frame\n",
    "    team_col=\"team\",\n",
    "    risk_col=None,\n",
    "    case_type_col=\"case_type\",\n",
    ")\n",
    "\n",
    "eda = OPGInvestigationEDA(di, cfg)\n",
    "\n",
    "print(\"== QUICK OVERVIEW ==\")\n",
    "print(eda.quick_overview())\n",
    "\n",
    "print(\"\\n== MISSINGNESS ==\")\n",
    "print(eda.missingness_matrix().head())\n",
    "\n",
    "print(\"\\n== MONTHLY KPIs ==\")\n",
    "print(eda.monthly_kpis().head())\n",
    "\n",
    "# You can extend this with the other methods: imbalance_summary, leakage_scan, km_quantiles_by_group, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
