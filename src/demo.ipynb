{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Git\n",
    "\n",
    "## if failed in black (commit):\n",
    "- ignore the failing pre-commit hooks / local checks and just commit + push to my branch”, \n",
    "- bypass them — as long as your remote (GitHub/GitLab) doesn’t block pushes via branch protection.\n",
    "    - Commit without running hooks (pre-commit / git hooks)\n",
    "        git commit -m \"WIP: push despite failing hooks\" --no-verify\n",
    "    - Push without running pre-push hooks\n",
    "        git push origin <your-branch> --no-verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# small safety helpers\n",
    "# --------------------------\n",
    "def _existing_cols(df, cols):\n",
    "    \"\"\"Return only columns that exist (prevents KeyError).\"\"\"\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "def _to_numeric(s):\n",
    "    \"\"\"Convert pandas scalars/NAType/object -> float with NaN for bad values.\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _ensure_numeric_cols(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _ensure_int_flag(df, col):\n",
    "    \"\"\"Make 0/1 columns numeric so .mean() works (fixes category dtype mean error).\"\"\"\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "# --------------------------\n",
    "# tidy interval stats (overall OR by group)\n",
    "# --------------------------\n",
    "_PCTS = [(0.10, \"p10\"), (0.25, \"p25\"), (0.50, \"p50\"), (0.75, \"p75\"), (0.90, \"p90\")]\n",
    "\n",
    "def interval_summary(df, metrics, group_cols=None):\n",
    "    \"\"\"\n",
    "    Returns tidy table:\n",
    "    - overall: index=metric\n",
    "    - grouped: columns=[*group_cols, metric, count, mean, std, min, p10, p25, p50, p75, p90, max]\n",
    "    \"\"\"\n",
    "    metrics = _existing_cols(df, metrics)\n",
    "    if not metrics:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # make sure metrics are numeric\n",
    "    df = df.copy()\n",
    "    df = _ensure_numeric_cols(df, metrics)\n",
    "\n",
    "    def _describe(s):\n",
    "        s = s.dropna()\n",
    "        if s.empty:\n",
    "            return {\"count\": 0}\n",
    "        out = {\n",
    "            \"count\": int(s.shape[0]),\n",
    "            \"mean\": float(s.mean()),\n",
    "            \"std\": float(s.std(ddof=1)),\n",
    "            \"min\": float(s.min()),\n",
    "            \"max\": float(s.max()),\n",
    "        }\n",
    "        qs = s.quantile([p for p, _ in _PCTS])\n",
    "        for p, name in _PCTS:\n",
    "            out[name] = float(qs.loc[p])\n",
    "        return out\n",
    "\n",
    "    if not group_cols:\n",
    "        rows = []\n",
    "        for m in metrics:\n",
    "            d = _describe(df[m])\n",
    "            d[\"metric\"] = m\n",
    "            rows.append(d)\n",
    "        out = pd.DataFrame(rows).set_index(\"metric\")\n",
    "        # enforce consistent columns ordering where present\n",
    "        cols_order = [\"count\",\"mean\",\"std\",\"min\",\"p10\",\"p25\",\"p50\",\"p75\",\"p90\",\"max\"]\n",
    "        return out.reindex(columns=[c for c in cols_order if c in out.columns])\n",
    "\n",
    "    group_cols = [c for c in group_cols if c in df.columns]\n",
    "    if not group_cols:\n",
    "        return interval_summary(df, metrics, group_cols=None)\n",
    "\n",
    "    out_rows = []\n",
    "    for keys, sub in df.groupby(group_cols, dropna=False):\n",
    "        if not isinstance(keys, tuple):\n",
    "            keys = (keys,)\n",
    "        key_dict = dict(zip(group_cols, keys))\n",
    "        for m in metrics:\n",
    "            d = _describe(sub[m])\n",
    "            out_rows.append({**key_dict, \"metric\": m, **d})\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    cols_order = group_cols + [\"metric\",\"count\",\"mean\",\"std\",\"min\",\"p10\",\"p25\",\"p50\",\"p75\",\"p90\",\"max\"]\n",
    "    return out.reindex(columns=[c for c in cols_order if c in out.columns])\n",
    "\n",
    "# --------------------------\n",
    "# plots\n",
    "# --------------------------\n",
    "def plot_metric_histograms(df, metric, group_col=None, outpath=None, bins=40):\n",
    "    \"\"\"Histogram overall, or small multiples via looping (keeps it readable).\"\"\"\n",
    "    if metric not in df.columns:\n",
    "        return None\n",
    "    x = pd.to_numeric(df[metric], errors=\"coerce\").dropna()\n",
    "    if x.empty:\n",
    "        return None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.hist(x, bins=bins)\n",
    "    ax.set_title(f\"Distribution: {metric}\" + (\" (overall)\" if not group_col else \"\"))\n",
    "    ax.set_xlabel(\"Days\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    fig.tight_layout()\n",
    "    if outpath:\n",
    "        fig.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "    return outpath\n",
    "\n",
    "def plot_group_median_bar(summary_df, group_col, metric, outpath=None):\n",
    "    \"\"\"Bar chart of median by group for ONE metric (tidy + interpretable).\"\"\"\n",
    "    if summary_df.empty:\n",
    "        return None\n",
    "    sdf = summary_df.copy()\n",
    "    sdf = sdf[(sdf[\"metric\"] == metric)].copy()\n",
    "    if group_col not in sdf.columns or \"p50\" not in sdf.columns:\n",
    "        return None\n",
    "\n",
    "    sdf[\"p50\"] = pd.to_numeric(sdf[\"p50\"], errors=\"coerce\")\n",
    "    sdf = sdf.dropna(subset=[\"p50\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.bar(sdf[group_col].astype(str), sdf[\"p50\"].astype(float))\n",
    "    ax.set_title(f\"Median {metric} by {group_col}\")\n",
    "    ax.set_ylabel(\"Days (median)\")\n",
    "    ax.set_xlabel(group_col)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    fig.tight_layout()\n",
    "    if outpath:\n",
    "        fig.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "    return outpath\n",
    "\n",
    "def pickup_rule_tables(di, wip_col=\"wip_band\", gap_col=\"gap_band\", event_col=\"event_newcase\"):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      prob: P(new case today | wip_band, gap_band)\n",
    "      counts: staff-days per cell\n",
    "    \"\"\"\n",
    "    needed = _existing_cols(di, [wip_col, gap_col, event_col])\n",
    "    if len(needed) < 3:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    tmp = di[[wip_col, gap_col, event_col]].copy()\n",
    "    tmp[event_col] = pd.to_numeric(tmp[event_col], errors=\"coerce\")  # make mean work\n",
    "    prob = tmp.groupby([wip_col, gap_col], dropna=False)[event_col].mean().unstack(gap_col)\n",
    "    counts = tmp.groupby([wip_col, gap_col], dropna=False)[event_col].size().unstack(gap_col)\n",
    "    return prob, counts\n",
    "\n",
    "def plot_prob_heatmap(prob, outpath=None, title=\"P(new case | workload, gap)\"):\n",
    "    \"\"\"Matplotlib heatmap; forces float dtype to avoid 'dtype object' imshow error.\"\"\"\n",
    "    if prob.empty:\n",
    "        return None\n",
    "    prob2 = prob.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    arr = prob2.to_numpy(dtype=float)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "    im = ax.imshow(arr, aspect=\"auto\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_yticks(range(len(prob2.index)))\n",
    "    ax.set_yticklabels([str(i) for i in prob2.index])\n",
    "    ax.set_xticks(range(len(prob2.columns)))\n",
    "    ax.set_xticklabels([str(c) for c in prob2.columns], rotation=45, ha=\"right\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.02, pad=0.03)\n",
    "    fig.tight_layout()\n",
    "    if outpath:\n",
    "        fig.savefig(outpath, dpi=200)\n",
    "    plt.show()\n",
    "    return outpath\n",
    "\n",
    "# --------------------------\n",
    "# THE wrapper\n",
    "# --------------------------\n",
    "def run_interval_outputs(\n",
    "    typed,\n",
    "    di,\n",
    "    group_col=\"case_type\",\n",
    "    outdir=\"data/out/plot/plots\",\n",
    "    interval_metrics=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces your hard-coded 'by case_type' section.\n",
    "    Call it with group_col='case_type' OR group_col='application_type' etc.\n",
    "    \"\"\"\n",
    "    outdir = Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # choose metrics you want in the interval distribution table\n",
    "    if interval_metrics is None:\n",
    "        interval_metrics = [\"days_to_pg_signoff\", \"days_alloc_to_close\", \"inter_pickup_days\"]\n",
    "\n",
    "    # ---------- interval distributions (overall) ----------\n",
    "    overall = interval_summary(di, interval_metrics, group_cols=None)\n",
    "    display(overall)\n",
    "\n",
    "    # save tidy table\n",
    "    overall.to_csv(outdir / \"interval_summary_overall.csv\")\n",
    "\n",
    "    # simple histogram for the key metrics\n",
    "    for m in _existing_cols(di, interval_metrics):\n",
    "        plot_metric_histograms(di, m, outpath=outdir / f\"hist_{m}_overall.png\")\n",
    "\n",
    "    # ---------- interval distributions (by chosen group) ----------\n",
    "    by_group = interval_summary(di, interval_metrics, group_cols=[group_col])\n",
    "    display(by_group.head(20))\n",
    "    by_group.to_csv(outdir / f\"interval_summary_by_{group_col}.csv\", index=False)\n",
    "\n",
    "    # a readable bar chart: median inter-pickup gap by group (or swap metric)\n",
    "    if \"inter_pickup_days\" in by_group[\"metric\"].unique():\n",
    "        plot_group_median_bar(by_group, group_col=group_col, metric=\"inter_pickup_days\",\n",
    "                              outpath=outdir / f\"median_inter_pickup_days_by_{group_col}.png\")\n",
    "\n",
    "    # ---------- rules table: P(new case | wip, gap) ----------\n",
    "    prob, counts = pickup_rule_tables(di)\n",
    "    display(prob)\n",
    "    display(counts)\n",
    "    prob.to_csv(outdir / \"pickup_prob_matrix.csv\")\n",
    "    counts.to_csv(outdir / \"pickup_counts_matrix.csv\")\n",
    "\n",
    "    plot_prob_heatmap(prob, outpath=outdir / \"pickup_prob_heatmap.png\")\n",
    "\n",
    "    # OPTIONAL: bar chart for Low workload only (fixes your NAType bar error)\n",
    "    if not prob.empty and \"Low\" in prob.index:\n",
    "        rules_low = prob.loc[\"Low\"].copy()\n",
    "        rules_low = pd.to_numeric(rules_low, errors=\"coerce\").dropna()\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.bar(rules_low.index.astype(str), rules_low.values.astype(float))\n",
    "        ax.set_ylabel(\"P(new case today)\")\n",
    "        ax.set_title(\"Probability of new case vs gap since last pickup\\n(Low workload band)\")\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(outdir / \"pickup_prob_low_bar.png\", dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"overall\": overall,\n",
    "        \"by_group\": by_group,\n",
    "        \"pickup_prob\": prob,\n",
    "        \"pickup_counts\": counts,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Generate distributions for ALL metrics (overall + breakdowns)\n",
    "# Helper: filter + tidy \n",
    "# (creates a metric column so you don’t hit KeyError: 'metric')\n",
    "# --------------------------\n",
    "\n",
    "def _filter_metrics(d: dict, metrics: list[str]) -> dict:\n",
    "    \"\"\"Keep only keys we care about AND that exist in the output dict.\"\"\"\n",
    "    return {m: d[m] for m in metrics if m in d}\n",
    "\n",
    "def tidy_overall(interval_dists_overall: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    interval_dists_overall is: {metric: stats_dict}\n",
    "    -> DataFrame with a 'metric' column\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(interval_dists_overall).T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "    return df\n",
    "\n",
    "def tidy_by(interval_dists_by: dict, by_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    interval_dists_by is: {metric: {group_key_tuple: stats_dict}}\n",
    "    -> long DataFrame with columns: by_cols + ['metric'] + stats\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for metric, groups in interval_dists_by.items():\n",
    "        for gkey, stats in groups.items():\n",
    "            if not isinstance(gkey, tuple):\n",
    "                gkey = (gkey,)\n",
    "            row = {\"metric\": metric}\n",
    "            for col, val in zip(by_cols, gkey):\n",
    "                row[col] = val\n",
    "            row.update(stats)\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# “school holiday season” label (Dec/Jan, Apr, Jul/Aug)\n",
    "# --------------------------\n",
    "# “multi-week” gaps in new case starts / pickups are very plausibly driven by \n",
    "# (a) people being off the system for chunks of time (term-time contracts, annual leave, sickness, training), \n",
    "# and (b) school holiday blocks (Christmas, Easter-ish, summer). \n",
    "# Test this directly with a couple of lightweight additions to your existing notebook: \n",
    "# (1) tag each pickup/gap with a “holiday season” label, \n",
    "# (2) infer likely term-time workers from repeated summer/Christmas inactivity, \n",
    "# and (3) add a 3-month rolling average to smooth month-to-month volatility.\n",
    "# long gaps are “people not starting cases during holidays”:\n",
    "#     - higher inter_pickup_days ending in Jan (post-Christmas),\n",
    "#     - higher gaps around Apr,\n",
    "#     - higher gaps around Sep or late Aug (post-summer).  \n",
    "\n",
    "def school_holiday_season(d: pd.Timestamp) -> str:\n",
    "    \"\"\"\n",
    "    Coarse UK school-holiday seasons (not region-specific dates):\n",
    "    - Christmas: mid-Dec to early Jan\n",
    "    - Easter: April (approx)\n",
    "    - Summer: Jul/Aug\n",
    "    Everything else: Term-time/Other\n",
    "    \"\"\"\n",
    "    if pd.isna(d):\n",
    "        return \"__NA__\"\n",
    "    d = pd.Timestamp(d)\n",
    "\n",
    "    # Christmas window spanning year boundary\n",
    "    if (d.month == 12 and d.day >= 15) or (d.month == 1 and d.day <= 10):\n",
    "        return \"Christmas\"\n",
    "    # Easter-ish (coarse; you can refine later)\n",
    "    if d.month == 4:\n",
    "        return \"Easter\"\n",
    "    # Summer holidays\n",
    "    if d.month in (7, 8):\n",
    "        return \"Summer\"\n",
    "\n",
    "    return \"Other/Term\"\n",
    "\n",
    "def add_holiday_flags(df: pd.DataFrame, date_col: str = \"date\") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    out[\"holiday_season\"] = out[date_col].apply(school_holiday_season)\n",
    "    out[\"is_school_holiday_season\"] = out[\"holiday_season\"].isin([\"Christmas\", \"Easter\", \"Summer\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Infer “term-time workers” (so they don’t inflate full-time medians)\n",
    "# --------------------------\n",
    "# This is worthwhile if term-time staff are recorded with FTE≈1 during on-strength weeks, \n",
    "# because your “Full-time” bucket will pick up their holiday gaps and inflate the median gap\n",
    "\n",
    "# def infer_term_time_workers(\n",
    "#     typed: pd.DataFrame,\n",
    "#     staff_col: str = \"staff_id\",\n",
    "#     alloc_col: str = \"dt_alloc_invest\",\n",
    "#     fte_col: str = \"fte\",\n",
    "#     min_cases_per_year: int = 8,\n",
    "#     years_required: int = 2,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Heuristic:\n",
    "#     - Consider each staff-year where they have enough allocations (>= min_cases_per_year)\n",
    "#     - If they have *zero* allocations in Jul/Aug in that year, count that as \"summer off\"\n",
    "#     - If \"summer off\" happens in >= years_required distinct years, mark as term-time-like\n",
    "#     \"\"\"\n",
    "#     df = typed[[staff_col, alloc_col, fte_col]].copy()\n",
    "#     df[alloc_col] = pd.to_datetime(df[alloc_col], errors=\"coerce\")\n",
    "#     df = df.dropna(subset=[staff_col, alloc_col])\n",
    "\n",
    "#     df[\"year\"] = df[alloc_col].dt.year\n",
    "#     df[\"month\"] = df[alloc_col].dt.month\n",
    "\n",
    "#     per_year_total = df.groupby([staff_col, \"year\"]).size().rename(\"n_allocs\").reset_index()\n",
    "#     per_year_summer = (\n",
    "#         df[df[\"month\"].isin([7, 8])]\n",
    "#         .groupby([staff_col, \"year\"]).size()\n",
    "#         .rename(\"n_summer_allocs\")\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     per_year = per_year_total.merge(per_year_summer, on=[staff_col, \"year\"], how=\"left\")\n",
    "#     per_year[\"n_summer_allocs\"] = per_year[\"n_summer_allocs\"].fillna(0)\n",
    "\n",
    "#     # only judge years with enough activity\n",
    "#     per_year = per_year[per_year[\"n_allocs\"] >= min_cases_per_year].copy()\n",
    "#     per_year[\"summer_off\"] = per_year[\"n_summer_allocs\"].eq(0)\n",
    "\n",
    "#     staff_flag = (\n",
    "#         per_year.groupby(staff_col)[\"summer_off\"].sum()\n",
    "#         .rename(\"years_summer_off\")\n",
    "#         .reset_index()\n",
    "#     )\n",
    "#     staff_flag[\"term_time_inferred\"] = staff_flag[\"years_summer_off\"] >= years_required\n",
    "\n",
    "#     # Attach a stable staff FTE estimate (median)\n",
    "#     staff_fte = typed.groupby(staff_col)[fte_col].median().rename(\"fte_median\").reset_index()\n",
    "#     out = staff_flag.merge(staff_fte, on=staff_col, how=\"left\")\n",
    "\n",
    "#     # final label\n",
    "#     def label_row(r):\n",
    "#         if bool(r.get(\"term_time_inferred\", False)):\n",
    "#             return \"Term-time (inferred)\"\n",
    "#         if pd.isna(r.get(\"fte_median\")):\n",
    "#             return \"__NA__\"\n",
    "#         return \"Full-time\" if r[\"fte_median\"] >= 0.9 else \"Part-time\"\n",
    "\n",
    "#     out[\"staff_work_pattern\"] = out.apply(label_row, axis=1)\n",
    "#     return out[[staff_col, \"fte_median\", \"years_summer_off\", \"term_time_inferred\", \"staff_work_pattern\"]]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3-month rolling average to smooth volatility (Dec↔Jan etc.)\n",
    "# --------------------------\n",
    "def add_rolling_avg(series: pd.Series, window: int = 3) -> pd.Series:\n",
    "    return series.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# monthly counts from 2020 onwards\n",
    "# --------------------------\n",
    "# cases received (dt_received_inv)\n",
    "# cases allocated (dt_alloc_invest)\n",
    "# cases PG sign-off (dt_pg_signoff)\n",
    "# cases closed (dt_close)\n",
    "\n",
    "\n",
    "CASE_FLOW_DATE_COLS = {\n",
    "    \"received\": \"dt_received_inv\",\n",
    "    \"allocated\": \"dt_alloc_invest\",\n",
    "    \"pg_signoff\": \"dt_pg_signoff\",\n",
    "    \"closed\": \"dt_close\",\n",
    "}\n",
    "\n",
    "def monthly_case_flow_counts(\n",
    "    typed: pd.DataFrame,\n",
    "    start: str = \"2020-01-01\",\n",
    "    end: str = \"2025-10-31\",\n",
    "    date_cols: dict = CASE_FLOW_DATE_COLS,\n",
    "    case_id_col: str = \"case_id\",\n",
    "    distinct_cases: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a wide monthly table with counts for each event type:\n",
    "      month | received | allocated | pg_signoff | closed\n",
    "    By default counts DISTINCT case_id per month per event.\n",
    "    Set distinct_cases=False to count rows/events instead.\n",
    "    \"\"\"\n",
    "    df = typed.copy()\n",
    "    start_ts = pd.Timestamp(start)\n",
    "    end_ts = pd.Timestamp(end)\n",
    "    \n",
    "    # Ensure datetime\n",
    "    for _, col in date_cols.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    out = None\n",
    "\n",
    "    for label, col in date_cols.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        sub = (\n",
    "            df[[case_id_col, col]]\n",
    "            .dropna(subset=[col])\n",
    "            .loc[lambda x: ((x[col] >= start_ts) & (x[col] <= end_ts))]\n",
    "            .assign(month=lambda x: x[col].dt.to_period(\"M\").dt.to_timestamp())\n",
    "        )\n",
    "\n",
    "        if distinct_cases:\n",
    "            s = sub.groupby(\"month\")[case_id_col].nunique()\n",
    "        else:\n",
    "            s = sub.groupby(\"month\")[case_id_col].size()\n",
    "\n",
    "        tmp = s.rename(label).reset_index()\n",
    "        out = tmp if out is None else out.merge(tmp, on=\"month\", how=\"outer\")\n",
    "\n",
    "    if out is None:\n",
    "        return pd.DataFrame(columns=[\"month\"] + list(date_cols.keys()))\n",
    "\n",
    "    # Fill missing months and zeros for nicer plots\n",
    "    out = out.sort_values(\"month\").set_index(\"month\")\n",
    "    full_idx = pd.date_range(\n",
    "        start=start_ts.to_period(\"M\").to_timestamp(),\n",
    "        end=out.index.max(),\n",
    "        freq=\"MS\"\n",
    "    )\n",
    "    out = out.reindex(full_idx, fill_value=0)\n",
    "    out.index.name = \"month\"\n",
    "    out = out.reset_index()\n",
    "    \n",
    "    # Make integer counts safely (outer merges create NaNs)\n",
    "    for c in out.columns:\n",
    "        if c != \"month\":\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_monthly_case_flow(\n",
    "    monthly_df: pd.DataFrame,\n",
    "    rolling_months: int | None = None,\n",
    "    title: str = \"Monthly case flow counts (from 2020)\",\n",
    "    outpath=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots all numeric series in monthly_df (expects a 'month' column).\n",
    "    If rolling_months is set (e.g., 3), plots a rolling mean to smooth volatility.\n",
    "    \"\"\"\n",
    "    df = monthly_df.copy().set_index(\"month\").sort_index()\n",
    "\n",
    "    y = df\n",
    "    if rolling_months and rolling_months > 1:\n",
    "        y = df.rolling(rolling_months, min_periods=1).mean()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    for col in y.columns:\n",
    "        ax.plot(y.index, y[col], label=col)\n",
    "\n",
    "    t = title\n",
    "    if rolling_months and rolling_months > 1:\n",
    "        t += f\" — {rolling_months}m rolling avg\"\n",
    "    ax.set_title(t)\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Number of cases\")\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    if outpath is not None:\n",
    "        fig.savefig(outpath, dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "def monthly_case_flow_by(\n",
    "    typed: pd.DataFrame,\n",
    "    group_cols: list[str] = [\"case_type\"],\n",
    "    start=\"2020-01-01\",\n",
    "    date_cols=CASE_FLOW_DATE_COLS,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns monthly counts of received / allocated / pg_signoff / closed\n",
    "    grouped by case_type (or other columns in group_cols).\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    start_ts = pd.Timestamp(start)\n",
    "    for label, col in date_cols.items():\n",
    "        if col not in typed.columns:\n",
    "            continue\n",
    "        sub = (\n",
    "            typed.dropna(subset=[col])\n",
    "            .assign(month=lambda x: pd.to_datetime(x[col]).dt.to_period(\"M\").dt.to_timestamp())\n",
    "            .loc[lambda x: x[\"month\"] >= start_ts]\n",
    "        )\n",
    "        g = (\n",
    "            sub.groupby(group_cols + [\"month\"])[\"case_id\"]\n",
    "            .nunique()\n",
    "            .rename(label)\n",
    "            .reset_index()\n",
    "        )\n",
    "        dfs.append(g)\n",
    "\n",
    "    out = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        out = out.merge(d, on=group_cols + [\"month\"], how=\"outer\")\n",
    "\n",
    "    out = out.fillna(0).sort_values(group_cols + [\"month\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def infer_term_time_workers(\n",
    "    typed: pd.DataFrame,\n",
    "    staff_col: str = \"staff_id\",\n",
    "    alloc_date_col: str = \"dt_alloc_invest\",\n",
    "    case_id_col: str = \"case_id\",\n",
    "    start: str = \"2020-01-01\",\n",
    "    # school holiday months: Jan (xmas spillover), Apr, Jul, Aug, Dec\n",
    "    holiday_months=(1, 4, 7, 8, 12),\n",
    "    min_months: int = 12,\n",
    "    holiday_gap_threshold: float = 0.25,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag staff as 'term-time-like' if they have disproportionately many zero-allocation months\n",
    "    in school-holiday months compared to term months.\n",
    "    Returns one row per staff_id with diagnostics + a boolean flag.\n",
    "    \"\"\"\n",
    "    df = typed.copy()\n",
    "\n",
    "    if staff_col not in df.columns:\n",
    "        raise KeyError(f\"'{staff_col}' not in typed.columns\")\n",
    "\n",
    "    # Choose allocation date column (fallbacks if you use different naming)\n",
    "    if alloc_date_col not in df.columns:\n",
    "        for c in [\"dt_alloc_invest\", \"dt_alloc_team\"]:\n",
    "            if c in df.columns:\n",
    "                alloc_date_col = c\n",
    "                break\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                f\"'{alloc_date_col}' not found. Expected something like dt_alloc_invest / dt_alloc_team.\"\n",
    "            )\n",
    "\n",
    "    df[alloc_date_col] = pd.to_datetime(df[alloc_date_col], errors=\"coerce\")\n",
    "    df = df[df[alloc_date_col].notna()].copy()\n",
    "\n",
    "    if start is not None:\n",
    "        df = df[df[alloc_date_col] >= pd.Timestamp(start)]\n",
    "\n",
    "    # Monthly bucket\n",
    "    df[\"month\"] = df[alloc_date_col].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    # Monthly allocations per staff (distinct cases if case_id exists)\n",
    "    if case_id_col in df.columns:\n",
    "        monthly = (\n",
    "            df.groupby([staff_col, \"month\"])[case_id_col]\n",
    "            .nunique()\n",
    "            .rename(\"alloc_cases\")\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        monthly = (\n",
    "            df.groupby([staff_col, \"month\"])\n",
    "            .size()\n",
    "            .rename(\"alloc_cases\")\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    if monthly.empty:\n",
    "        # No data -> return empty frame with expected columns\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                staff_col,\n",
    "                \"term_time_like\",\n",
    "                \"term_time_band\",\n",
    "                \"n_months\",\n",
    "                \"active_months\",\n",
    "                \"holiday_zero_rate\",\n",
    "                \"term_zero_rate\",\n",
    "                \"holiday_minus_term\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Build complete staff x month grid so missing months become 0\n",
    "    all_staff = monthly[staff_col].dropna().unique()\n",
    "    month_index = pd.date_range(monthly[\"month\"].min(), monthly[\"month\"].max(), freq=\"MS\")\n",
    "\n",
    "    grid = (\n",
    "        pd.MultiIndex.from_product([all_staff, month_index], names=[staff_col, \"month\"])\n",
    "        .to_frame(index=False)\n",
    "    )\n",
    "\n",
    "    monthly = grid.merge(monthly, on=[staff_col, \"month\"], how=\"left\")\n",
    "    monthly[\"alloc_cases\"] = monthly[\"alloc_cases\"].fillna(0)\n",
    "\n",
    "    monthly[\"is_holiday_month\"] = monthly[\"month\"].dt.month.isin(list(holiday_months))\n",
    "    monthly[\"is_zero\"] = monthly[\"alloc_cases\"].eq(0)\n",
    "\n",
    "    # Rates: zero-month fraction in holiday vs term months\n",
    "    rates = (\n",
    "        monthly.groupby([staff_col, \"is_holiday_month\"])[\"is_zero\"]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        .rename(columns={False: \"term_zero_rate\", True: \"holiday_zero_rate\"})\n",
    "    )\n",
    "\n",
    "    # Meta: months observed and active months\n",
    "    meta = monthly.groupby(staff_col).agg(\n",
    "        n_months=(\"month\", \"nunique\"),\n",
    "        active_months=(\"alloc_cases\", lambda s: (s > 0).sum()),\n",
    "    )\n",
    "\n",
    "    out = meta.join(rates).reset_index()\n",
    "\n",
    "    out[\"holiday_minus_term\"] = out[\"holiday_zero_rate\"] - out[\"term_zero_rate\"]\n",
    "    out[\"term_time_like\"] = (\n",
    "        (out[\"n_months\"] >= min_months) &\n",
    "        (out[\"holiday_minus_term\"] >= holiday_gap_threshold)\n",
    "    )\n",
    "    out[\"term_time_band\"] = np.where(out[\"term_time_like\"], \"Term-time-like\", \"Other\")\n",
    "\n",
    "    return out[\n",
    "        [\n",
    "            staff_col,\n",
    "            \"term_time_like\",\n",
    "            \"term_time_band\",\n",
    "            \"n_months\",\n",
    "            \"active_months\",\n",
    "            \"holiday_zero_rate\",\n",
    "            \"term_zero_rate\",\n",
    "            \"holiday_minus_term\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "def _group_key_to_label(key, group_cols):\n",
    "    \"\"\"Pretty label for legend when grouping by 1 or many cols.\"\"\"\n",
    "    if not isinstance(key, tuple):\n",
    "        key = (key,)\n",
    "    parts = []\n",
    "    for c, v in zip(group_cols, key):\n",
    "        parts.append(f\"{c}={v}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "def _top_groups_mask(df, group_cols, value_col, top_n):\n",
    "    \"\"\"Keep only top_n groups by total volume (helps if concern_type has many levels).\"\"\"\n",
    "    if top_n is None:\n",
    "        return pd.Series(True, index=df.index)\n",
    "\n",
    "    totals = (\n",
    "        df.groupby(group_cols, dropna=False)[value_col]\n",
    "          .sum()\n",
    "          .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    if isinstance(totals.index, pd.MultiIndex):\n",
    "        keep = set(totals.head(top_n).index.tolist())\n",
    "        keys = list(map(tuple, df[group_cols].astype(\"object\").to_numpy()))\n",
    "    else:\n",
    "        keep = set((k,) for k in totals.head(top_n).index.tolist())\n",
    "        keys = [(k,) for k in df[group_cols[0]].astype(\"object\").to_numpy()]\n",
    "\n",
    "    return pd.Series([k in keep for k in keys], index=df.index)\n",
    "\n",
    "def plot_monthly_flow_lines(\n",
    "    monthly_df: pd.DataFrame,\n",
    "    group_cols: list,\n",
    "    value_col: str,\n",
    "    outpath,\n",
    "    title: str,\n",
    "    step_date=\"2022-10-01\",\n",
    "    rolling=None,          # e.g. 3 for 3-month rolling average\n",
    "    top_n=12,              # keep plots readable for high-cardinality columns\n",
    "):\n",
    "    outpath = Path(outpath)\n",
    "    outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if \"month\" not in monthly_df.columns:\n",
    "        raise KeyError(\"monthly_df must include a 'month' column\")\n",
    "\n",
    "    if value_col not in monthly_df.columns:\n",
    "        print(f\"[skip plot] '{value_col}' not in monthly_df\")\n",
    "        return\n",
    "\n",
    "    df = monthly_df.copy()\n",
    "    df[\"month\"] = pd.to_datetime(df[\"month\"])\n",
    "\n",
    "    # Keep only top groups (optional)\n",
    "    mask = _top_groups_mask(df, group_cols, value_col=value_col, top_n=top_n)\n",
    "    df = df.loc[mask].copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    for key, sub in df.groupby(group_cols, dropna=False):\n",
    "        sub = sub.sort_values(\"month\")\n",
    "        y = sub[value_col]\n",
    "\n",
    "        # optional rolling average (per group)\n",
    "        if rolling is not None and rolling > 1:\n",
    "            y = y.rolling(window=rolling, min_periods=1).mean()\n",
    "\n",
    "        ax.plot(sub[\"month\"], y, label=_group_key_to_label(key, group_cols), alpha=0.65)\n",
    "    ax.axvline(pd.Timestamp(step_date), color=\"red\", ls=\"--\", lw=2, label=f\"Step change {step_date}\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Number of cases\")\n",
    "    # 1) create the legend and KEEP a handle to it\n",
    "    leg = ax.legend(loc=\"center left\", \n",
    "                    bbox_to_anchor=(1.02, 0.5),\n",
    "                    frameon=False, fontsize=8)\n",
    "    # ax.legend(loc=\"best\")\n",
    "    # ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "    fig.subplots_adjust(right=0.78)\n",
    "    # 2) leave space for the legend on the right (optional but helps)\n",
    "    fig.tight_layout(rect=[0, 0, 0.78, 1])\n",
    "    #plt.tight_layout()\n",
    "    # 3) save including the legend explicitly + a bit of padding\n",
    "    fig.savefig(outpath, dpi=150,\n",
    "                bbox_inches=\"tight\",\n",
    "                bbox_extra_artists=(leg,),\n",
    "                pad_inches=0.2)\n",
    "    plt.show()\n",
    "\n",
    "def monthly_flow_outputs_by_breakdown(\n",
    "    typed: pd.DataFrame,\n",
    "    group_cols: list,\n",
    "    start=\"2020-01-01\",\n",
    "    outcsv_dir=None,\n",
    "    outplot_dir=None,\n",
    "    name=None,\n",
    "    plot_cols=(\"received\", \"pg_signoff\"),\n",
    "    step_date=\"2022-10-01\",\n",
    "    rolling=None,\n",
    "    top_n=12,\n",
    "):\n",
    "    # defensive: skip if columns missing\n",
    "    missing = [c for c in group_cols if c not in typed.columns]\n",
    "    if missing:\n",
    "        print(f\"[skip] {name or group_cols}: missing columns {missing}\")\n",
    "        return None\n",
    "\n",
    "    monthly = monthly_case_flow_by(typed, group_cols, start=start)\n",
    "\n",
    "    tag = name or \"_\".join(group_cols)\n",
    "\n",
    "    # save CSV\n",
    "    if outcsv_dir is not None:\n",
    "        outcsv_dir = Path(outcsv_dir)\n",
    "        outcsv_dir.mkdir(parents=True, exist_ok=True)\n",
    "        monthly.to_csv(outcsv_dir / f\"monthly_flow_by_{tag}.csv\", index=False)\n",
    "\n",
    "    # plots\n",
    "    if outplot_dir is not None:\n",
    "        outplot_dir = Path(outplot_dir)\n",
    "        outplot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for col in plot_cols:\n",
    "            plot_monthly_flow_lines(\n",
    "                monthly_df=monthly,\n",
    "                group_cols=group_cols,\n",
    "                value_col=col,\n",
    "                outpath=outplot_dir / f\"monthly_{col}_by_{tag}.png\",\n",
    "                title=f\"Monthly {col} by {tag}\",\n",
    "                step_date=step_date,\n",
    "                rolling=rolling,\n",
    "                top_n=top_n,\n",
    "            )\n",
    "\n",
    "    return monthly\n",
    "\n",
    "def run_monthly_flow_breakdowns(\n",
    "    typed: pd.DataFrame,\n",
    "    breakdowns: dict,\n",
    "    start=\"2020-01-01\",\n",
    "    outcsv_dir=None,\n",
    "    outplot_dir=None,\n",
    "    plot_cols=(\"received\", \"pg_signoff\"),\n",
    "    step_date=\"2022-10-01\",\n",
    "    rolling=None,\n",
    "    top_n=12,\n",
    "):\n",
    "    outputs = {}\n",
    "    for name, cols in breakdowns.items():\n",
    "        monthly = monthly_flow_outputs_by_breakdown(\n",
    "            typed=typed,\n",
    "            group_cols=cols,\n",
    "            start=start,\n",
    "            outcsv_dir=outcsv_dir,\n",
    "            outplot_dir=outplot_dir,\n",
    "            name=name,\n",
    "            plot_cols=plot_cols,\n",
    "            step_date=step_date,\n",
    "            rolling=rolling,\n",
    "            top_n=top_n,\n",
    "        )\n",
    "        if monthly is not None:\n",
    "            outputs[name] = monthly\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One collective end-to-end demo\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessing import load_raw, engineer\n",
    "from time_series import build_backlog_series, build_daily_panel\n",
    "from interval_analysis import IntervalAnalysis, plot_pg_signoff_monthly_trends, plot_allocation_monthly_trends\n",
    "from eda_opg import EDAConfig, OPGInvestigationEDA\n",
    "from distributions import interval_change_distribution\n",
    "\n",
    "\n",
    "\n",
    "def demo_all():\n",
    "    from pathlib import Path\n",
    "\n",
    "    DEFAULT_INTERVAL_METRICS = [\n",
    "        # “new case start” gap\n",
    "        \"inter_pickup_days\",\n",
    "        # alloc → PG sign-off (already engineered in preprocessing.py)\n",
    "        \"days_to_signoff\",\n",
    "        # received → PG sign-off\n",
    "        \"days_to_pg_signoff\",\n",
    "        # alloc → close\n",
    "        \"days_alloc_to_close\",\n",
    "        # received/alloc → legal review request\n",
    "        \"days_recieved_to_legal_review\",\n",
    "        \"days_alloc_to_req_legal_review\",\n",
    "        # received → alloc\n",
    "        \"days_to_alloc\",\n",
    "    ]\n",
    "    \n",
    "    def _describe_days(s: pd.Series) -> dict:\n",
    "        s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "        if s.empty:\n",
    "            return {\"count\": 0, \"mean\": np.nan, \"std\": np.nan, \"min\": np.nan,\n",
    "                    \"p10\": np.nan, \"p25\": np.nan, \"p50\": np.nan, \"p75\": np.nan, \"p90\": np.nan, \"max\": np.nan}\n",
    "        return {\n",
    "            \"count\": int(s.shape[0]),\n",
    "            \"mean\": float(s.mean()),\n",
    "            \"std\": float(s.std(ddof=1)),\n",
    "            \"min\": float(s.min()),\n",
    "            \"p10\": float(s.quantile(0.10)),\n",
    "            \"p25\": float(s.quantile(0.25)),\n",
    "            \"p50\": float(s.quantile(0.50)),\n",
    "            \"p75\": float(s.quantile(0.75)),\n",
    "            \"p90\": float(s.quantile(0.90)),\n",
    "            \"max\": float(s.max()),\n",
    "        }\n",
    "    \n",
    "    def interval_distributions_by(di: pd.DataFrame, by: list[str], metrics: list[str] | None = None) -> dict[str, pd.DataFrame]:\n",
    "        metrics = metrics or [m for m in DEFAULT_INTERVAL_METRICS if m in di.columns]\n",
    "        out: dict[str, pd.DataFrame] = {}\n",
    "        for metric in metrics:\n",
    "            tbl = (\n",
    "                di.groupby(by, dropna=False)[metric]\n",
    "                  .apply(_describe_days)\n",
    "                  .apply(pd.Series)\n",
    "                  .reset_index()\n",
    "            )\n",
    "            out[metric] = tbl\n",
    "        return out\n",
    "    \n",
    "    def save_and_plot_interval_breakdowns(\n",
    "        di: pd.DataFrame,\n",
    "        by: list[str],\n",
    "        outdir: Path,\n",
    "        label: str | None = None,\n",
    "        metrics: list[str] | None = None,\n",
    "        max_categories: int = 25,\n",
    "    ) -> dict[str, pd.DataFrame]:\n",
    "    \n",
    "        label = label or \"_\".join(by)\n",
    "        outdir = Path(outdir)\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        tables = interval_distributions_by(di, by=by, metrics=metrics)\n",
    "    \n",
    "        for metric, df in tables.items():\n",
    "            # Save tidy table\n",
    "            df.to_csv(outdir / f\"interval_dists_by_{label}__{metric}.csv\", index=False)\n",
    "    \n",
    "            # Bar chart of mean days\n",
    "            plot_df = df.copy()\n",
    "            plot_df[\"mean\"] = pd.to_numeric(plot_df[\"mean\"], errors=\"coerce\")\n",
    "            plot_df = plot_df.dropna(subset=[\"mean\"])\n",
    "    \n",
    "            if len(by) == 1 and plot_df.shape[0] > max_categories:\n",
    "                plot_df = plot_df.sort_values(\"count\", ascending=False).head(max_categories)\n",
    "    \n",
    "            if len(by) == 1:\n",
    "                x = plot_df[by[0]].astype(str)\n",
    "            else:\n",
    "                x = plot_df[by].astype(str).agg(\" | \".join, axis=1)\n",
    "    \n",
    "            fig, ax = plt.subplots(figsize=(10, 4))\n",
    "            ax.bar(x, plot_df[\"mean\"])\n",
    "            ax.set_ylabel(f\"Mean {metric} (days)\")\n",
    "            ax.set_title(f\"Mean {metric} by {label}\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45, labelsize=8)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(outdir / f\"bar_mean_{metric}_by_{label}.png\", dpi=150)\n",
    "            plt.close(fig)\n",
    "    \n",
    "        return tables\n",
    "\n",
    "        \n",
    "    outdir = \"data/out/plot/plots\"\n",
    "    outdir = Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    outcsv = \"data/out/csv/csvs\"\n",
    "    outcsv = Path(outcsv)\n",
    "    outcsv.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    raw, colmap = load_raw(Path(\"data/raw/raw.csv\")) # REAL DATA\n",
    "    typed = engineer(raw, colmap)\n",
    "    \n",
    "    # infer term-time-like pattern and merge back\n",
    "    staff_pattern = infer_term_time_workers(typed)\n",
    "    # keep existing logic expecting \"staff_work_pattern\"\n",
    "    staff_pattern[\"staff_work_pattern\"] = staff_pattern[\"term_time_band\"]\n",
    "    typed = typed.merge(staff_pattern, on=\"staff_id\", how=\"left\")\n",
    "    typed = typed.copy()\n",
    "\n",
    "\n",
    "        \n",
    "    # --------------------------\n",
    "    # Monthly case flow counts (from 2020 onwards)\n",
    "    # --------------------------\n",
    "    # ---- Monthly case flow counts (from 2020 onwards) ----\n",
    "    outcsv = Path(\"data/out/csv/csvs/monthly_flows\")\n",
    "    outcsv.mkdir(parents=True, exist_ok=True)\n",
    "    outdir = Path(\"data/out/plot/plots/monthly_flows\")\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Distinct vs events: distinct_cases=True counts unique case_id per month per event (usually what people mean by “number of cases”). \n",
    "    # If you have multiple allocations per case and you want to count allocations rather than cases, set distinct_cases=False.\n",
    "    # Missing columns: if any of the date columns don’t exist (e.g., dt_close missing), the function will just skip that series\n",
    "    # and still produce the others.\n",
    "    # Date column names: if your engineered dataframe uses different names, update CASE_FLOW_DATE_COLS.\n",
    "    \n",
    "    monthly_flows = monthly_case_flow_counts(\n",
    "        typed,\n",
    "        start=\"2020-01-01\",\n",
    "        end=\"2025-10-31\",\n",
    "        distinct_cases=True,   # change to False if you want to count events/rows\n",
    "    )\n",
    "    \n",
    "    monthly_flows.to_csv(outcsv / \"monthly_case_flow_counts_from_2020.csv\", index=False)\n",
    "    \n",
    "    # One chart with all four series\n",
    "    plot_monthly_case_flow(\n",
    "        monthly_flows,\n",
    "        rolling_months=None,\n",
    "        outpath=outdir / \"monthly_case_flow_counts_from_2020.png\",\n",
    "    )\n",
    "    \n",
    "    # Optional: smoother view (3-month rolling average)\n",
    "    plot_monthly_case_flow(\n",
    "        monthly_flows,\n",
    "        rolling_months=3,\n",
    "        outpath=outdir / \"monthly_case_flow_counts_from_2020_roll3.png\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # --------------------------\n",
    "    # Monthly case flow counts case-type trends (from 2020 onwards)\n",
    "    # --------------------------\n",
    "    FLOW_BREAKDOWNS = {\n",
    "        \"case_type\": [\"case_type\"],\n",
    "        \"application_type\": [\"application_type\"],\n",
    "        \"legal_review\": [\"legal_review\"],\n",
    "        \"concern_type\": [\"concern_type\"],\n",
    "        # NOTE: this combo can create loads of lines; top_n will keep it readable\n",
    "        \"case_status_app_legal\": [\"case_type\", \"application_type\", \"legal_review\", \"concern_type\"],\n",
    "    }\n",
    "    \n",
    "    monthly_flow_outputs = run_monthly_flow_breakdowns(\n",
    "        typed,\n",
    "        breakdowns=FLOW_BREAKDOWNS,\n",
    "        start=\"2020-01-01\",\n",
    "        outcsv_dir=outcsv,  # e.g. Path(\"data/out/csv\")\n",
    "        outplot_dir=outdir / \"monthly_flows_by_breakdown\",  # e.g. Path(\"data/out/plot/plots\")\n",
    "        plot_cols=(\"received\", \"pg_signoff\"),   # add \"closed\"/\"allocated\" if your monthly fn includes them\n",
    "        step_date=\"2022-10-01\",\n",
    "        rolling=3,     # set None to turn off smoothing\n",
    "        top_n=12,      # increase if you want more lines on high-cardinality columns\n",
    "    )\n",
    "\n",
    "    # monthly_flow_outputs_by_breakdown(\n",
    "    #     typed,\n",
    "    #     group_cols=[\"some_other_cat_col\"],\n",
    "    #     start=\"2020-01-01\",\n",
    "    #     outcsv_dir=outcsv,\n",
    "    #     outplot_dir=outdir / \"monthly_flows_by_breakdown\",\n",
    "    #     name=\"some_other_cat_col\",\n",
    "    #     rolling=3,\n",
    "    #     top_n=15,\n",
    "    # )\n",
    "\n",
    "\n",
    "    # monthly_by_case_type = monthly_case_flow_by(typed, [\"case_type\"], start=\"2020-01-01\")\n",
    "    # monthly_by_case_type.to_csv(outcsv / \"monthly_by_case_type_flow_by_case_type.csv\", index=False)\n",
    "\n",
    "    # # recieved\n",
    "    # fig, ax = plt.subplots(figsize=(10,6))\n",
    "    # for ct in monthly_by_case_type[\"case_type\"].unique():\n",
    "    #     sub = monthly_by_case_type[monthly_by_case_type[\"case_type\"]==ct]\n",
    "    #     ax.plot(sub[\"month\"], sub[\"received\"], label=f\"{ct} received\", alpha=0.6)\n",
    "    # ax.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\", lw=2, label=\"Step change Oct 2022\")\n",
    "    # ax.set_title(\"Monthly investigations (received) by case type\")\n",
    "    # ax.set_ylabel(\"Number of cases\")\n",
    "    # ax.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plot_monthly_by_case_type_pg_received = outdir / \"monthly_by_case_type_pg_received.png\"\n",
    "    # plt.savefig(plot_monthly_by_case_type_pg_received, dpi=150)\n",
    "    # plt.show()\n",
    "    \n",
    "    # # signoff\n",
    "    # fig, ax = plt.subplots(figsize=(10,6))\n",
    "    # for ct in monthly_by_case_type[\"case_type\"].unique():\n",
    "    #     sub = monthly_by_case_type[monthly_by_case_type[\"case_type\"]==ct]\n",
    "    #     ax.plot(sub[\"month\"], sub[\"pg_signoff\"], label=f\"{ct} pg_signoff\", alpha=0.6)\n",
    "    # ax.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\", lw=2, label=\"Step change Oct 2022\")\n",
    "    # ax.set_title(\"Monthly investigations (pg_signoff) by case type\")\n",
    "    # ax.set_ylabel(\"Number of cases\")\n",
    "    # ax.legend()\n",
    "    # plt.tight_layout()\n",
    "    # plot_monthly_by_case_type_pg_signoff = outdir / \"monthly_by_case_type_pg_signoff.png\"\n",
    "    # plt.savefig(plot_monthly_by_case_type_pg_signoff, dpi=150)\n",
    "    # plt.show()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Link workforce metrics (FTE) - demand vs staff\n",
    "    # --------------------------\n",
    "    # fte_monthly = (\n",
    "    #     typed.groupby(typed[\"dt_alloc_invest\"].dt.to_period(\"M\"))[\"fte\"]\n",
    "    #     .mean()\n",
    "    #     .rename(\"avg_fte\")\n",
    "    #     .reset_index()\n",
    "    # )\n",
    "    # fte_monthly[\"month\"] = fte_monthly[\"dt_alloc_invest\"].dt.to_timestamp()\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # plt.plot(monthly_flows[\"month\"], monthly_flows[\"received\"], label=\"New investigations\")\n",
    "    # plt.plot(fte_monthly[\"month\"], fte_monthly[\"avg_fte\"]*10, label=\"Avg FTE × 10 (scaled)\")\n",
    "    # plt.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\")\n",
    "    # plt.legend(); plt.title(\"New investigations vs staff FTE\")\n",
    "    # plt.show()\n",
    "    # plot_fte_monthly = outdir / \"fte_monthly.png\"\n",
    "    # plt.savefig(plot_fte_monthly, dpi=150)\n",
    "\n",
    "    # avg_fte line is “flat” because you’re taking the mean FTE across allocated cases each month \n",
    "    # — if most staff have stable FTE (often 1.0), the monthly mean won’t move much.\n",
    "    # What we want instead is a monthly headcount of investigators available/active, \n",
    "    # split into full-time vs part-time, and optionally a monthly total FTE capacity (sum of FTEs of active staff).\n",
    "\n",
    "\n",
    "    # --- configure ---\n",
    "    FTE_FULL_TIME_CUTOFF = 0.8              # change threshold if needed\n",
    "    STAFF_MONTH_DATE_COL = \"dt_alloc_invest\"  # \"activity\" definition (see note below)\n",
    "    #     “available/active” = had at least one allocation (dt_alloc_invest) that month.\n",
    "    # If you want “active” to mean any work happened, switch STAFF_MONTH_DATE_COL to another date \n",
    "    #     (e.g., dt_pg_signoff, dt_close) or build activity from multiple event dates (I can give you that version too).\n",
    "    \n",
    "    # 1) Estimate each staff member's typical FTE (use median to be robust)\n",
    "    staff_fte = (\n",
    "        typed.dropna(subset=[\"staff_id\"])\n",
    "            .groupby(\"staff_id\")[\"fte\"]\n",
    "            .median()\n",
    "            .rename(\"staff_fte\")\n",
    "            .reset_index()\n",
    "    )\n",
    "    \n",
    "    # 2) Classify staff as Full-time / Part-time (and keep Unknown if missing)\n",
    "    staff_fte[\"fte_band\"] = np.where(\n",
    "        staff_fte[\"staff_fte\"].ge(FTE_FULL_TIME_CUTOFF), \"Full-time\",\n",
    "        np.where(staff_fte[\"staff_fte\"].notna(), \"Part-time\", \"Unknown\")\n",
    "    )\n",
    "    \n",
    "    # 3) Attach staff_fte + band back onto typed\n",
    "    typed_staff = typed.merge(staff_fte, on=\"staff_id\", how=\"left\")\n",
    "    typed_staff = typed_staff[typed_staff[\"dt_alloc_invest\"] >= pd.Timestamp(\"2020-01-01\")]\n",
    "    \n",
    "    # 4) Build staff \"active this month\" based on STAFF_MONTH_DATE_COL\n",
    "    staff_active = (\n",
    "        typed_staff.dropna(subset=[STAFF_MONTH_DATE_COL, \"staff_id\"])\n",
    "            .assign(month=lambda d: d[STAFF_MONTH_DATE_COL].dt.to_period(\"M\").dt.to_timestamp())\n",
    "            [[\"month\", \"staff_id\", \"staff_fte\", \"fte_band\"]]\n",
    "            .drop_duplicates([\"month\", \"staff_id\"])   # key: count each staff once per month\n",
    "    )\n",
    "    \n",
    "    # 5) Monthly headcount (unique staff) by band + total FTE capacity by band\n",
    "    staff_monthly = (\n",
    "        staff_active.groupby([\"month\", \"fte_band\"], as_index=False)\n",
    "            .agg(\n",
    "                n_staff=(\"staff_id\", \"nunique\"),\n",
    "                sum_fte=(\"staff_fte\", \"sum\"),\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    # Wide forms for plotting\n",
    "    staff_counts_wide = (\n",
    "        staff_monthly.pivot(index=\"month\", columns=\"fte_band\", values=\"n_staff\")\n",
    "        .fillna(0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    staff_fte_wide = (\n",
    "        staff_monthly.pivot(index=\"month\", columns=\"fte_band\", values=\"sum_fte\")\n",
    "        .fillna(0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Convenience totals\n",
    "    for df in (staff_counts_wide, staff_fte_wide):\n",
    "        cols = [c for c in df.columns if c != \"month\"]\n",
    "        df[\"total\"] = df[cols].sum(axis=1)\n",
    "\n",
    "    # Ensure months are aligned (optional but helpful)\n",
    "    mf = monthly_flows.copy()\n",
    "    mf[\"month\"] = pd.to_datetime(mf[\"month\"])\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Demand side (cases)\n",
    "    if \"received\" in mf.columns:\n",
    "        ax1.plot(mf[\"month\"], mf[\"received\"], label=\"Received (new investigations)\", color=\"cyan\")\n",
    "    if \"pg_signoff\" in mf.columns:\n",
    "        ax1.plot(mf[\"month\"], mf[\"pg_signoff\"], label=\"PG sign-offs\", color=\"pink\")\n",
    "    # if \"closed\" in mf.columns:\n",
    "    #     ax1.plot(mf[\"month\"], mf[\"closed\"], label=\"Closed\")\n",
    "    \n",
    "    ax1.set_ylabel(\"Cases per month\")\n",
    "    ax1.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\")\n",
    "    \n",
    "    # Supply side (staff headcount)\n",
    "    ax2 = ax1.twinx()\n",
    "    if \"Full-time\" in staff_counts_wide.columns:\n",
    "        ax2.plot(staff_counts_wide[\"month\"], staff_counts_wide[\"Full-time\"], label=\"Full-time investigators\")\n",
    "    if \"Part-time\" in staff_counts_wide.columns:\n",
    "        ax2.plot(staff_counts_wide[\"month\"], staff_counts_wide[\"Part-time\"], label=\"Part-time investigators\")\n",
    "    ax2.plot(staff_counts_wide[\"month\"], staff_counts_wide[\"total\"], label=\"Total investigators\", linestyle=\":\")\n",
    "    \n",
    "    ax2.set_ylabel(\"Investigators active (unique staff/month)\")\n",
    "    \n",
    "    # combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n",
    "    ax1.set_title(\"Monthly demand vs investigator availability (FT/PT headcount)\")\n",
    "    plt.tight_layout()\n",
    "    plot_fte_monthly = outdir / \"demand_vs_staff_headcount.png\"\n",
    "    plt.savefig(plot_fte_monthly, dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Quick check: did staff mix change around Oct 2022?\n",
    "    # This gives a simple pre/post comparison including “PG sign-offs per investigator”:\n",
    "    cut = pd.Timestamp(\"2022-10-01\")\n",
    "    # merge demand + staff totals\n",
    "    staff_tot = staff_counts_wide[[\"month\", \"total\"]].rename(columns={\"total\": \"n_investigators\"})\n",
    "    merged = mf.merge(staff_tot, on=\"month\", how=\"left\")\n",
    "    \n",
    "    merged[\"pg_signoff_per_investigator\"] = np.where(\n",
    "        merged[\"n_investigators\"] > 0,\n",
    "        merged.get(\"pg_signoff\", np.nan) / merged[\"n_investigators\"],\n",
    "        np.nan\n",
    "    )\n",
    "    summary = (\n",
    "        merged.assign(period=np.where(merged[\"month\"] < cut, \"pre\", \"post\"))\n",
    "              .groupby(\"period\")[[\"received\", \"pg_signoff\", \"n_investigators\", \"pg_signoff_per_investigator\"]] # , \"closed\"\n",
    "              .mean(numeric_only=True)\n",
    "    )\n",
    "    print(summary)\n",
    "\n",
    "    \n",
    "    # --------------------------\n",
    "    # “case age” lens (from receipt → PG sign-off)\n",
    "    # --------------------------\n",
    "    typed[\"case_age_days\"] = (typed[\"dt_pg_signoff\"] - typed[\"dt_received_inv\"]).dt.days\n",
    "    typed[\"age_band\"] = pd.cut(\n",
    "        typed[\"case_age_days\"],\n",
    "        bins=[0,90,180,365,730,5000],\n",
    "        labels=[\"<3 m\",\"3-6 m\",\"6-12 m\",\"1-2 y\",\">2 y\"]\n",
    "    )\n",
    "    age_monthly = (\n",
    "        typed.groupby([typed[\"dt_received_inv\"].dt.to_period(\"M\"),\"age_band\"])[\"case_id\"]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"dt_received_inv\":\"month\"})\n",
    "    )\n",
    "    age_monthly[\"month\"]=age_monthly[\"month\"].dt.to_timestamp()\n",
    "    age_monthly.to_csv(outcsv / \"age_monthly_case_flow_counts_from_2020.csv\", index=False)\n",
    "    # ---- tidy/standardise ----\n",
    "    age_monthly = age_monthly.copy()\n",
    "    age_monthly[\"month\"] = pd.to_datetime(age_monthly[\"month\"])\n",
    "    \n",
    "    # if your count column is still called case_id, rename it\n",
    "    if \"case_id\" in age_monthly.columns and \"n_cases\" not in age_monthly.columns:\n",
    "        age_monthly = age_monthly.rename(columns={\"case_id\": \"n_cases\"})\n",
    "    \n",
    "    # pivot to wide\n",
    "    pivot = (\n",
    "        age_monthly\n",
    "        .pivot_table(index=\"month\", columns=\"age_band\", values=\"n_cases\", aggfunc=\"sum\")\n",
    "        .fillna(0)\n",
    "        .sort_index()\n",
    "    )\n",
    "    \n",
    "    # ensure continuous monthly index\n",
    "    all_months = pd.date_range(pivot.index.min(), pivot.index.max(), freq=\"MS\")\n",
    "    pivot = pivot.reindex(all_months, fill_value=0)\n",
    "    pivot.index.name = \"month\"\n",
    "    \n",
    "    # ---- plot ----\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    ax.stackplot(\n",
    "        pivot.index,\n",
    "        [pivot[c].values for c in pivot.columns],\n",
    "        labels=[str(c) for c in pivot.columns],\n",
    "        alpha=0.9\n",
    "    )\n",
    "    ax.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\", lw=2, label=\"Oct 2022\")\n",
    "    ax.set_title(\"Monthly case counts by Case Age Band (Received - PG signed off) \")\n",
    "    ax.set_ylabel(\"Number of cases\")\n",
    "    ax.legend(loc=\"upper left\", ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plot_age_monthly = outdir / \"age_monthly.png\"\n",
    "    plt.savefig(plot_age_monthly, dpi=150)\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    bottom = np.zeros(len(pivot.index))\n",
    "    \n",
    "    for col in pivot.columns:\n",
    "        ax.bar(pivot.index, pivot[col].values, bottom=bottom, label=str(col), width=25)  # ~month width\n",
    "        bottom += pivot[col].values\n",
    "    \n",
    "    ax.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\", lw=2)\n",
    "    ax.set_title(\"Monthly case counts by Case Age Band (Received - PG signed off) stacked bars\")\n",
    "    ax.set_ylabel(\"Number of cases\")\n",
    "    ax.legend(loc=\"upper left\", ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plot_age_monthly_stacked_car_chart = outdir / \"age_monthly_stacked_car_chart.png\"\n",
    "    plt.savefig(plot_age_monthly_stacked_car_chart, dpi=150)\n",
    "    plt.show()\n",
    "    total = pivot.sum(axis=1)\n",
    "    roll3 = total.rolling(3, min_periods=1).mean()\n",
    "    \n",
    "    plt.figure(figsize=(11, 4))\n",
    "    plt.plot(total.index, total.values, label=\"Monthly total\")\n",
    "    plt.plot(roll3.index, roll3.values, label=\"3-month rolling avg\")\n",
    "    plt.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\", lw=2)\n",
    "    plt.title(\"Monthly total (All Case Age Band (Received - PG signed off)) with 3-month rolling average\")\n",
    "    plt.ylabel(\"Number of cases\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_age_monthly_stacked_car_chart_3m = outdir / \"age_monthly_stacked_car_chart_3m.png\"\n",
    "    plt.savefig(plot_age_monthly_stacked_car_chart_3m, dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    pivot_roll3 = pivot.rolling(3, min_periods=1).mean()\n",
    "    fig, ax = plt.subplots(figsize=(11, 5))\n",
    "    ax.stackplot(\n",
    "        pivot_roll3.index,\n",
    "        [pivot_roll3[c].values for c in pivot_roll3.columns],\n",
    "        labels=[str(c) for c in pivot_roll3.columns],\n",
    "        alpha=0.9\n",
    "    )\n",
    "    ax.axvline(pd.Timestamp(\"2022-10-01\"), color=\"red\", ls=\"--\", lw=2)\n",
    "    ax.set_title(\"Case Age Band (Received - PG signed off) volumes (3-month rolling average)\")\n",
    "    ax.set_ylabel(\"Avg cases / month (3-mo)\")\n",
    "    ax.legend(loc=\"upper left\", ncol=3)\n",
    "    plt.tight_layout()\n",
    "    plot_age_monthly_stacked_car_chart_3m_ageband = outdir / \"age_monthly_stacked_car_chart_3m_ageband.png\"\n",
    "    plt.savefig(plot_age_monthly_stacked_car_chart_3m_ageband, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # make sure key date columns are datetime (skip if not present)\n",
    "    for c in [\"dt_received_inv\", \"dt_alloc_invest\", \"dt_pg_signoff\", \"dt_close\", \"dt_legal_review_req1\"]:\n",
    "        if c in typed.columns:\n",
    "            typed[c] = pd.to_datetime(typed[c], errors=\"coerce\")\n",
    "    \n",
    "    # alloc → PG sign-off\n",
    "    if \"days_to_signoff\" not in typed.columns and {\"dt_pg_signoff\", \"dt_alloc_invest\"} <= set(typed.columns):\n",
    "        typed[\"days_to_signoff\"] = (typed[\"dt_pg_signoff\"] - typed[\"dt_alloc_invest\"]).dt.days\n",
    "    \n",
    "    # alloc → close\n",
    "    if \"days_alloc_to_close\" not in typed.columns and {\"dt_close\", \"dt_alloc_invest\"} <= set(typed.columns):\n",
    "        typed[\"days_alloc_to_close\"] = (typed[\"dt_close\"] - typed[\"dt_alloc_invest\"]).dt.days\n",
    "    \n",
    "    # received → PG sign-off\n",
    "    if \"days_to_pg_signoff\" not in typed.columns and {\"dt_pg_signoff\", \"dt_received_inv\"} <= set(typed.columns):\n",
    "        typed[\"days_to_pg_signoff\"] = (typed[\"dt_pg_signoff\"] - typed[\"dt_received_inv\"]).dt.days\n",
    "    \n",
    "    # received → alloc\n",
    "    if \"days_to_alloc\" not in typed.columns and {\"dt_alloc_invest\", \"dt_received_inv\"} <= set(typed.columns):\n",
    "        typed[\"days_to_alloc\"] = (typed[\"dt_alloc_invest\"] - typed[\"dt_received_inv\"]).dt.days\n",
    "    \n",
    "    # received → legal review request\n",
    "    if \"days_recieved_to_legal_review\" not in typed.columns and {\"dt_legal_review_req1\", \"dt_received_inv\"} <= set(typed.columns):\n",
    "        typed[\"days_recieved_to_legal_review\"] = (typed[\"dt_legal_review_req1\"] - typed[\"dt_received_inv\"]).dt.days\n",
    "    \n",
    "    # alloc → legal review request\n",
    "    if \"days_alloc_to_req_legal_review\" not in typed.columns and {\"dt_legal_review_req1\", \"dt_alloc_invest\"} <= set(typed.columns):\n",
    "        typed[\"days_alloc_to_req_legal_review\"] = (typed[\"dt_legal_review_req1\"] - typed[\"dt_alloc_invest\"]).dt.days\n",
    "\n",
    "    \n",
    "    if \"inter_pickup_days\" not in typed.columns and \"time_since_last_pickup\" in typed.columns:\n",
    "        typed[\"inter_pickup_days\"] = typed[\"time_since_last_pickup\"]\n",
    "        \n",
    "    typed['days_to_signoff'] = typed[\"days_to_pg_signoff\"]\n",
    "    typed[\"legal_review\"] = pd.to_numeric(typed[\"legal_review\"], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "    # or: typed[\"legal_review\"] = typed[\"legal_review\"].astype(\"int8\")\n",
    "\n",
    "    typed.to_csv(outcsv / \"typed.csv\", index=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    typed['days_to_alloc'].hist(ax=axes[0], bins=50)\n",
    "    axes[0].set_title('Days to allocation (all cases)')\n",
    "    axes[0].set_xlabel('Days')\n",
    "    axes[0].set_ylabel('Number of cases')\n",
    "    typed['days_to_signoff'].hist(ax=axes[1], bins=50)\n",
    "    axes[1].set_title('Days to sign-off (all cases)')\n",
    "    axes[1].set_xlabel('Days')\n",
    "    plt.tight_layout()\n",
    "    plot_outliers = outdir / \"outliers.png\"\n",
    "    plt.savefig(plot_outliers, bbox_inches=\"tight\", dpi=150)\n",
    "\n",
    "    # Time-series panels\n",
    "    backlog = build_backlog_series(typed)\n",
    "    if \"backlog_available\" in backlog.columns and \"backlog\" not in backlog.columns:\n",
    "        backlog = backlog.rename(columns={\"backlog_available\": \"backlog\"})\n",
    "    daily, backlog_ts, events = build_daily_panel(typed)\n",
    "    daily.to_csv(outcsv / \"daily.csv\", index=False)\n",
    "    backlog_ts.to_csv(outcsv / \"backlog_ts.csv\", index=False)\n",
    "    events.to_csv(outcsv / \"events.csv\", index=False)\n",
    "    \n",
    "    # Interval frame for analysis (includes wip_load, time_since_last_pickup, event_newcase, etc.)\n",
    "    di = IntervalAnalysis.build_interval_frame(\n",
    "        typed, \n",
    "        backlog_series=backlog_ts#,\n",
    "        #bank_holidays=True\n",
    "    )\n",
    "    \n",
    "    # add holiday flags on di \"date\"\n",
    "    #di = add_holiday_flags(di, date_col=\"date\")\n",
    "    \n",
    "    # bring staff pattern onto di (if staff_id is present in di; otherwise merge via case_id from typed)\n",
    "    if \"staff_id\" in di.columns:\n",
    "        di = di.merge(staff_pattern[[\"staff_id\", \"staff_work_pattern\"]], on=\"staff_id\", how=\"left\")\n",
    "    else:\n",
    "        di = di.merge(typed[[\"case_id\", \"staff_id\"]], on=\"case_id\", how=\"left\")\n",
    "        di = di.merge(staff_pattern[[\"staff_id\", \"staff_work_pattern\"]], on=\"staff_id\", how=\"left\")\n",
    "\n",
    "        \n",
    "    # Ensure `di` contains case-level fields needed for breakdowns (e.g. application_type).\n",
    "    # `IntervalAnalysis.build_interval_frame()` keeps a fixed set of columns, so we merge extra case-level fields here.\n",
    "    if \"application_type\" in typed.columns and \"application_type\" not in di.columns:\n",
    "        _app = typed[[\"case_id\", \"application_type\"]].drop_duplicates(\"case_id\").copy()\n",
    "        # Robust join even if case_id is stored as int in one frame and string in the other\n",
    "        _app[\"case_id\"] = _app[\"case_id\"].astype(\"string\")\n",
    "        di[\"case_id\"] = di[\"case_id\"].astype(\"string\")\n",
    "        di = di.merge(_app, on=\"case_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    # All the “interval” analysis we want should be plugged in after di is created. \n",
    "    # That guarantees we are working on real data, not synthetic.\n",
    "    di.to_csv(outcsv / \"di.csv\", index=False)\n",
    "\n",
    "    \n",
    "    # --------------------------\n",
    "    # Direct test: do multi-week gaps spike in those holiday seasons?\n",
    "    # --------------------------\n",
    "    alloc = (\n",
    "        typed[[\"staff_id\", \"dt_alloc_invest\", \"staff_work_pattern\"]]\n",
    "        .copy()\n",
    "    )\n",
    "    alloc[\"dt_alloc_invest\"] = pd.to_datetime(alloc[\"dt_alloc_invest\"], errors=\"coerce\")\n",
    "    alloc = alloc.dropna(subset=[\"staff_id\", \"dt_alloc_invest\"]).sort_values([\"staff_id\", \"dt_alloc_invest\"])\n",
    "    \n",
    "    alloc[\"prev_alloc\"] = alloc.groupby(\"staff_id\")[\"dt_alloc_invest\"].shift(1)\n",
    "    alloc[\"inter_pickup_days\"] = (alloc[\"dt_alloc_invest\"] - alloc[\"prev_alloc\"]).dt.days\n",
    "    alloc = alloc.dropna(subset=[\"inter_pickup_days\"])\n",
    "    \n",
    "    alloc[\"holiday_season\"] = alloc[\"dt_alloc_invest\"].apply(school_holiday_season)\n",
    "    \n",
    "    gap_summary = (\n",
    "        alloc.groupby([\"staff_work_pattern\", \"holiday_season\"])[\"inter_pickup_days\"]\n",
    "        .agg(n=\"count\", median=\"median\", p75=lambda s: s.quantile(0.75), p90=lambda s: s.quantile(0.90), mean=\"mean\")\n",
    "        .reset_index()\n",
    "        .sort_values([\"staff_work_pattern\", \"holiday_season\"])\n",
    "    )\n",
    "    display(gap_summary)\n",
    "\n",
    "    outplot = Path(\"data/out/plot/plots/Inter-pickup_gap_by_holiday_season\")\n",
    "    outplot.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plot_df = alloc.copy()\n",
    "    plot_df = plot_df[plot_df[\"inter_pickup_days\"].between(0, plot_df[\"inter_pickup_days\"].quantile(0.99))]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    plot_df.boxplot(column=\"inter_pickup_days\", by=\"holiday_season\", ax=ax)\n",
    "    ax.set_title(\"Inter-pickup gap by holiday season (outliers clipped at p99)\")\n",
    "    ax.set_xlabel(\"Holiday season\")\n",
    "    ax.set_ylabel(\"Days between case pickups\")\n",
    "    plt.suptitle(\"\")\n",
    "    fig.savefig(outplot / \"boxplot_pickup_gap_by_holiday_season.png\", dpi=150)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "    \n",
    "    # --------------------------\n",
    "    # Overall distributions (all interval metrics)\n",
    "    # --------------------------\n",
    "    \n",
    "    INTERVAL_METRICS = [\n",
    "        # “new case start” gap\n",
    "        \"inter_pickup_days\",\n",
    "        # alloc → PG sign-off\n",
    "        \"days_to_signoff\",\n",
    "        # received → PG sign-off\n",
    "        \"days_to_pg_signoff\",\n",
    "        # alloc → close\n",
    "        \"days_alloc_to_close\",\n",
    "        # received/alloc → legal review request\n",
    "        \"days_recieved_to_legal_review\",\n",
    "        \"days_alloc_to_req_legal_review\",\n",
    "        # received → alloc\n",
    "        \"days_to_alloc\",\n",
    "    ]\n",
    "\n",
    "    # Set any negative durations to NA so they don't affect mins/means/plots\n",
    "    for c in INTERVAL_METRICS:\n",
    "        if c in di.columns:\n",
    "            di.loc[di[c] < 0, c] = pd.NA\n",
    "    \n",
    "    # --- overall ---\n",
    "    all_overall = IntervalAnalysis.analyse_interval_distributions(di)  # uses interval_columns_available internally\n",
    "    interval_dists_overall = _filter_metrics(all_overall, INTERVAL_METRICS)\n",
    "    \n",
    "    overall_df = tidy_overall(interval_dists_overall).sort_values(\"metric\")\n",
    "    print(\"\\n=== Interval distributions (overall, last 4 years) ===\")\n",
    "    display(overall_df)\n",
    "\n",
    "\n",
    "    # Breakdowns (case_type, risk, application_type, legal_review, and combo)\n",
    "    # “frequency distribution summary” table (count/mean/std/percentiles/etc) \n",
    "    # for each metric, broken down by case type, risk, application type, legal review, and all combined.\n",
    "    \n",
    "    BREAKDOWNS = {\n",
    "        \"case_type\": [\"case_type\"],\n",
    "        #\"risk_band\": [\"risk_band\"],\n",
    "        \"application_type\": [\"application_type\"],\n",
    "        \"legal_review\": [\"legal_review\"],\n",
    "        \"concern_type\": [\"concern_type\"],\n",
    "        # full breakdown requested:\n",
    "        \"case_status_app_legal\": [\"case_type\", \"application_type\", \"legal_review\", \"concern_type\"], #\"risk_band\", \n",
    "    }\n",
    "    \n",
    "    breakdown_tables = {}\n",
    "    \n",
    "    for name, cols in BREAKDOWNS.items():\n",
    "        all_by = IntervalAnalysis.analyse_interval_distributions(di, by=cols)\n",
    "        d = _filter_metrics(all_by, INTERVAL_METRICS)\n",
    "        df = tidy_by(d, cols)\n",
    "        breakdown_tables[name] = df\n",
    "    \n",
    "        print(f\"\\n=== Interval distributions by {name} ===\")\n",
    "        display(df.head(20))\n",
    "\n",
    "\n",
    "    # --------------------------\n",
    "    # Add basic charts for each metric (hist + box + trend)\n",
    "    # --------------------------\n",
    "\n",
    "    # Histograms (overall)\n",
    "    outplot = Path(\"data/out/plot/plots/interval_metrics\")\n",
    "    outplot.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the metric series the SAME way IntervalAnalysis does (handles derived metrics)\n",
    "    metric_series = IntervalAnalysis.interval_columns_available(di)\n",
    "    \n",
    "    for metric in INTERVAL_METRICS:\n",
    "        s = metric_series.get(metric)\n",
    "        if s is None:\n",
    "            continue\n",
    "    \n",
    "        x = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "        if x.empty:\n",
    "            continue\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        ax.hist(x, bins=50)\n",
    "        ax.set_title(f\"Distribution: {metric}\")\n",
    "        ax.set_xlabel(\"Days\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(outplot / f\"hist_{metric}.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    # Boxplots by case type (repeatable for other group cols)\n",
    "    GROUP_COL = \"case_type\"  # change to \"risk_band\" or \"application_type\" etc.\n",
    "    \n",
    "    for metric in INTERVAL_METRICS:\n",
    "        s = metric_series.get(metric)\n",
    "        if s is None or GROUP_COL not in di.columns:\n",
    "            continue\n",
    "    \n",
    "        tmp = di[[GROUP_COL]].copy()\n",
    "        tmp[metric] = pd.to_numeric(s, errors=\"coerce\")\n",
    "        tmp = tmp.dropna(subset=[GROUP_COL, metric])\n",
    "    \n",
    "        if tmp.empty:\n",
    "            continue\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        tmp.boxplot(column=metric, by=GROUP_COL, ax=ax, rot=45)\n",
    "        ax.set_title(f\"{metric} by {GROUP_COL}\")\n",
    "        ax.set_xlabel(GROUP_COL)\n",
    "        ax.set_ylabel(\"Days\")\n",
    "        plt.suptitle(\"\")  # removes pandas default subtitle\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(outplot / f\"box_{metric}_by_{GROUP_COL}.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    \n",
    "    # materialises any derived metrics into di\n",
    "    _available = IntervalAnalysis.interval_columns_available(di)\n",
    "    for _m in INTERVAL_METRICS:\n",
    "        if _m not in di.columns and _m in _available:\n",
    "            di[_m] = _available[_m]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Monthly trends (median by case type) for ALL metrics\n",
    "    for metric in INTERVAL_METRICS:\n",
    "        if metric not in di.columns:\n",
    "            continue\n",
    "        trend = IntervalAnalysis.monthly_trend(di, metric=metric, by=[\"case_type\"])\n",
    "        # print(\"\\n=== Trend ===\")\n",
    "        # display(f\"trend \\n: {trend}\")\n",
    "\n",
    "    # for metric in INTERVAL_METRICS:\n",
    "    #     trend = IntervalAnalysis.monthly_trend(di, metric=metric, by=[\"case_type\"])\n",
    "    #     if trend is None or len(trend) == 0:\n",
    "    #         continue\n",
    "    \n",
    "        # trend columns: ['case_type','yyyymm','value',...] varies by your implementation;\n",
    "        # In your earlier output you had the metric column present, so pivot on that:\n",
    "        if metric not in trend.columns:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Ensure we have a real datetime month column for pivoting/plotting\n",
    "        if \"month\" not in trend.columns:\n",
    "            if \"yyyymm\" in trend.columns:\n",
    "                trend = trend.copy()\n",
    "                trend[\"month\"] = pd.to_datetime(trend[\"yyyymm\"].astype(str) + \"-01\")\n",
    "            elif \"date\" in trend.columns:\n",
    "                trend = trend.copy()\n",
    "                trend[\"month\"] = pd.to_datetime(trend[\"date\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "        \n",
    "        pivot = (\n",
    "            trend.pivot_table(index=\"month\", columns=\"case_type\", values=metric, aggfunc=\"median\")\n",
    "            .sort_index()\n",
    "        )\n",
    "\n",
    "        pivot = trend.pivot_table(index=\"month\", columns=\"case_type\", values=metric, aggfunc=\"median\")\n",
    "        # pivot = trend.pivot_table(index=\"yyyymm\", columns=\"case_type\", values=metric, aggfunc=\"median\").sort_index()\n",
    "\n",
    "        if pivot.empty:\n",
    "            continue\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        pivot.plot(ax=ax)\n",
    "        ax.set_title(f\"Monthly median {metric} by case_type\")\n",
    "        ax.set_xlabel(\"Month\")\n",
    "        ax.set_ylabel(\"Days\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(outplot / f\"trend_{metric}_by_case_type.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        # Rolling - overall monthly median of any metric\n",
    "        tmp = di[[\"date\", metric]].copy()\n",
    "        tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], errors=\"coerce\")\n",
    "        tmp = tmp.dropna(subset=[\"date\", metric])\n",
    "        tmp[\"month\"] = tmp[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "        \n",
    "        monthly = tmp.groupby(\"month\")[metric].median().sort_index()\n",
    "        rolling3 = add_rolling_avg(monthly, window=3)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.plot(monthly.index, monthly.values, label=\"Monthly median\")\n",
    "        ax.plot(rolling3.index, rolling3.values, label=\"3-mo rolling avg\")\n",
    "        ax.set_title(f\"{metric} over time (smoothed)\")\n",
    "        ax.set_xlabel(\"Month\")\n",
    "        ax.set_ylabel(f\"Median {metric}\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(outplot / f\"rolling average_median_{metric}_by_case_type.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Distributions of key time intervals:\n",
    "    # How long from allocation to close vs to PG signoff\n",
    "    # How long gaps between pickups vs case lifecycle durations\n",
    "    # Distribution shape (boxplots)\n",
    "    # ------------------------------------------------------\n",
    "    interval_dists_overall = IntervalAnalysis.analyse_interval_distributions(di)\n",
    "    \n",
    "    # Remove unnecessary columns/metrics\n",
    "    #interval_dists_overall.pop(\"days_alloc_to_req_legal_review\", None)\n",
    "\n",
    "    \n",
    "    # Optional: print a quick summary so you can see something immediately\n",
    "    print(\"\\n=== Interval distributions (overall, last 4 years) ===\")\n",
    "    interval_dists_overall_df = pd.DataFrame(interval_dists_overall).T\n",
    "    interval_dists_overall_df.to_csv(outcsv / \"interval_dists_overall_last4yrs.csv\")\n",
    "    \n",
    "    overall_df = (\n",
    "        pd.DataFrame(interval_dists_overall)\n",
    "        .T                      # metrics become rows\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"metric\", \"p50\": \"median\"})\n",
    "        [[\"metric\", \"count\", \"mean\", \"std\", \"min\", \"p10\", \"p25\", \"median\", \"p75\", \"p90\", \"max\"]]\n",
    "    )\n",
    "    print(overall_df)\n",
    "    #print(pd.DataFrame(interval_dists_overall).T.head())\n",
    "    # Save Interval distributions DataFrame to CSV\n",
    "    overall_df.to_csv(outcsv / \"interval_dists_overall_df.csv\", index=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.bar(overall_df[\"metric\"], overall_df[\"median\"])\n",
    "    ax.set_ylabel(\"Median days\")\n",
    "    ax.set_title(\"Median interval length (last 4 years)\")\n",
    "    ax.set_xticklabels(overall_df[\"metric\"], rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plot_interval_dists_overall = outdir / \"plot_interval_dists_overall.png\"\n",
    "    plt.savefig(plot_interval_dists_overall, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Distribution shape (boxplots)\n",
    "    # Keep it simple: use a subset of metrics\n",
    "    metrics_to_plot = [\"days_to_alloc\", \n",
    "                       \"days_to_pg_signoff\", \n",
    "                       \"days_alloc_to_close\", \n",
    "                       \"days_recieved_to_legal_review\", \n",
    "                       \"days_alloc_to_req_legal_review\", \n",
    "                       \"inter_pickup_days\"\n",
    "                      ]\n",
    "\n",
    "    # Set any negative durations to NA so they don't affect mins/means/plots\n",
    "    # That will prevent negative mins/means in anything computed from di afterwards \n",
    "    # (distributions, charts, trends, etc.), without inventing fake values.\n",
    "    for c in metrics_to_plot:\n",
    "        if c in di.columns:\n",
    "            di.loc[di[c] < 0, c] = pd.NA\n",
    "            \n",
    "    # Use the interval frame (di) for derived interval metrics\n",
    "    df_for_box = di.copy()\n",
    "    \n",
    "    # Ensure derived columns exist (in case you want to reference them directly)\n",
    "    if \"days_alloc_to_close\" not in df_for_box.columns and {\"dt_close\", \"dt_alloc_invest\"}.issubset(df_for_box.columns):\n",
    "        df_for_box[\"days_alloc_to_close\"] = (df_for_box[\"dt_close\"] - df_for_box[\"dt_alloc_invest\"]).dt.days.astype(\"float\")\n",
    "    \n",
    "    if \"inter_pickup_days\" not in df_for_box.columns and \"time_since_last_pickup\" in df_for_box.columns:\n",
    "        df_for_box[\"inter_pickup_days\"] = df_for_box[\"time_since_last_pickup\"]\n",
    "    \n",
    "    cols_for_box = [\"days_to_alloc\", \n",
    "                       \"days_to_pg_signoff\", \n",
    "                       \"days_alloc_to_close\", \n",
    "                       \"days_recieved_to_legal_review\", \n",
    "                       \"days_alloc_to_req_legal_review\", \n",
    "                       \"inter_pickup_days\"\n",
    "                      ]\n",
    "    cols_present = [c for c in cols_for_box if c in df_for_box.columns]\n",
    "    \n",
    "    missing = sorted(set(cols_for_box) - set(cols_present))\n",
    "    if missing:\n",
    "        print(f\"Skipping missing metrics (not found in df): {missing}\")\n",
    "    \n",
    "    box_df = (\n",
    "        df_for_box[cols_present]\n",
    "          .melt(var_name=\"metric\", value_name=\"value\")\n",
    "          .dropna()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    box_df.boxplot(by=\"metric\", column=\"value\", ax=ax)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Days\")\n",
    "    ax.set_title(\"Interval distributions (last 4 years)\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.tight_layout()\n",
    "    plot_interval_dists_boxplots = outdir / \"plot_interval_dists_boxplots.png\"\n",
    "    plt.savefig(plot_interval_dists_boxplots, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # Interval distributions by case_type and application_type\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # Interval distributions by case_type\n",
    "    \n",
    "    out_case_type = run_interval_outputs(\n",
    "        typed=typed,\n",
    "        di=di,\n",
    "        group_col=\"case_type\",\n",
    "        outdir=\"data/out/plot/plots/by_case_type\",\n",
    "        interval_metrics=INTERVAL_METRICS,\n",
    "    )\n",
    "\n",
    "    # Interval distributions by application_type\n",
    "\n",
    "    if \"application_type\" in typed.columns and \"application_type\" not in di.columns:\n",
    "        di = di.merge(typed[[\"case_id\", \"application_type\"]], on=\"case_id\", how=\"left\")\n",
    "    \n",
    "    out_app_type = run_interval_outputs(\n",
    "        typed=typed,\n",
    "        di=di,\n",
    "        group_col=\"application_type\",\n",
    "        outdir=\"data/out/plot/plots/by_application_type\",\n",
    "        interval_metrics=INTERVAL_METRICS,\n",
    "    )\n",
    "\n",
    "    # Interval distributions by application_type\n",
    "\n",
    "    if \"concern_type\" in typed.columns and \"concern_type\" not in di.columns:\n",
    "        di = di.merge(typed[[\"case_id\", \"concern_type\"]], on=\"case_id\", how=\"left\")\n",
    "    \n",
    "    out_app_type = run_interval_outputs(\n",
    "        typed=typed,\n",
    "        di=di,\n",
    "        group_col=\"concern_type\",\n",
    "        outdir=\"data/out/plot/plots/by_concern_type\",\n",
    "        interval_metrics=INTERVAL_METRICS,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Quick sanity check (optional)\n",
    "    print(\"Metrics present in di:\", [m for m in INTERVAL_METRICS if m in di.columns])\n",
    "    print(\"Metrics missing from di:\", [m for m in INTERVAL_METRICS if m not in di.columns])\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Add the extra breakdowns for (risk, application type, legal reviewed)\n",
    "    # - Time intervals for new case starts broken down by: case_type, risk, application_type, legal_review\n",
    "    # - Time from allocated → PG sign-off broken down by same\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # Make sure legal_review is numeric (also fixes your \"category mean\" error later)\n",
    "    typed = typed.copy()\n",
    "    typed[\"legal_review\"] = pd.to_numeric(typed.get(\"legal_review\"), errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    \n",
    "    # If risk_band is missing, create a simple one from 'risk' if present\n",
    "    if \"status\" not in typed.columns and \"status\" in typed.columns:\n",
    "        typed[\"status\"] = typed[\"status\"].astype(\"string\")\n",
    "    \n",
    "    # Ensure the same columns exist in di (merge from typed by case_id if needed)\n",
    "    need_in_di = [\"case_id\", \"case_type\", \"status\", \"application_type\", \"legal_review\"]\n",
    "    missing_in_di = [c for c in need_in_di if c not in di.columns]\n",
    "    if missing_in_di and \"case_id\" in di.columns and \"case_id\" in typed.columns:\n",
    "        di = di.merge(typed[[\"case_id\"] + [c for c in missing_in_di if c in typed.columns]],\n",
    "                      on=\"case_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # “New case start interval” distributions (frequency + summary)\n",
    "    # - This uses new case start rows (event_newcase==1) and summarises the “gap since last pickup”.\n",
    "    # Time from allocated → PG sign-off broken down by same\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    from pathlib import Path\n",
    "    outdir = Path(\"data/out/plot/plots/newcase_intervals\")\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Use whichever column you actually have for the gap-on-newcase rows\n",
    "    # Prefer: inter_pickup_days (if it exists)\n",
    "    gap_col = \"inter_pickup_days\" if \"inter_pickup_days\" in di.columns else \"time_since_last_pickup\"\n",
    "    \n",
    "    newcase = di.loc[di.get(\"event_newcase\") == 1].copy()\n",
    "    newcase[gap_col] = pd.to_numeric(newcase[gap_col], errors=\"coerce\")\n",
    "    \n",
    "    # Create the same bands you’ve been using (if gap_band not present)\n",
    "    if \"gap_band\" not in newcase.columns:\n",
    "        newcase[\"gap_band\"] = pd.cut(\n",
    "            newcase[gap_col],\n",
    "            bins=[-0.1, 7, 14, 28, 91, np.inf],\n",
    "            labels=[\"<1 week\", \"1–2 weeks\", \"2–4 weeks\", \"4–13 weeks\", \">13 weeks\"],\n",
    "        )\n",
    "    \n",
    "    # (1) Summary stats by requested breakdown\n",
    "    breakdowns = [\"case_type\", \"status\", \"application_type\", \"legal_review\"]\n",
    "    for b in breakdowns:\n",
    "        if b in newcase.columns:\n",
    "            summ = interval_summary(newcase, metrics=[gap_col], group_cols=[b])\n",
    "            display(summ.head(30))\n",
    "            summ.to_csv(outdir / f\"summary_gap_by_{b}.csv\", index=False)\n",
    "    \n",
    "    # (2) Frequency distributions (counts in bands) by each breakdown\n",
    "    for b in breakdowns:\n",
    "        if b in newcase.columns:\n",
    "            freq = (\n",
    "                newcase.groupby([b, \"gap_band\"], dropna=False)[gap_col]\n",
    "                .size()\n",
    "                .reset_index(name=\"n\")\n",
    "            )\n",
    "            freq.to_csv(outdir / f\"freq_gapband_by_{b}.csv\", index=False)\n",
    "    \n",
    "            # quick plot: stacked bars is messy; do a simple “top line” plot per band total\n",
    "            pivot = freq.pivot_table(index=b, columns=\"gap_band\", values=\"n\", fill_value=0)\n",
    "            fig, ax = plt.subplots(figsize=(9, 4))\n",
    "            for col in pivot.columns:\n",
    "                ax.plot(pivot.index.astype(str), pivot[col].values, label=str(col))\n",
    "            ax.set_title(f\"New case start gaps: frequency by {b} (by gap band)\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45)\n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(outdir / f\"freq_gapband_lines_by_{b}.png\", dpi=200)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # “Allocated → PG sign-off” interval broken down the same way\n",
    "    # ------------------------------------------------------\n",
    "    outdir2 = Path(\"data/out/plot/plots/alloc_to_pg_signoff\")\n",
    "    outdir2.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # compute alloc -> signoff days (only where dates exist)\n",
    "    typed = typed.copy()\n",
    "    typed[\"days_alloc_to_pg_signoff\"] = (\n",
    "        (pd.to_datetime(typed[\"dt_pg_signoff\"], errors=\"coerce\") -\n",
    "         pd.to_datetime(typed[\"dt_alloc_invest\"], errors=\"coerce\"))\n",
    "        .dt.days\n",
    "    )\n",
    "    \n",
    "    metric = \"days_alloc_to_pg_signoff\"\n",
    "    \n",
    "    # summary stats by breakdown\n",
    "    breakdowns = [\"case_type\", \"application_type\", \"legal_review\"] # , \"risk_band\"\n",
    "    for b in breakdowns:\n",
    "        if b in typed.columns:\n",
    "            summ = interval_summary(typed, metrics=[metric], group_cols=[b])\n",
    "            display(summ.head(30))\n",
    "            summ.to_csv(outdir2 / f\"summary_{metric}_by_{b}.csv\", index=False)\n",
    "    \n",
    "            # median bar chart\n",
    "            plot_group_median_bar(summ, group_col=b, metric=metric,\n",
    "                                  outpath=outdir2 / f\"median_{metric}_by_{b}.png\")\n",
    "    \n",
    "    # optional: histogram overall\n",
    "    plot_metric_histograms(typed, metric, outpath=outdir2 / f\"hist_{metric}_overall.png\")\n",
    "\n",
    "    \n",
    "    # -----------------------------------------------------------------------\n",
    "    # Legal review rate: break down + “Risk × age” deeper dive\n",
    "    # - Legal review rate by case_type / risk / application_type\n",
    "    # - “Risk × age”: why legal review appears more likely as time increases\n",
    "    # -----------------------------------------------------------------------\n",
    "    \n",
    "    # - Legal review rate by case_type / risk / application_type\n",
    "    outdir3 = Path(\"data/out/plot/plots/legal_review\")\n",
    "    outdir3.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    typed = typed.copy()\n",
    "    typed[\"legal_review\"] = pd.to_numeric(typed[\"legal_review\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "    def legal_rate_table(df, by):\n",
    "        cols = [c for c in by if c in df.columns] + [\"legal_review\", \"case_id\"]\n",
    "        tmp = df[cols].copy()\n",
    "        out = (\n",
    "            tmp.groupby([c for c in by if c in tmp.columns], dropna=False)\n",
    "               .agg(n_cases=(\"case_id\", \"count\"),\n",
    "                    legal_rate=(\"legal_review\", \"mean\"))\n",
    "               .reset_index()\n",
    "        )\n",
    "        return out\n",
    "    \n",
    "    for b in [\"case_type\", \"status\", \"application_type\"]:\n",
    "        if b in typed.columns:\n",
    "            tab = legal_rate_table(typed, by=[b])\n",
    "            display(tab.sort_values(\"legal_rate\", ascending=False).head(30))\n",
    "            tab.to_csv(outdir3 / f\"legal_rate_by_{b}.csv\", index=False)\n",
    "    \n",
    "            # bar plot\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            ax.bar(tab[b].astype(str), tab[\"legal_rate\"].astype(float))\n",
    "            ax.set_title(f\"Legal review rate by {b}\")\n",
    "            ax.set_ylabel(\"Proportion legal_review=1\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45)\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(outdir3 / f\"legal_rate_by_{b}.png\", dpi=200)\n",
    "            plt.show()\n",
    "    \n",
    "    # two-way breakdown: case_type x status (often very informative)\n",
    "    if \"case_type\" in typed.columns and \"status\" in typed.columns:\n",
    "        tab2 = legal_rate_table(typed, by=[\"case_type\", \"status\"])\n",
    "        pivot = tab2.pivot_table(index=\"case_type\", columns=\"status\", values=\"legal_rate\")\n",
    "        display(pivot)\n",
    "    \n",
    "        # heatmap\n",
    "        plot_prob_heatmap(pivot, outpath=outdir3 / \"legal_rate_heatmap_case_type_x_status.png\",\n",
    "                          title=\"Legal review rate (case_type × status)\")\n",
    "\n",
    "\n",
    "\n",
    "    # - “Risk × age”: why legal review appears more likely as time increases\n",
    "    # There are two different “age” concepts people often mix:\n",
    "    # Case age (how long the case has been open)\n",
    "    # Donor age (actual age of the donor — requires linked data)\n",
    "    # Your earlier “Risk × age” slide sounds like case age. If so, it’s very common to see legal review likelihood rise with case age because:\n",
    "    # Accumulation effect: the longer a case stays open, the more opportunities it has to trigger a legal step (this is basically a hazard story).\n",
    "    # Confounding by complexity: complex cases both (a) last longer and (b) need legal review more often.\n",
    "    # Reverse causality: legal review itself can extend duration (queues, waiting for opinion, rework).\n",
    "    # A better way to “explain” it is to estimate hazard of legal review by case age band, optionally controlling for risk/case type.\n",
    "    # If di has daily rows with event_legal and a “case age” (days since received or since allocation):\n",
    "\n",
    "    outdir4 = Path(\"data/out/plot/plots/legal_hazard\")\n",
    "    outdir4.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # pick a case-age column:\n",
    "    # if you already have something like weeks_since_start or time index, use it.\n",
    "    # Otherwise compute case_age_days from di[\"date\"] - di[\"dt_received_inv\"] (if both exist)\n",
    "    if \"case_age_days\" not in di.columns and \"date\" in di.columns and \"dt_received_inv\" in di.columns:\n",
    "        di = di.copy()\n",
    "        di[\"case_age_days\"] = (\n",
    "            pd.to_datetime(di[\"date\"], errors=\"coerce\") -\n",
    "            pd.to_datetime(di[\"dt_received_inv\"], errors=\"coerce\")\n",
    "        ).dt.days\n",
    "    \n",
    "    # age bands\n",
    "    if \"case_age_days\" in di.columns:\n",
    "        di[\"case_age_band\"] = pd.cut(\n",
    "            pd.to_numeric(di[\"case_age_days\"], errors=\"coerce\"),\n",
    "            bins=[-0.1, 7, 14, 28, 56, 91, 182, 365, np.inf],\n",
    "            labels=[\"0–7\",\"8–14\",\"15–28\",\"29–56\",\"57–91\",\"92–182\",\"183–365\",\">365\"],\n",
    "        )\n",
    "    \n",
    "        # hazard = P(event_legal today | in that age band)\n",
    "        if \"event_req_legal_review\" in di.columns:\n",
    "            di[\"event_req_legal_review\"] = pd.to_numeric(di[\"event_req_legal_review\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    \n",
    "            # overall hazard by age band\n",
    "            hz = di.groupby(\"case_age_band\", dropna=False)[\"event_req_legal_review\"].mean().reset_index(name=\"p_legal_today\")\n",
    "            display(hz)\n",
    "    \n",
    "            fig, ax = plt.subplots(figsize=(7, 4))\n",
    "            ax.plot(hz[\"case_age_band\"].astype(str), hz[\"p_legal_today\"].astype(float))\n",
    "            ax.set_title(\"Daily hazard of legal review vs case age band\")\n",
    "            ax.set_ylabel(\"P(legal review today)\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45)\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(outdir4 / \"hazard_legal_by_case_age.png\", dpi=200)\n",
    "            plt.show()\n",
    "    \n",
    "            # hazard by risk band (if present)\n",
    "            if \"status\" in di.columns:\n",
    "                hz2 = (\n",
    "                    di.groupby([\"status\", \"case_age_band\"], dropna=False)[\"event_legal\"]\n",
    "                      .mean()\n",
    "                      .unstack(\"case_age_band\")\n",
    "                )\n",
    "                display(hz2)\n",
    "                plot_prob_heatmap(hz2, outpath=outdir4 / \"hazard_legal_status_x_age.png\",\n",
    "                                  title=\"Daily hazard of legal review (status × case age)\")\n",
    "    \n",
    "    # To do this properly, you need those fields in your typed table (or join them in). The workflow is:\n",
    "    # Join linked investigations + LPA register data into typed on a stable key (often the “LPA/Deputy ID”).\n",
    "    # Make sure the joined fields are clean types:\n",
    "    # - donor_age numeric\n",
    "    # - donor_sex category/string\n",
    "    # - num_attorneys numeric\n",
    "    # - lpa_type category/string\n",
    "    # - time_since_registered_days numeric\n",
    "    \n",
    "    # Produce:\n",
    "    # - legal review rate by each characteristic (1D)\n",
    "    # - 2D interactions that matter (e.g., lpa_type × donor_age_band, num_attorneys_band × case_type)\n",
    "    # - a simple baseline predictive model (logistic regression) as a benchmark for “is it predictable at all?”\n",
    "\n",
    "    # Important modelling note (so you don’t accidentally “cheat”)\n",
    "    # - If your goal is “predict legal review at receipt/allocation”, \n",
    "    # - avoid using post-start variables like final duration, days_to_signoff, etc. Those leak future information.\n",
    "\n",
    "    # Minimal “rate by characteristic” pattern:\n",
    "\n",
    "    # Example: donor age bands (once donor_age exists in typed)\n",
    "    if \"donor_age\" in typed.columns:\n",
    "        typed = typed.copy()\n",
    "        typed[\"donor_age\"] = pd.to_numeric(typed[\"donor_age\"], errors=\"coerce\")\n",
    "        typed[\"donor_age_band\"] = pd.cut(\n",
    "            typed[\"donor_age\"],\n",
    "            bins=[0, 30, 40, 50, 60, 70, 80, 90, 120],\n",
    "            right=False,\n",
    "            labels=[\"<30\",\"30–39\",\"40–49\",\"50–59\",\"60–69\",\"70–79\",\"80–89\",\"90+\"],\n",
    "        )\n",
    "    \n",
    "        tab = typed.groupby(\"donor_age_band\", dropna=False).agg(\n",
    "            n_cases=(\"case_id\", \"count\"),\n",
    "            legal_rate=(\"legal_review\", \"mean\")\n",
    "        ).reset_index()\n",
    "    \n",
    "        display(tab)\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Add “linked characteristics” for predicting legal review \n",
    "    # (LPA type, donor age, sex, #attorneys, time since registered)\n",
    "    # --------------------------------------------------------------\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Probability of new case start vs workload & gap\n",
    "    # ------------------------------------------------------\n",
    "    \n",
    "    # “Rules” about new case starts vs workload & time since last case:\n",
    "    # - From the interval frame di:\n",
    "    # - wip_load – investigators’ weighted caseload on that date\n",
    "    # - time_since_last_pickup – days since they last started a new case\n",
    "    # - event_newcase – indicator that a new case started that day\n",
    "    # - What we want is a conditional probability table:\n",
    "    #     - P(new case today | caseload band, gap since last pickup band)\n",
    "    # - If caseload LOW & gap LONG → probability HIGH?\n",
    "    # - If caseload HIGH & gap LONG → probability LOW?\n",
    "    # etc.\n",
    "\n",
    "    # rules like…” question (workload + time since last pickup)\n",
    "    # Yes — you can infer empirical “rules” like:\n",
    "    # - P(new case start | workload band, gap band)\n",
    "    # - …because that’s exactly what the pickup_prob matrix represents.\n",
    "\n",
    "    # What we can safely say:\n",
    "    # - You’ve estimated conditional probabilities from observed staff-days.\n",
    "    # - Those can be used as a policy rule in simulation (“if an investigator is Low workload and gap is >13 weeks then sample a new case with probability p”).\n",
    "    # What you should not claim without further work:\n",
    "    # - That workload/gap cause allocations (there may be operational policies, staffing changes, backlog availability, etc.)\n",
    "   \n",
    "    # If we want stronger evidence, next step is a simple model:\n",
    "    # - logistic regression (or gradient boosting) predicting event_newcase from \n",
    "    # wip_load, time_since_last_pickup, seasonality, bank holidays, backlog level, team, etc.\n",
    "    # - then compare fitted probabilities to your banded rule table (they should broadly align).\n",
    "\n",
    "\n",
    "    \n",
    "    # “Rules” table – probability of new case start by workload & gap\n",
    "\n",
    "    pickup_df = di[\n",
    "        [\"date\", \"staff_id\", \"wip_load\", \"time_since_last_pickup\", \"event_newcase\"]\n",
    "    ].copy()\n",
    "\n",
    "    # Drop rows where we don't know the gap or caseload\n",
    "    pickup_df = pickup_df.dropna(subset=[\"wip_load\", \"time_since_last_pickup\"])\n",
    "\n",
    "    # Define workload bands (tweak thresholds as needed for OPG)\n",
    "    pickup_df[\"wip_band\"] = pd.cut(\n",
    "        pickup_df[\"wip_load\"],\n",
    "        #bins=[0, 40, 80, 120, float(\"inf\")],\n",
    "        bins=[0, 2, 3, 4, float(\"inf\")],\n",
    "        labels=[\"Low\", \"Medium\", \"High\", \"Very high\"],\n",
    "        right=False,\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "    # Define time-since-last-pickup bands (gap in days)\n",
    "    pickup_df[\"gap_band\"] = pd.cut(\n",
    "        pickup_df[\"time_since_last_pickup\"],\n",
    "        bins=[0, 7, 14, 28, 90, float(\"inf\")],\n",
    "        labels=[\"<1 week\", \"1–2 weeks\", \"2–4 weeks\", \"4–13 weeks\", \">13 weeks\"],\n",
    "        right=False,\n",
    "        include_lowest=True,\n",
    "    )\n",
    "\n",
    "    # Probability: mean of event_newcase within each (wip_band, gap_band)\n",
    "    pickup_prob = (\n",
    "        pickup_df\n",
    "        .groupby([\"wip_band\", \"gap_band\"], dropna=False, observed=False)[\"event_newcase\"]\n",
    "        .mean()\n",
    "        .unstack(\"gap_band\")\n",
    "    )\n",
    "\n",
    "    # Counts: how many staff-days in each cell (for reliability)\n",
    "    pickup_counts = (\n",
    "        pickup_df\n",
    "        .groupby([\"wip_band\", \"gap_band\"], dropna=False, observed=False)[\"event_newcase\"]\n",
    "        .size()\n",
    "        .unstack(\"gap_band\")\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== P(new case start | workload band, gap band) ===\")\n",
    "    print(pickup_prob)\n",
    "\n",
    "    print(\"\\n=== Number of staff-days underlying each cell ===\")\n",
    "    print(pickup_counts)\n",
    "\n",
    "    # Convert to long / tidy format\n",
    "    prob_long = (\n",
    "        pickup_prob\n",
    "        .reset_index()\n",
    "        .melt(id_vars=\"wip_band\", var_name=\"gap_band\", value_name=\"prob_new_case\")\n",
    "    )\n",
    "    \n",
    "    counts_long = (\n",
    "        pickup_counts\n",
    "        .reset_index()\n",
    "        .melt(id_vars=\"wip_band\", var_name=\"gap_band\", value_name=\"staff_days\")\n",
    "    )\n",
    "    \n",
    "    rules_df = (\n",
    "        prob_long\n",
    "        .merge(counts_long, on=[\"wip_band\", \"gap_band\"])\n",
    "        .sort_values([\"wip_band\", \"gap_band\"])\n",
    "    )\n",
    "    \n",
    "    print(\"===== “Rules” table – probability of new case start by workload & gap =====\")\n",
    "    print(rules_df)\n",
    "\n",
    "    # rules_low = pd.DataFrame({\n",
    "    #     'C': ['<1 week','1–2 weeks','2–4 weeks','4–13 weeks','>13 weeks'],\n",
    "    #     'prob_new_case': [0.058057,0.033535,0.048967,0.123724,0.142045],\n",
    "    #     'staff_days': [7441,4294,3145,1665,352]\n",
    "    # })\n",
    "    # Keep only the Low workload band (your chart title says Low)\n",
    "    rules_low = (\n",
    "        rules_df.loc[\n",
    "            rules_df[\"wip_band\"].eq(\"Low\"),\n",
    "            [\"gap_band\", \"prob_new_case\", \"staff_days\"]\n",
    "        ]\n",
    "        .copy()\n",
    "    )\n",
    "    \n",
    "    # Keep only cells that actually have observations\n",
    "    rules_low[\"staff_days\"] = pd.to_numeric(rules_low[\"staff_days\"], errors=\"coerce\")\n",
    "    rules_low = rules_low.loc[rules_low[\"staff_days\"].fillna(0) > 0]\n",
    "    \n",
    "    # Convert <NA> -> NaN -> float (matplotlib-safe)\n",
    "    rules_low[\"prob_new_case\"] = pd.to_numeric(rules_low[\"prob_new_case\"], errors=\"coerce\")\n",
    "    \n",
    "    # Optional: enforce sensible x-axis order\n",
    "    gap_order = [\"<1 week\", \"1–2 weeks\", \"2–4 weeks\", \"4–13 weeks\", \">13 weeks\"]\n",
    "    rules_low[\"gap_band\"] = pd.Categorical(rules_low[\"gap_band\"], categories=gap_order, ordered=True)\n",
    "    rules_low = rules_low.sort_values(\"gap_band\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.bar(rules_low[\"gap_band\"].astype(str), rules_low[\"prob_new_case\"].astype(float))\n",
    "    ax.set_ylabel(\"P(new case today)\")\n",
    "    ax.set_title(\"Probability of new case vs gap since last pickup\\n(Low workload band)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # Convert to float for plotting; <NA> becomes NaN\n",
    "    prob = pickup_prob.astype(\"float\")\n",
    "    counts = pickup_counts.astype(\"float\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "    im = ax.imshow(prob.values, aspect=\"auto\")\n",
    "    \n",
    "    ax.set_xticks(range(prob.shape[1]))\n",
    "    ax.set_xticklabels(prob.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(prob.shape[0]))\n",
    "    ax.set_yticklabels(prob.index)\n",
    "    \n",
    "    ax.set_title(\"P(new case start | workload band, gap band)\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # annotate with counts\n",
    "    for i in range(prob.shape[0]):\n",
    "        for j in range(prob.shape[1]):\n",
    "            p = prob.values[i, j]\n",
    "            n = counts.values[i, j]\n",
    "            if np.isfinite(p):\n",
    "                ax.text(j, i, f\"{p:.3f}\\\\n(n={int(n)})\", ha=\"center\", va=\"center\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_P_new_case_workload_gap_band = outdir / \"plot_P_new_case_workload_gap_band.png\"\n",
    "    plt.savefig(plot_P_new_case_workload_gap_band, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"If your table only shows Low and everything else is empty: that usually means your wip_band calculation (or the WIP itself) is not varying in the dataset you’re feeding into the rule-table. The fastest check is:\")\n",
    "\n",
    "    # Always show details\n",
    "    # daily[[\"wip\", \"wip_load\"]].describe()\n",
    "    # daily[\"wip_band\"].value_counts(dropna=False)\n",
    "\n",
    "\n",
    "    # Heatmap of “rules” (visual fuzzy rule surface)\n",
    "    # If you later get non-zero values for Medium/High/Very high, this becomes a really intuitive “rule map” for a fuzzy system.    \n",
    "    # Ensure the matrix is truly numeric (not object / pd.NA)\n",
    "    prob_matrix = pickup_prob.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    print(prob_matrix.dtypes)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    im = ax.imshow(prob_matrix.to_numpy(dtype=float), aspect=\"auto\", vmin=0, vmax=1)\n",
    "\n",
    "    \n",
    "    ax.set_xticks(range(len(prob_matrix.columns)))\n",
    "    ax.set_xticklabels(prob_matrix.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(prob_matrix.index)))\n",
    "    ax.set_yticklabels(prob_matrix.index)\n",
    "    \n",
    "    ax.set_xlabel(\"Gap band (time since last pickup)\")\n",
    "    ax.set_ylabel(\"WIP band (current caseload)\")\n",
    "    ax.set_title(\"P(new case start | workload, gap)\")\n",
    "    \n",
    "    fig.colorbar(im, ax=ax, label=\"Probability of new case start\")\n",
    "    plt.tight_layout()\n",
    "    plot_heatmaps_rules_fuzzy = outdir / \"plot_heatmaps_rules_fuzzy.png\"\n",
    "    plt.savefig(plot_heatmaps_rules_fuzzy, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # year-on-year interval changes\n",
    "    alloc_change = interval_change_distribution(\n",
    "        typed,\n",
    "        interval_col=\"days_to_alloc\",\n",
    "        date_col=\"dt_received_inv\",  # <- real received-date column\n",
    "        group=\"case_type\",\n",
    "    )\n",
    "\n",
    "    alloc_annual_stats = alloc_change[\"annual_stats\"]\n",
    "    alloc_yoy_change = alloc_change[\"yoy_change\"]\n",
    "\n",
    "    print(\"\\n=== Annual distributions of days_to_alloc by case_type ===\")\n",
    "    print(alloc_annual_stats.head())\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    for ct, grp in alloc_annual_stats.groupby(\"case_type\"):\n",
    "        ax.plot(grp[\"year\"], grp[\"median\"], marker=\"o\", label=ct)\n",
    "    \n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Median days from received to allocation\")\n",
    "    ax.set_title(\"Median allocation delay by case type over time\")\n",
    "    ax.legend(title=\"Case type\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plot_Annual_dist_days_to_alloc_b_case_type = outdir / \"plot_Annual_dist_days_to_alloc_b_case_type.png\"\n",
    "    plt.savefig(plot_Annual_dist_days_to_alloc_b_case_type, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    main_types = [\"Aspect\", \"Fraud\", \"Investigation\", \"Multiple\", \"Multiple Sub\", \"TPO\"]\n",
    "    mask = alloc_annual_stats[\"case_type\"].isin(main_types)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for ct, grp in alloc_annual_stats[mask].groupby(\"case_type\"):\n",
    "        ax.plot(grp[\"year\"], grp[\"median\"], marker=\"o\", label=ct)\n",
    "    \n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Median days to allocation\")\n",
    "    ax.set_title(\"Median allocation delay for main case types\")\n",
    "    ax.legend(title=\"Case type\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plot_Annual_dist_days_to_alloc_restricted_case_type = outdir / \"plot_Annual_dist_days_to_alloc_restricted_case_type.png\"\n",
    "    plt.savefig(plot_Annual_dist_days_to_alloc_restricted_case_type, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n=== Year-on-year change in median days_to_alloc by case_type ===\")\n",
    "    print(alloc_yoy_change.head())\n",
    "\n",
    "    # Focus on realistic years to avoid huge early artefacts if needed\n",
    "    yoy_df = alloc_yoy_change.copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    for ct, grp in yoy_df.groupby(\"case_type\"):\n",
    "        ax.plot(grp[\"year\"], grp[\"yoy_median_change\"], marker=\"o\", label=ct)\n",
    "    \n",
    "    ax.axhline(0, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Change in median days to allocation vs previous year\")\n",
    "    ax.set_title(\"Year-on-year change in allocation delays by case type\")\n",
    "    ax.legend(title=\"Case type\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plot_alloc_yoy_change = outdir / \"plot_alloc_yoy_change.png\"\n",
    "    plt.savefig(plot_alloc_yoy_change, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"===== TREND ANALYSIS =====\")\n",
    "    # Trend Analysis\n",
    "    trend = IntervalAnalysis.monthly_trend(\n",
    "        di, metric=\"days_to_pg_signoff\", agg=\"median\", by=[\"case_type\"]\n",
    "    ).copy()\n",
    "    trend[\"month\"] = pd.to_datetime(trend[\"yyyymm\"] + \"-01\")\n",
    "    \n",
    "    print(\"\\n=== INTERVAL TREND HEAD ===\")\n",
    "    print(trend.head())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    ax.plot(trend[\"month\"], trend[\"days_to_pg_signoff\"], marker=\"o\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Median days to PG signoff\")\n",
    "    ax.set_title(\"Monthly median days to PG signoff – ALL case types\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plot_trend_analysis = outdir / \"plot_trend_analysis.png\"\n",
    "    plt.savefig(plot_trend_analysis, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Faceted by case_type (simple overlay version)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    \n",
    "    for ct, grp in trend.groupby(\"case_type\"):\n",
    "        ax.plot(grp[\"month\"], grp[\"days_to_pg_signoff\"], marker=\"o\", linewidth=1, label=ct)\n",
    "    \n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Median days to PG signoff\")\n",
    "    ax.set_title(\"Monthly median PG signoff times by case type\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    ax.legend(title=\"Case type\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plot_trend_analysis_by_case_type = outdir / \"plot_trend_analysis_by_case_type.png\"\n",
    "    plt.savefig(plot_trend_analysis_by_case_type, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Expand Above for the FTE and investigator invertal destributions\n",
    "    \n",
    "    interval_dists_by_fte = IntervalAnalysis.analyse_interval_distributions(\n",
    "        di, by=[\"fte\"]\n",
    "    )\n",
    "    interval_dists_by_fte.pop(\"days_alloc_to_req_legal_review\", None)\n",
    "    \n",
    "    interval_dists_by_investigator = IntervalAnalysis.analyse_interval_distributions(\n",
    "        di, by=[\"staff_id\"]\n",
    "    )\n",
    "    interval_dists_by_investigator.pop(\"days_alloc_to_req_legal_review\", None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cfg = EDAConfig(\n",
    "        id_col=\"case_id\",\n",
    "        date_received=\"dt_received_inv\",\n",
    "        date_allocated=\"dt_alloc_invest\",\n",
    "        date_signed_off=\"dt_pg_signoff\",\n",
    "    )\n",
    "    cfg.numeric_cols = [\n",
    "        \"days_to_alloc\",\n",
    "        \"days_to_signoff\",\n",
    "        \"legal_review\",\n",
    "        \"fte\",\n",
    "        \"weighting\",\n",
    "    ]\n",
    "    eda = OPGInvestigationEDA(typed, cfg)\n",
    "    print(\"=== EDA COLUMNS ===\")\n",
    "    print(eda.df.columns.tolist())\n",
    "    \n",
    "    # --- EDA code from demo_eda.py ---\n",
    "    \n",
    "    print(\"=== EDA OVERVIEW ===\")\n",
    "    overview = eda.quick_overview()\n",
    "    #print(overview)\n",
    "\n",
    "    print(\"=== EDA MISSING ===\")\n",
    "    missing_pct = eda.missingness_matrix()\n",
    "    missing_vs_target = eda.missing_vs_target(\"days_to_signoff\")#, \"legal_review\")\n",
    "    outliers_signoff = eda.iqr_outliers(\"days_to_signoff\")\n",
    "    outliers_allocate = eda.iqr_outliers(\"days_to_alloc\")\n",
    "    #cat_summary = eda.group_summary([\"case_type\", \"risk_band\"], target=\"legal_review\")\n",
    "    # cat_summary = eda.group_summary(\n",
    "    #     [\"case_type\", \"risk_band\"], \n",
    "    #     metrics={\"legal_rate\": (\"legal_review\", \"mean\")} \n",
    "    # ) #\"n\": (\"id\", \"count\"), \n",
    "    weight_summary = eda.group_summary( \n",
    "        by=[\"weighting\"],\n",
    "        metrics={ #\"new_column_name\": (\"existing_column_name\", \"aggfunc\")\n",
    "            \"n_cases\": (\"case_id\", \"count\"),\n",
    "            \"legal_rate\": (\"legal_review\", \"mean\"),          # proportion of cases with legal_review=1\n",
    "            \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"), # \"aggfunc\" one of: \"count\", \"mean\", \"median\", \"min\", \"max\", \"std\", etc.\n",
    "        },\n",
    "    )\n",
    "\n",
    "    case_weight_summary = eda.group_summary(\n",
    "        by=[\"case_type\", \"weighting\"],\n",
    "        metrics={\n",
    "            \"n_cases\": (\"case_id\", \"count\"),\n",
    "            \"median_days_to_alloc\": (\"days_to_alloc\", \"median\"),\n",
    "            \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"),\n",
    "        },\n",
    "    )\n",
    "    case_weight_summary = case_weight_summary.sort_values([\"case_type\", \"weighting\"])\n",
    "\n",
    "    legal_review_by_case_type = eda.group_summary(\n",
    "        by=[\"case_type\"],\n",
    "        metrics={\n",
    "            #\"avg_backlog\": (\"backlog\", \"mean\"),\n",
    "            \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    legal_review_by_case_status = eda.group_summary(\n",
    "        by=[\"case_type\", \"status\"],\n",
    "        metrics={\n",
    "            #\"avg_backlog\": (\"backlog\", \"mean\"),\n",
    "            \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    staff_summary = eda.group_summary(\n",
    "        by=[\"staff_id\", \"fte\"],\n",
    "        metrics={\n",
    "            \"n_cases\": (\"case_id\", \"count\"),\n",
    "            \"mean_days_to_alloc\": (\"days_to_alloc\", \"mean\"),\n",
    "            \"mean_days_to_signoff\": (\"days_to_signoff\", \"mean\"),\n",
    "            \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fte_weight_summary = eda.group_summary(\n",
    "        by=[\"fte\"],\n",
    "        metrics={\n",
    "            \"total_weight\": (\"weighting\", \"sum\"),\n",
    "            \"avg_weight\": (\"weighting\", \"mean\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    case_weight_full = eda.group_summary(\n",
    "        by=[\"case_type\", \"weighting\"],\n",
    "        metrics={\n",
    "            \"staff_id\": (\"staff_id\", \"count\"),\n",
    "            #\"avg_backlog\": (\"backlog\", \"mean\"),\n",
    "            \"median_days_to_alloc\": (\"days_to_alloc\", \"median\"),\n",
    "            \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"),\n",
    "            \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "        },\n",
    "    )\n",
    "    case_weight_full = case_weight_full.sort_values([\"case_type\", \"weighting\"])\n",
    "\n",
    "\n",
    "    status_summary = eda.group_summary(\n",
    "        by=[\"status\"],\n",
    "        metrics={\n",
    "            \"staff_id\": (\"staff_id\", \"count\"),\n",
    "            \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "            \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"),\n",
    "            \"median_days_to_alloc\": (\"days_to_alloc\", \"median\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    legal_case_summary = eda.group_summary(\n",
    "        by=[\"case_type\", \"legal_review\"],\n",
    "        metrics={\n",
    "            \"staff_id\": (\"staff_id\", \"count\"),\n",
    "            \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"),\n",
    "            \"median_days_to_alloc\": (\"days_to_alloc\", \"median\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    corrs = eda.numeric_correlations(method=\"spearman\")\n",
    "    class_balance = eda.imbalance_summary()\n",
    "    leakage_hits = eda.leakage_scan([\"post\", \"signed\", \"decision\"])\n",
    "    interaction = eda.binned_interaction_rate(\n",
    "        num_col=\"days_to_alloc\",\n",
    "        cat_col=\"weighting\",\n",
    "        target=\"legal_review\",\n",
    "    )\n",
    "    \n",
    "    # ts_7d, lag_corrs = eda.resample_time_series(\n",
    "    #     metrics={\"days_to_alloc\": (\"days_to_alloc\", \"last\"), \n",
    "    #              \"staff_count\": (\"staff_id\", \"count\")}\n",
    "    # )\n",
    "    \n",
    "    # km_q = eda.km_quantiles_by_group(group=\"weighting\")\n",
    "    # monthly_kpis = eda.monthly_kpis()\n",
    "    cramers_case_type_w = eda.cramers_v(typed[\"case_type\"],typed[\"weighting\"])\n",
    "    cramers_case_type_fte = eda.cramers_v(typed[\"case_type\"],typed[\"fte\"])\n",
    "    \n",
    "    \n",
    "    # --- END EDA code ---\n",
    "\n",
    "\n",
    "\n",
    "    def weighted_mean(values, weights):\n",
    "        v = np.asarray(values)\n",
    "        w = np.asarray(weights)\n",
    "        mask = ~np.isnan(v) & ~np.isnan(w)\n",
    "        if mask.sum() == 0:\n",
    "            return np.nan\n",
    "        return (v[mask] * w[mask]).sum() / w[mask].sum()\n",
    "    \n",
    "    # weighted mean days_to_signoff by case_type\n",
    "    weighted_signoff = (\n",
    "        eda.df\n",
    "        .groupby(\"case_type\")\n",
    "        .apply(lambda g: weighted_mean(g[\"days_to_signoff\"], g[\"weighting\"]))\n",
    "        .reset_index(name=\"w_mean_days_to_signoff\")\n",
    "    )\n",
    "\n",
    "    # weighted mean days_to_alloc by case_type\n",
    "    weighted_alloc = (\n",
    "        eda.df\n",
    "        .groupby(\"case_type\")\n",
    "        .apply(lambda g: weighted_mean(g[\"days_to_alloc\"], g[\"weighting\"]))\n",
    "        .reset_index(name=\"w_mean_days_to_alloc\")\n",
    "    )\n",
    "    \n",
    "    # Call your plotting function for the interval and trends\n",
    "    results = plot_pg_signoff_monthly_trends(di,\"data/out/plot/plots\")\n",
    "    # Extract for returning\n",
    "    trend_all = results[\"trend_all\"]\n",
    "    plot_paths = results[\"plots\"]\n",
    "\n",
    "    results_alloc = plot_allocation_monthly_trends(di,\"data/out/plot/plots\")\n",
    "    # Extract for returning\n",
    "    trend_all_alloc = results_alloc[\"trend_all\"]\n",
    "    plot_paths_alloc = results_alloc[\"plots\"]\n",
    "\n",
    "    print(\"=== LEGAL REVIEW, DATE OF LEGAL REQUEST, STATUS ===\")\n",
    "    typed[[\"legal_review\", \"dt_legal_review_req1\", \"status\"]].head()\n",
    "    print(\"=== LEGAL REVIEW COUNTS ===\")\n",
    "    typed[\"legal_review\"].value_counts()\n",
    "    print(\"=== STATUS ===\")\n",
    "    print(typed[\"status\"].value_counts(dropna=False))\n",
    "\n",
    "    # status_summary = eda.group_summary(\n",
    "    #     by=[\"status\"],\n",
    "    #     metrics={\n",
    "    #         \"n_cases\": (\"id\", \"count\"),\n",
    "    #         \"legal_rate\": (\"legal_review\", \"mean\"),\n",
    "    #         \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"),\n",
    "    #         \"median_days_to_alloc\": (\"days_to_alloc\", \"median\"),\n",
    "    #     },\n",
    "    # )\n",
    "    # print(status_summary)\n",
    "    \n",
    "    # risk_legal_summary = eda.group_summary(\n",
    "    #     by=[\"risk_band\", \"legal_review\"],\n",
    "    #     metrics={\n",
    "    #         \"n_cases\": (\"id\", \"count\"),\n",
    "    #         \"median_days_to_signoff\": (\"days_to_signoff\", \"median\"),\n",
    "    #     },\n",
    "    # )\n",
    "    # print(risk_legal_summary)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # Fuzzy inference + micro-simulation: is it appropriate?\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # It can be a very good fit if the stakeholder goal is:\n",
    "    # - transparent “human-like” decision logic\n",
    "    # - smooth transitions between LOW/MED/HIGH (instead of hard thresholds)\n",
    "    # - scenario testing (e.g., “what if we increase staffing”, “what if we change triage rules”)\n",
    "    \n",
    "    # How I’d use it here:\n",
    "    # - Use your empirical tables / fitted model to calibrate membership functions and rule strengths.\n",
    "    # - Use fuzzy inference to output:\n",
    "    #    - probability of picking up a new case today\n",
    "    #    - probability of escalating to legal today (hazard)\n",
    "    #    - probability of closing today (hazard)\n",
    "    # - Then your micro-sim steps day-by-day.\n",
    "    \n",
    "    # Big watch-outs:\n",
    "    # - Fuzzy systems are easy to write and easy to miscalibrate; you still need a validation loop:\n",
    "    #    - reproduce historical backlog curve\n",
    "    #    - reproduce distributions (pickup gaps, time-to-signoff, legal review rates)\n",
    "    #    - reproduce variation by team/case type\n",
    "    \n",
    "    # A pragmatic compromise that often works well:\n",
    "    # - Fit a simple statistical model first (logistic / survival / gradient boosting),\n",
    "    # - Then translate it into fuzzy rules for interpretability.\n",
    "\n",
    "    return {\n",
    "        \"raw\": raw,\n",
    "        \"typed\": typed,\n",
    "        \"daily\": daily,\n",
    "        \"backlog\": backlog_ts,\n",
    "        \"events\": events,\n",
    "        \"backlog_ts\": backlog_ts,\n",
    "        \"di\": di,\n",
    "        \"trend\": trend,\n",
    "        \"trend_all\": trend_all,\n",
    "        \"trend_all_alloc\": trend_all_alloc,\n",
    "        \"eda\": {\n",
    "            \"cfg\": cfg,\n",
    "            \"overview\": overview,\n",
    "            \"missing_pct\": missing_pct,\n",
    "            \"missing_vs_target\": missing_vs_target,\n",
    "            \"outliers_signoff\": outliers_signoff,\n",
    "            \"outliers_allocate\": outliers_allocate,\n",
    "            \"weight_summary\": weight_summary,\n",
    "            \"case_weight_summary\": case_weight_summary,\n",
    "            \"legal_review_by_case_type\": legal_review_by_case_type,\n",
    "            \"legal_review_by_case_status\": legal_review_by_case_status,\n",
    "            \"staff_summary\": staff_summary,\n",
    "            \"fte_weight_summary\": fte_weight_summary,\n",
    "            \"status_summary\": status_summary,\n",
    "            \"case_weight_full\": case_weight_full,\n",
    "            \"legal_case_summary\": legal_case_summary,\n",
    "            \"corrs\": corrs,\n",
    "            \"cramers_case_type_w\": cramers_case_type_w,\n",
    "            \"cramers_case_type_fte\": cramers_case_type_fte,\n",
    "            \"class_balance\": class_balance,\n",
    "            \"leakage_hits\": leakage_hits,\n",
    "            \"interaction\": interaction,\n",
    "            #\"ts_7d\": ts_7d,\n",
    "            #\"lag_corrs\": lag_corrs,\n",
    "            #\"km_quantiles\": km_q,\n",
    "            #\"monthly_kpis\": monthly_kpis,\n",
    "        },\n",
    "        \"interval_dists_overall\": interval_dists_overall,\n",
    "        \"interval_dists_by_case_type\": out_case_type,\n",
    "        \"interval_dists_by_app_type\": out_app_type,\n",
    "        \"interval_dists_by_fte\": interval_dists_by_fte,\n",
    "        \"interval_dists_by_investigator\": interval_dists_by_investigator,\n",
    "        \"pickup_prob\": pickup_prob,\n",
    "        \"pickup_counts\": pickup_counts,\n",
    "        \"alloc_annual_stats\": alloc_annual_stats,\n",
    "        \"alloc_yoy_change\": alloc_yoy_change,\n",
    "        \"weighted_signoff\": weighted_signoff,\n",
    "        \"weighted_alloc\": weighted_alloc,\n",
    "        \"monthly_case_flow_counts\": monthly_flows,\n",
    "        \"monthly_flow_outputs\": monthly_flow_outputs,\n",
    "        \"plot_paths\": plot_paths,\n",
    "        \"plot_paths_alloc\": plot_paths_alloc,\n",
    "        #\"plots\": plot_paths,\n",
    "    }\n",
    "\n",
    "# from demo_pipeline import demo_all\n",
    "outputs = demo_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
