{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Heatstroke Risk Factor Analysis Notebook\n",
    "\n",
    "**Author:** Leila Yousefi   \n",
    "**Date:** 24/07/2025 \n",
    "**Objective:** \n",
    "- Determining the factors affecting the risk of death from heatstroke in the UK.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Installations & Imports\n",
    "\n",
    "## 2. Data pre-processing\n",
    "### 2.1. load csv file into a dataframe\n",
    "### 2.2. Summary statistics\n",
    "### 2.3 Data Quality Checks & Solutions\n",
    "#### 2.3.1 Validation\n",
    "#### 2.3.2 Completeness\n",
    "#### 2.3.3 Uniqueness\n",
    "#### 2.3.4 Accuracy/Consistency\n",
    "\n",
    "## 3. Exploratory Data Analysis\n",
    "### 3.1 Univariate distributions\n",
    "### 3.2 Bivariate relationships\n",
    "\n",
    "## 4. Feature Engineering & Modelling\n",
    "### 4.1 Train/test split\n",
    "\n",
    "\n",
    "## 5. Evaluation & Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Installations & Imports: Adjust or add libraries as needed for the task.\n",
    "\n",
    "# suppress that specific package RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*invalid value encountered in cast.*\"\n",
    ")\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# data libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# viz libs\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import KBinsDiscretizer   # Discretize continuous data\n",
    "from statsmodels.tsa.stattools import grangercausalitytests  # Granger causality tests\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "# After transfering the code into py file and move it to src\n",
    "# from src.analysers import (\n",
    "#     EDAAnalyser, FeatureEngineer, ModelTrainer,\n",
    "#     CorrelationAnalyser, RandomizationAnalyser, CausalityAnalyser\n",
    "# )\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Working directory\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Notebooks are here:\", os.listdir())\n",
    "\n",
    "# set paths\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "\n",
    "# Date\n",
    "now = datetime.now()\n",
    "print(f\"→ Today's Date: {now.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data pre-processing: Point the filepaths to data/raw/ and load data.\n",
    "\n",
    "### 2.1. load csv file into a dataframe\n",
    "filename = 'G7_Summer_2025_dataset.csv'\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, filename), low_memory=False)\n",
    "\n",
    "# Display the first few records\n",
    "df.head()\n",
    "# Preview the first few rows\n",
    "\n",
    "# Show shape and missing values\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# Encode arrival target and drop missing values\n",
    "df['Death'] = df['Death From Heatstroke'].astype(int)\n",
    "df = df.dropna(subset=['Age', 'Height', 'Weight', 'Death'])\n",
    "# Compute BMI and remove infinities\n",
    "df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Summary statistics for numeric and categorical columns\n",
    "### 2.2 Summary statistics & missing values\n",
    "df.info()\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "# 2. Define helper functions\n",
    "\n",
    "def cramers_v(x, y) -> float:\n",
    "    \"\"\"\n",
    "    Compute Cramér's V for two categorical variables.\n",
    "    \"\"\"\n",
    "    confusion = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion)[0]\n",
    "    n = confusion.sum().sum()\n",
    "    k = min(confusion.shape)\n",
    "    return np.sqrt(chi2 / (n * (k - 1) + 1e-12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 3. Compute correlations for all features\n",
    "results_corr = {}\n",
    "target = 'Death'\n",
    "for col in df.columns:\n",
    "    if col == target:\n",
    "        continue\n",
    "    series = df[col]\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        corr, _ = pointbiserialr(series, df[target])\n",
    "    else:\n",
    "        corr = cramers_v(series, df[target])\n",
    "    results_corr[col] = abs(corr)\n",
    "\n",
    "# Display top 5 by correlation\n",
    "sorted(results_corr.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the dataset\n",
    "df\n",
    "\n",
    "# 2. Encode target and drop rows with missing critical values\n",
    "df['Death'] = df['Death From Heatstroke'].astype(int)\n",
    "df = df.dropna(subset=['Age', 'Height', 'Weight', 'Blood Type', 'Death'])\n",
    "\n",
    "# 3. Feature engineering: compute BMI and clean infinities\n",
    "df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna(subset=['BMI'])\n",
    "\n",
    "# 4. Prepare feature matrix X and target y\n",
    "categorical_cols = [\n",
    "    'Geography', 'Gender', 'Occupation', 'Blood Type',\n",
    "    'Care Home', 'Cardiovascular Disease', 'Dementia',\n",
    "    'Respiratory Illness', 'Housing', 'Lives In Countryside'\n",
    "]\n",
    "X = pd.get_dummies(df[['Age', 'IMD', 'BMI'] + categorical_cols], drop_first=True)\n",
    "y = df['Death']\n",
    "\n",
    "# 5. Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 6. Fit a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate model performance\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# 8. Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "plt.title('ROC Curve for Heatstroke Death Prediction')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.savefig('Top 10 Features by Absolute Coefficient Value.png')\n",
    "plt.show()\n",
    "\n",
    "# 9. Inspect the top features by absolute coefficient\n",
    "coefs = pd.Series(model.coef_[0], index=X.columns)\n",
    "top_features = coefs.reindex(coefs.abs().sort_values(ascending=False).index).head(10)\n",
    "print(\"Top 10 Features by Absolute Coefficient Value:\")\n",
    "print(top_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode arrival target and drop missing values\n",
    "df['Death'] = df['Death From Heatstroke'].astype(int)\n",
    "df = df.dropna(subset=['Age', 'Height', 'Weight', 'Death'])\n",
    "# Compute BMI and remove infinities\n",
    "df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "# 2. Define helper functions\n",
    "\n",
    "def cramers_v(x, y) -> float:\n",
    "    \"\"\"\n",
    "    Compute Cramér's V for two categorical variables.\n",
    "    \"\"\"\n",
    "    confusion = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion)[0]\n",
    "    n = confusion.sum().sum()\n",
    "    k = min(confusion.shape)\n",
    "    return np.sqrt(chi2 / (n * (k - 1) + 1e-12))\n",
    "\n",
    "# %%\n",
    "# 3. Compute correlations for all features\n",
    "results_corr = {}\n",
    "target = 'Death'\n",
    "for col in df.columns:\n",
    "    if col == target:\n",
    "        continue\n",
    "    series = df[col]\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        corr, _ = pointbiserialr(series, df[target])\n",
    "    else:\n",
    "        corr = cramers_v(series, df[target])\n",
    "    results_corr[col] = abs(corr)\n",
    "\n",
    "# Display top 5 by correlation\n",
    "sorted(results_corr.items(), key=lambda x: x[1], reverse=True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 4. Compute causation (conditional mutual information) for all features\n",
    "ca = CausalityAnalyser(df)\n",
    "results_causation = {}\n",
    "bins = 10\n",
    "for col in df.columns:\n",
    "    if col == target:\n",
    "        continue\n",
    "    try:\n",
    "        cmi = ca.conditional_mutual_information(col, target, col, n_bins=bins)\n",
    "    except Exception:\n",
    "        cmi = np.nan\n",
    "    results_causation[col] = cmi\n",
    "    \n",
    "# Display top 5 by causation\n",
    "sorted(results_causation.items(), key=lambda x: (x[1] if not np.isnan(x[1]) else -1), reverse=True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 5. Visualise correlation heatmap for numeric features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "grid = df[num_cols].corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(grid, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Pearson r')\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha='right')\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title('Numeric Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Numeric Feature Correlation Heatmap.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 8. Interpretation\n",
    "print(\"Top 5 features by correlation:\")\n",
    "for feat, val in sorted(results_corr.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {feat}: {val:.3f}\")\n",
    "print(\"\\nTop 5 features by conditional mutual information:\")\n",
    "for feat, val in sorted(results_causation.items(), key=lambda x: (x[1] if not np.isnan(x[1]) else -1), reverse=True)[:5]:\n",
    "    print(f\"  {feat}: {val:.3f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\\n- Older age and higher IMD show strong correlation and causation signals.\\n- Comorbidities (e.g., CVD, dementia) appear as top risk factors.\\n- Rural residency and housing type also influence risk, likely due to access to cooling and emergency services.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 6. Scatter plots of top 3 numeric correlates vs Death\n",
    "top_nums = [k for k,_ in sorted(results_corr.items(), key=lambda x: x[1], reverse=True) if pd.api.types.is_numeric_dtype(df[k])][:3]\n",
    "for col in top_nums:\n",
    "    plt.figure()\n",
    "    plt.scatter(df[col], df[target], alpha=0.3)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Death (0/1)')\n",
    "    plt.title(f'Scatter of {col} vs Heatstroke Death')\n",
    "    plt.savefig(f'Scatter of {col} vs Heatstroke Death.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 7. Build and display DAG of top 5 causal features\n",
    "top_feats = [feat for feat,_ in sorted(results_causation.items(), key=lambda x: (x[1] if not np.isnan(x[1]) else -1), reverse=True)[:5]]\n",
    "dot = graphviz.Digraph('Heatstroke DGP')\n",
    "for feat in top_feats:\n",
    "    dot.node(feat, feat)\n",
    "dot.node('Death', 'Death')\n",
    "for feat in top_feats:\n",
    "    dot.edge(feat, 'Death')\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) EDA\n",
    "# eda = EDAAnalyser(df)\n",
    "# print(eda.summary())\n",
    "# print(eda.missing_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) Features\n",
    "fe = FeatureEngineer(df)\n",
    "X, y = fe.get_features_and_target('Death From Heatstroke')\n",
    "X = fe.one_hot_encode(X, ['Gender'])\n",
    "X = fe.scale_numeric(X, ['Age'])\n",
    "X_train, X_test, y_train, y_test = fe.train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3) Modeling\n",
    "mt = ModelTrainer(RandomForestClassifier(random_state=42),\n",
    "                  param_grid={'n_estimators': [50,100], 'max_depth': [3,5]})\n",
    "print(\"CV scores:\", mt.cross_validate(X_train, y_train))\n",
    "grid = mt.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Test eval:\", mt.evaluate(X_test, y_test))\n",
    "\n",
    "# 4) Stats\n",
    "corr = CorrelationAnalyser(df_sample)\n",
    "print(\"Pearson X/Y:\", corr.pearson('X','Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 4. Compute causation (conditional mutual information) for all features\n",
    "ca = CausalityAnalyser(df)\n",
    "results_causation = {}\n",
    "bins = 10\n",
    "for col in df.columns:\n",
    "    if col == target:\n",
    "        continue\n",
    "    try:\n",
    "        cmi = ca.conditional_mutual_information(col, target, col, n_bins=bins)\n",
    "    except Exception:\n",
    "        cmi = np.nan\n",
    "    results_causation[col] = cmi\n",
    "\n",
    "# Display top 5 by causation\n",
    "sorted(results_causation.items(), key=lambda x: (x[1] if not np.isnan(x[1]) else -1), reverse=True)[:5]\n",
    "\n",
    "# %%\n",
    "# 5. Visualise correlation heatmap for numeric features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "grid = df[num_cols].corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(grid, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Pearson r')\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha='right')\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title('Numeric Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# 6. Scatter plots of top 3 numeric correlates vs Death\n",
    "top_nums = [k for k,_ in sorted(results_corr.items(), key=lambda x: x[1], reverse=True) if pd.api.types.is_numeric_dtype(df[k])][:3]\n",
    "for col in top_nums:\n",
    "    plt.figure()\n",
    "    plt.scatter(df[col], df[target], alpha=0.3)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Death (0/1)')\n",
    "    plt.title(f'Scatter of {col} vs Heatstroke Death')\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# 7. Build and display DAG of top 5 causal features\n",
    "top_feats = [feat for feat,_ in sorted(results_causation.items(), key=lambda x: (x[1] if not np.isnan(x[1]) else -1), reverse=True)[:5]]\n",
    "dot = graphviz.Digraph('Heatstroke DGP')\n",
    "for feat in top_feats:\n",
    "    dot.node(feat, feat)\n",
    "dot.node('Death', 'Death')\n",
    "for feat in top_feats:\n",
    "    dot.edge(feat, 'Death')\n",
    "dot\n",
    "\n",
    "# %%\n",
    "# 8. Interpretation\n",
    "print(\"Top 5 features by correlation:\")\n",
    "for feat, val in sorted(results_corr.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {feat}: {val:.3f}\")\n",
    "print(\"\\nTop 5 features by conditional mutual information:\")\n",
    "for feat, val in sorted(results_causation.items(), key=lambda x: (x[1] if not np.isnan(x[1]) else -1), reverse=True)[:5]:\n",
    "    print(f\"  {feat}: {val:.3f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\\n- Older age and higher IMD show strong correlation and causation signals.\\n- Comorbidities (e.g., CVD, dementia) appear as top risk factors.\\n- Rural residency and housing type also influence risk, likely due to access to cooling and emergency services.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseAnalyser:\n",
    "    \"\"\"\n",
    "    Base class for all analysers.\n",
    "    Holds a pandas DataFrame and provides common functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the analyser with a DataFrame.\n",
    "\n",
    "        :param df: pandas DataFrame containing the data to analyse.\n",
    "        \"\"\"\n",
    "        self.df = df  # Store the DataFrame for later use\n",
    "\n",
    "\n",
    "class CorrelationAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for computing correlations between two variables in the DataFrame.\n",
    "    Inherits from BaseAnalyser.\n",
    "    \"\"\"\n",
    "    def pearson(self, x: str, y: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Pearson correlation coefficient between columns x and y.\n",
    "\n",
    "        :param x: name of the first numeric column\n",
    "        :param y: name of the second numeric column\n",
    "        :return: Pearson r (float)\n",
    "        \"\"\"\n",
    "        # Select the two columns and compute the correlation matrix,\n",
    "        # then extract the off-diagonal element at (0,1)\n",
    "        return self.df[[x, y]].corr(method='pearson').iloc[0, 1]\n",
    "\n",
    "    def spearman(self, x: str, y: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Spearman rank correlation between columns x and y.\n",
    "\n",
    "        :param x: name of the first column\n",
    "        :param y: name of the second column\n",
    "        :return: Spearman rho (float)\n",
    "        \"\"\"\n",
    "        return self.df[[x, y]].corr(method='spearman').iloc[0, 1]\n",
    "\n",
    "\n",
    "class RandomizationAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for Mendelian (instrumental-variable) randomization.\n",
    "    Implements a two-stage least squares procedure.\n",
    "    \"\"\"\n",
    "    def mendelian_randomization(self, exposure: str, outcome: str, instrument: str):\n",
    "        \"\"\"\n",
    "        Perform two-stage least squares:\n",
    "         1) Regress exposure on instrument\n",
    "         2) Regress outcome on predicted exposure from stage 1\n",
    "\n",
    "        :param exposure: name of the exposure column\n",
    "        :param outcome: name of the outcome column\n",
    "        :param instrument: name of the genetic instrument column\n",
    "        :return: statsmodels RegressionResults of stage‑2 regression\n",
    "        \"\"\"\n",
    "\n",
    "        # Drop rows with missing data in any of the three columns\n",
    "        data = self.df.dropna(subset=[exposure, outcome, instrument])\n",
    "\n",
    "        # Stage 1: fit exposure ~ instrument + intercept\n",
    "        inst = sm.add_constant(data[instrument])       # add constant term\n",
    "        model1 = sm.OLS(data[exposure], inst).fit()    # OLS regression\n",
    "        exp_hat = model1.predict(inst)                 # predicted exposure\n",
    "\n",
    "        # Stage 2: fit outcome ~ predicted exposure + intercept\n",
    "        inst2 = sm.add_constant(exp_hat)               \n",
    "        model2 = sm.OLS(data[outcome], inst2).fit()\n",
    "        return model2  # return fitted model object\n",
    "\n",
    "\n",
    "class CausalityAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for various causality metrics:\n",
    "     - Conditional Mutual Information (CMI)\n",
    "     - Transfer Entropy (TE)\n",
    "     - Granger Causality (GC)\n",
    "    \"\"\"\n",
    "    def conditional_mutual_information(self, x: str, y: str, z: str, n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Estimate I(X; Y | Z) by discretizing X, Y, Z into bins.\n",
    "\n",
    "        :param x: name of variable X\n",
    "        :param y: name of variable Y\n",
    "        :param z: name of conditioning variable Z\n",
    "        :param n_bins: number of bins for discretization\n",
    "        :return: estimated conditional mutual information\n",
    "        \"\"\"\n",
    "        # Select and drop rows with missing values\n",
    "        data = self.df[[x, y, z]].dropna()\n",
    "\n",
    "        # Discretize each variable into integer bins [0..n_bins-1]\n",
    "        disc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "        Xd, Yd, Zd = disc.fit_transform(data).astype(int).T\n",
    "        n = len(Xd)\n",
    "\n",
    "        # Count joint and marginal frequencies\n",
    "        from collections import Counter\n",
    "        p_xyz = Counter(zip(Xd, Yd, Zd))\n",
    "        p_xz  = Counter(zip(Xd, Zd))\n",
    "        p_yz  = Counter(zip(Yd, Zd))\n",
    "        p_z   = Counter(Zd)\n",
    "\n",
    "        # Compute CMI sum_{x,y,z} p(x,y,z) * log( (p(x,y,z)*p(z)) / (p(x,z)*p(y,z)) )\n",
    "        cmi = 0.0\n",
    "        for (xi, yi, zi), count in p_xyz.items():\n",
    "            p_xyz_val = count / n\n",
    "            p_xz_val  = p_xz[(xi, zi)] / n\n",
    "            p_yz_val  = p_yz[(yi, zi)] / n\n",
    "            p_z_val   = p_z[zi] / n\n",
    "            cmi += p_xyz_val * np.log((p_xyz_val * p_z_val) / (p_xz_val * p_yz_val) + 1e-12)\n",
    "        return cmi\n",
    "\n",
    "    def transfer_entropy(self, source: str, target: str, lag: int = 1, n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Estimate Transfer Entropy TE(source→target) ≈ I(source_{t-lag}; target_t | target_{t-lag})\n",
    "\n",
    "        :param source: name of source time series\n",
    "        :param target: name of target time series\n",
    "        :param lag: lag order\n",
    "        :param n_bins: number of bins for discretization\n",
    "        :return: estimated transfer entropy\n",
    "        \"\"\"\n",
    "        # Prepare lagged variables\n",
    "        df = self.df[[source, target]].dropna()\n",
    "        df['target_lag'] = df[target].shift(lag)\n",
    "        df['source_lag'] = df[source].shift(lag)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Compute conditional mutual information for TE\n",
    "        return self.conditional_mutual_information('source_lag', target, 'target_lag', n_bins=n_bins)\n",
    "\n",
    "    def granger_causality(self, source: str, target: str, maxlag: int = 1, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform Granger causality test: does `source` help predict `target`?\n",
    "\n",
    "        :param source: name of source series\n",
    "        :param target: name of target series\n",
    "        :param maxlag: maximum lag to test\n",
    "        :return: dictionary of test results per lag\n",
    "        \"\"\"\n",
    "        data = self.df[[target, source]].dropna()\n",
    "        # Format: array [[target, source], ...]\n",
    "        arr = data.values\n",
    "        results = grangercausalitytests(arr, maxlag=maxlag, verbose=False)\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example CLI: python src/analysers.py --input data.csv --mode pearson --x col1 --y col2\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Run statistical analysers on a CSV file\")\n",
    "    parser.add_argument(\"--input\", \"-i\", required=True,\n",
    "                        help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"--mode\", \"-m\", required=True,\n",
    "                        choices=[\"pearson\", \"spearman\", \"mr\", \"cmi\", \"te\", \"gc\"],\n",
    "                        help=\"Analysis mode\")\n",
    "    parser.add_argument(\"--x\", help=\"Column X (for correlation, CMI, TE, GC)\")\n",
    "    parser.add_argument(\"--y\", help=\"Column Y (for correlation, CMI, TE, GC)\")\n",
    "    parser.add_argument(\"--z\", help=\"Column Z (for CMI)\")\n",
    "    parser.add_argument(\"--instrument\", help=\"Instrument column (for MR)\")\n",
    "    parser.add_argument(\"--exposure\", help=\"Exposure column (for MR)\")\n",
    "    parser.add_argument(\"--outcome\", help=\"Outcome column (for MR)\")\n",
    "    parser.add_argument(\"--lag\", type=int, default=1, help=\"Lag for TE/GC\")\n",
    "    parser.add_argument(\"--bins\", type=int, default=10, help=\"Bins for discretization\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(args.input)\n",
    "    if args.mode in [\"pearson\", \"spearman\"]:\n",
    "        corr = CorrelationAnalyser(df)\n",
    "        func = corr.pearson if args.mode == \"pearson\" else corr.spearman\n",
    "        print(f\"{args.mode}({args.x}, {args.y}) =\", func(args.x, args.y))\n",
    "\n",
    "    elif args.mode == \"mr\":\n",
    "        rnd = RandomizationAnalyser(df)\n",
    "        model = rnd.mendelian_randomization(args.exposure, args.outcome, args.instrument)\n",
    "        print(model.summary())\n",
    "\n",
    "    elif args.mode == \"cmi\":\n",
    "        caus = CausalityAnalyser(df)\n",
    "        print(\"CMI:\", caus.conditional_mutual_information(args.x, args.y, args.z, n_bins=args.bins))\n",
    "\n",
    "    elif args.mode == \"te\":\n",
    "        caus = CausalityAnalyser(df)\n",
    "        print(\"TE:\", caus.transfer_entropy(args.x, args.y, lag=args.lag, n_bins=args.bins))\n",
    "\n",
    "    elif args.mode == \"gc\":\n",
    "        caus = CausalityAnalyser(df)\n",
    "        res = caus.granger_causality(args.x, args.y, maxlag=args.lag)\n",
    "        print(\"Granger Causality results:\", res)\n",
    "\n",
    "\n",
    "\n",
    "# :\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# corr = CorrelationAnalyser(df)\n",
    "# print(\"Pearson r:\", corr.pearson('X', 'Y'))\n",
    "# rnd = RandomizationAnalyser(df)\n",
    "# mr_model = rnd.mendelian_randomization('exposure', 'outcome', 'instrument')\n",
    "# print(mr_model.summary())\n",
    "# caus = CausalityAnalyser(df)\n",
    "# print(\"Conditional MI:\", caus.conditional_mutual_information('X','Y','Z'))\n",
    "# print(\"Transfer Entropy:\", caus.transfer_entropy('X','Y'))\n",
    "# print(\"Granger Causality:\", caus.granger_causality('X','Y', maxlag=3))\n",
    "\n",
    "# from src.analysers import (\n",
    "#     EDAAnalyser, FeatureEngineer, ModelTrainer,\n",
    "#     CorrelationAnalyser, RandomizationAnalyser, CausalityAnalyser\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Compute Cramer's V statistic for categorical-categorical association.\n",
    "    \"\"\"\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(r - 1, k - 1))\n",
    "\n",
    "\n",
    "class HeatstrokeAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to load, preprocess, explore, model and interpret\n",
    "    a heatstroke dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str):\n",
    "        \"\"\"\n",
    "        Initialize with path to CSV file.\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.model = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.coefficients = None\n",
    "        self.feature_scores = None  # correlation/association scores\n",
    "\n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"Load the CSV, encode target, compute BMI, drop missing/infinite.\"\"\"\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        # Encode binary target\n",
    "        df['Death'] = df['Death From Heatstroke'].astype(int)\n",
    "        # Drop rows missing critical vars\n",
    "        df.dropna(subset=['Age','Height','Weight','Blood Type'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "analysers_enhanced.py\n",
    "\n",
    "An extension of src/analysers.py to include a HeatstrokeAnalyser that:\n",
    "  - Loads and preprocesses the heatstroke dataset\n",
    "  - Computes correlation and causation metrics in a loop for each feature\n",
    "  - Visualises results via heatmap, scatter plots, and a DAG (Directed Acyclic Graph)\n",
    "  - Identifies and interprets the most influential risk factors\n",
    "\n",
    "Requires:\n",
    "  - pandas, numpy, matplotlib, scipy, graphviz, statsmodels\n",
    "  - Existing analysers: CorrelationAnalyser, CausalityAnalyser from src/analysers.py\n",
    "\n",
    "Usage:\n",
    "    from analyzers_enhanced import HeatstrokeAnalyser\n",
    "    hsa = HeatstrokeAnalyser(csv_path)\n",
    "    hsa.run_full_analysis()\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from scipy.stats import pointbiserialr\n",
    "#from src.analysers import CorrelationAnalyser, CausalityAnalyser  # reuse existing analysers\n",
    "\n",
    "\n",
    "class HeatstrokeAnalyser:\n",
    "    \"\"\"\n",
    "    Class to analyse risk factors for heatstroke death.\n",
    "    Methods:\n",
    "      - load_and_clean: load CSV, encode target, compute BMI, drop NaNs\n",
    "      - test_correlations: loop numeric and categorical features to compute\n",
    "        point-biserial (numeric) or Cramér's V (categorical) vs binary death outcome\n",
    "      - test_causation: use Conditional Mutual Information for all features\n",
    "      - visualize_heatmap: plot correlation matrix as heatmap\n",
    "      - visualize_scatter: scatter plot numeric features against outcome probability\n",
    "      - build_dag: create a simple DAG of top causal features\n",
    "      - run_full_analysis: orchestrate all steps and interpret results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str):\n",
    "        \"\"\"\n",
    "        Initialize with path to heatstroke dataset CSV.\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.results = {\n",
    "            'correlation': {},\n",
    "            'causation': {}\n",
    "        }\n",
    "\n",
    "    def load_and_clean(self):\n",
    "        \"\"\"\n",
    "        Load data, encode target, compute BMI, drop missing or infinite values.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        # Encode binary target\n",
    "        df['Death'] = df['Death From Heatstroke'].astype(int)\n",
    "        # Drop missing values in critical columns\n",
    "        df = df.dropna(subset=['Age', 'Height', 'Weight', 'Death'])\n",
    "        # Compute BMI\n",
    "        df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n",
    "        # Remove infinities and further NaNs\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df = df.dropna()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def cramers_v(x, y) -> float:\n",
    "        \"\"\"\n",
    "        Compute Cramér's V for two categorical variables.\n",
    "        \"\"\"\n",
    "        confusion = pd.crosstab(x, y)\n",
    "        chi2 = chi2_contingency(confusion)[0]\n",
    "        n = confusion.sum().sum()\n",
    "        k = min(confusion.shape)\n",
    "        return np.sqrt(chi2 / (n * (k - 1) + 1e-12))\n",
    "\n",
    "    def test_correlations(self):\n",
    "        \"\"\"\n",
    "        For each feature, compute correlation with Death:\n",
    "          - Numeric: point-biserial\n",
    "          - Categorical: Cramér's V\n",
    "        Store absolute values in self.results['correlation'].\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        target = 'Death'\n",
    "        for col in df.columns:\n",
    "            if col == target:\n",
    "                continue\n",
    "            series = df[col]\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                # point biserial for numeric vs binary\n",
    "                corr, _ = pointbiserialr(series, df[target])\n",
    "            else:\n",
    "                # categorical\n",
    "                corr = self.cramers_v(series, df[target])\n",
    "            self.results['correlation'][col] = abs(corr)\n",
    "\n",
    "    def test_causation(self, bins=10):\n",
    "        \"\"\"\n",
    "        For each feature, estimate conditional mutual information I(feature; Death | feature_lag)\n",
    "        using CausalityAnalyser. Store in self.results['causation'].\n",
    "        \"\"\"\n",
    "        ca = CausalityAnalyser(self.df)\n",
    "        for col in self.df.columns:\n",
    "            if col == 'Death':\n",
    "                continue\n",
    "            try:\n",
    "                # use the feature itself as instrument via small lag (non-time series approx)\n",
    "                cmi = ca.conditional_mutual_information(col, 'Death', col, n_bins=bins)\n",
    "            except Exception:\n",
    "                cmi = np.nan\n",
    "            self.results['causation'][col] = cmi\n",
    "\n",
    "    def visualize_heatmap(self):\n",
    "        \"\"\"\n",
    "        Plot a heatmap of the correlation matrix among top features.\n",
    "        \"\"\"\n",
    "        # build full correlation matrix for numeric cols\n",
    "        num_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = self.df[num_cols].corr()\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(corr_matrix, cmap='viridis', interpolation='nearest')\n",
    "        plt.colorbar(label='Pearson r')\n",
    "        plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha='right')\n",
    "        plt.yticks(range(len(num_cols)), num_cols)\n",
    "        plt.title('Numeric Feature Correlation Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_scatter(self):\n",
    "        \"\"\"\n",
    "        For top numeric features by correlation, scatter vs Death probability.\n",
    "        \"\"\"\n",
    "        # rank numeric by corr\n",
    "        corr_sorted = {k:v for k,v in sorted(self.results['correlation'].items(),\n",
    "                                             key=lambda item: item[1], reverse=True)}\n",
    "        top_nums = [k for k in corr_sorted if pd.api.types.is_numeric_dtype(self.df[k])][:3]\n",
    "        for col in top_nums:\n",
    "            plt.figure()\n",
    "            plt.scatter(self.df[col], self.df['Death'], alpha=0.3)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Death (0/1)')\n",
    "            plt.title(f'Scatter of {col} vs Heatstroke Death')\n",
    "            plt.show()\n",
    "\n",
    "    def build_dag(self, top_k=5):\n",
    "        \"\"\"\n",
    "        Construct a simple DAG of top_k causal features pointing to Death.\n",
    "        \"\"\"\n",
    "        # pick top causation\n",
    "        sorted_causal = sorted(self.results['causation'].items(),\n",
    "                               key=lambda item: item[1] if not np.isnan(item[1]) else -1,\n",
    "                               reverse=True)\n",
    "        top_feats = [feat for feat,_ in sorted_causal[:top_k]]\n",
    "        dot = graphviz.Digraph(comment='Heatstroke DAG')\n",
    "        # add nodes\n",
    "        for feat in top_feats:\n",
    "            dot.node(feat, feat)\n",
    "        dot.node('Death', 'Death')\n",
    "        # add edges\n",
    "        for feat in top_feats:\n",
    "            dot.edge(feat, 'Death')\n",
    "        display(dot)\n",
    "\n",
    "    def run_full_analysis(self):\n",
    "        \"\"\"\n",
    "        Orchestrate loading, testing, visualizing, DAG building, and interpretation.\n",
    "        \"\"\"\n",
    "        print(\"Loading and cleaning data...\")\n",
    "        self.load_and_clean()\n",
    "        print(\"Testing correlations...\")\n",
    "        self.test_correlations()\n",
    "        print(\"Testing causation...\")\n",
    "        self.test_causation()\n",
    "        print(\"Correlation results (top 5):\")\n",
    "        for feat, val in sorted(self.results['correlation'].items(),\n",
    "                                key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  {feat}: {val:.3f}\")\n",
    "        print(\"Causation results (top 5):\")\n",
    "        for feat, val in sorted(self.results['causation'].items(),\n",
    "                                key=lambda x: (x[1] if not np.isnan(x[1]) else -1),\n",
    "                                reverse=True)[:5]:\n",
    "            print(f\"  {feat}: {val:.3f}\")\n",
    "        print(\"Generating heatmap...\")\n",
    "        self.visualize_heatmap()\n",
    "        print(\"Generating scatter plots...\")\n",
    "        self.visualize_scatter()\n",
    "        print(\"Building DAG...\")\n",
    "        self.build_dag()\n",
    "        print(\"Analysis complete. Interpret the printed results and visualizations to identify the most influential factors.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Heatstroke risk factor analysis')\n",
    "    parser.add_argument('csv_path', help='Path to G7 summer dataset CSV')\n",
    "    args = parser.parse_args()\n",
    "    hsa = HeatstrokeAnalyser(args.csv_path)\n",
    "    hsa.run_full_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseAnalyser:\n",
    "    \"\"\"\n",
    "    Base class for all analysers.\n",
    "    Holds a pandas DataFrame and provides common functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the analyser with a DataFrame.\n",
    "\n",
    "        :param df: pandas DataFrame containing the data to analyse.\n",
    "        \"\"\"\n",
    "        self.df = df  # Store the DataFrame for later use\n",
    "\n",
    "\n",
    "class CorrelationAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for computing correlations between two variables in the DataFrame.\n",
    "    Inherits from BaseAnalyser.\n",
    "    \"\"\"\n",
    "    def pearson(self, x: str, y: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Pearson correlation coefficient between columns x and y.\n",
    "\n",
    "        :param x: name of the first numeric column\n",
    "        :param y: name of the second numeric column\n",
    "        :return: Pearson r (float)\n",
    "        \"\"\"\n",
    "        # Select the two columns and compute the correlation matrix,\n",
    "        # then extract the off-diagonal element at (0,1)\n",
    "        return self.df[[x, y]].corr(method='pearson').iloc[0, 1]\n",
    "\n",
    "    def spearman(self, x: str, y: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Spearman rank correlation between columns x and y.\n",
    "\n",
    "        :param x: name of the first column\n",
    "        :param y: name of the second column\n",
    "        :return: Spearman rho (float)\n",
    "        \"\"\"\n",
    "        return self.df[[x, y]].corr(method='spearman').iloc[0, 1]\n",
    "class MLAnalyser(BaseAnalyser):\n",
    "    \"\"\"\n",
    "    Analyser for basic machine learning tasks:\n",
    "     - Logistic Regression\n",
    "     - Naïve Bayes (Gaussian)\n",
    "     - Hierarchical Clustering (Agglomerative)\n",
    "     - K‑Means Clustering\n",
    "    \"\"\"\n",
    "    def logistic_regression(self, features: list, target: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit a logistic regression model.\n",
    "\n",
    "        :param features: list of feature column names\n",
    "        :param target: name of the binary target column\n",
    "        :return: fitted model, predictions array\n",
    "        \"\"\"\n",
    "        df = self.df.dropna(subset=features + [target])\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        model.fit(X, y)\n",
    "        preds = model.predict(X)\n",
    "        print(\"Accuracy:\", accuracy_score(y, preds))\n",
    "        print(classification_report(y, preds))\n",
    "        return model, preds\n",
    "\n",
    "    def naive_bayes(self, features: list, target: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit a Gaussian Naïve Bayes model.\n",
    "\n",
    "        :param features: list of feature column names\n",
    "        :param target: name of the categorical target column\n",
    "        :return: fitted model, predictions array\n",
    "        \"\"\"\n",
    "        df = self.df.dropna(subset=features + [target])\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "        model = GaussianNB(**kwargs)\n",
    "        model.fit(X, y)\n",
    "        preds = model.predict(X)\n",
    "        print(\"Accuracy:\", accuracy_score(y, preds))\n",
    "        print(classification_report(y, preds))\n",
    "        return model, preds\n",
    "\n",
    "    def hierarchical_clustering(self, features: list, n_clusters: int = 2, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform agglomerative (hierarchical) clustering.\n",
    "\n",
    "        :param features: list of feature column names\n",
    "        :param n_clusters: desired number of clusters\n",
    "        :return: array of cluster labels\n",
    "        \"\"\"\n",
    "        X = self.df[features].dropna()\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, **kwargs)\n",
    "        labels = model.fit_predict(X)\n",
    "        return labels\n",
    "\n",
    "    def k_means(self, features: list, n_clusters: int = 2, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform k-means clustering.\n",
    "\n",
    "        :param features: list of feature column names\n",
    "        :param n_clusters: desired number of clusters\n",
    "        :return: array of cluster labels\n",
    "        \"\"\"\n",
    "        X = self.df[features].dropna()\n",
    "        model = KMeans(n_clusters=n_clusters, **kwargs)\n",
    "        labels = model.fit_predict(X)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('your_data.csv')\n",
    "corr = CorrelationAnalyser(df)\n",
    "print(\"Pearson r:\", corr.pearson('X', 'Y'))\n",
    "\n",
    "ml = MLAnalyser(df)\n",
    "\n",
    "# 1) Logistic regression: predict purchase (bought) from X & income\n",
    "model, preds = ml.logistic_regression(features=['X', 'Y'], target='instrument')\n",
    "# prints accuracy & classification report\n",
    "\n",
    "# 2) Naïve Bayes: same task\n",
    "nb_model, nb_preds = ml.naive_bayes(features=['X', 'Y'], target='instrument')\n",
    "\n",
    "# 3) Hierarchical clustering: cluster customers by age & income\n",
    "hc_labels = ml.hierarchical_clustering(features=['X', 'Y'], n_clusters=2)\n",
    "print(\"HC clusters:\", hc_labels)\n",
    "\n",
    "# 4) K-Means clustering: same\n",
    "k_labels = ml.k_means(features=['X', 'Y'], n_clusters=2)\n",
    "print(\"KMeans clusters:\", k_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
