{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Principal Data Science Task Template\n",
    "\n",
    "**Author:** Leila Yousefi   \n",
    "**Date:** 24/07/2025  ({{ today().strftime(\"%Y-%m-%d\") }}\n",
    "**Objective:** Briefly restate the problem.\n",
    "\n",
    "## 1. Installations & Imports\n",
    "\n",
    "## 2. Data pre-processing\n",
    "### 2.1. load csv file into a dataframe\n",
    "### 2.2. Summary statistics\n",
    "### 2.3 Data Quality Checks & Solutions\n",
    "#### 2.3.1 Validation\n",
    "#### 2.3.2 Completeness\n",
    "#### 2.3.3 Uniqueness\n",
    "\n",
    "## 3. Exploratory Data Analysis\n",
    "### 3.1 Univariate distributions\n",
    "### 3.2 Bivariate relationships\n",
    "\n",
    "## 4. Feature Engineering & Modelling\n",
    "### 4.1 Train/test split\n",
    "\n",
    "\n",
    "## 5. Evaluation & Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Installations & Imports: Adjust or add libraries as needed for the task.\n",
    "\n",
    "# suppress that specific package RuntimeWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*invalid value encountered in cast.*\"\n",
    ")\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# data libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# viz libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from typing import List, Tuple, Optional\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Working directory\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"Notebooks are here:\", os.listdir())\n",
    "\n",
    "# set paths\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "OUTPUT_DIR = os.path.join(\"..\", \"data\", \"processed\")\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Data pre-processing: Point the filepaths to data/raw/ and load data.\n",
    "\n",
    "### 2.1. load csv file into a dataframe\n",
    "filename = 'pre2018_linked_inv_lpa_data.csv'\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, filename), low_memory=False)\n",
    "\n",
    "# Display the first few records\n",
    "df.head()\n",
    "\n",
    "### 2.2 Summary statistics & missing values\n",
    "df.info()\n",
    "df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2.3 Data Quality Checks & Solutions:\n",
    "\n",
    "#### 2.3.1 Validation: **Correct format** \n",
    "\n",
    "#### 2.3.2 Completeness: **Decisions on missing data**  \n",
    "- Column dates → drop rows (where both dates are missing)\n",
    "- Column X → make derieved id to detect and delete duplicates \n",
    "- Column Y → impute median  \n",
    "\n",
    "#### 2.3.3 Uniqueness: **Decisions onduplicates** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dq = DataQualityChecks('data.csv')\n",
    "dq.validate_dates(['registrationdate', 'date_received_in_opg'])\n",
    "dq.compute_delay('registrationdate', 'date_received_in_opg')\n",
    "dq.impute_delays()\n",
    "# Apply business-specific consistency checks\n",
    "issues = dq.check_consistency()\n",
    "if issues:\n",
    "    print(issues)\n",
    "df_clean = dq.run_all_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# data_quality.py\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class DataQualityChecks:\n",
    "    \"\"\"\n",
    "    A suite of data quality checks and resolution methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load the CSV file into a DataFrame.\n",
    "        :param filepath: Path to the CSV data file\n",
    "        \"\"\"\n",
    "        # Read CSV, allow large fields\n",
    "        self.df = pd.read_csv(filepath, low_memory=False)\n",
    "\n",
    "    def summary_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns summary statistics and info on missing values.\n",
    "        \"\"\"\n",
    "        # DataFrame info\n",
    "        info_buf = []\n",
    "        self.df.info(buf=info_buf)\n",
    "        info = ''.join(info_buf)\n",
    "        # Describe all columns\n",
    "        desc = self.df.describe(include='all')\n",
    "        return desc\n",
    "\n",
    "    def validate_dates(self, date_cols: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ensure specified columns are proper dates, coerce invalids to NaT.\n",
    "        :param date_cols: List of column names to convert\n",
    "        \"\"\"\n",
    "        for col in date_cols:\n",
    "            # Convert column to datetime, coerces errors to NaT\n",
    "            self.df[col] = pd.to_datetime(\n",
    "                self.df[col], errors='coerce', dayfirst=True\n",
    "            )\n",
    "        return self.df\n",
    "    \n",
    "    def parse_month(month_str: str) -> datetime:\n",
    "        \"\"\"Strip quotes/whitespace and parse 'YYYY-MM' → datetime.\"\"\"\n",
    "        cleaned = month_str.strip().strip(\"'\\\"\")\n",
    "        return datetime.strptime(cleaned, \"%Y-%m\")\n",
    "\n",
    "    def generate_month_list(start_month: str, end_month: str):\n",
    "        \"\"\"\n",
    "        Return a list of datetime objects for each month-start\n",
    "        from start_month to end_month inclusive.\n",
    "        \"\"\"\n",
    "        start_dt = parse_month(start_month)\n",
    "        end_dt = parse_month(end_month)\n",
    "        if start_dt > end_dt:\n",
    "            raise ValueError(f\"Start month ({start_month}) is after end month ({end_month})\")\n",
    "\n",
    "        months = []\n",
    "        current = start_dt\n",
    "        while current <= end_dt:\n",
    "            months.append(current)\n",
    "            current += relativedelta(months=1)\n",
    "        return months\n",
    "\n",
    "    def last_day_of_month(dt: datetime) -> str:\n",
    "        \"\"\"\n",
    "        Return the last day of dt's month as 'YYYY-MM-DD'.\n",
    "        \"\"\"\n",
    "        day = calendar.monthrange(dt.year, dt.month)[1]\n",
    "        return dt.replace(day=day).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def flag_invalid_delays(self, start_col: str, end_col: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Flag rows where registration > receipt or either date is missing.\n",
    "        Returns a boolean mask of invalid rows.\n",
    "        \"\"\"\n",
    "        mask = (\n",
    "            self.df[start_col].isna() |\n",
    "            self.df[end_col].isna() |\n",
    "            (self.df[start_col] > self.df[end_col])\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def compute_delay(self, start_col: str, end_col: str, drop_neg: bool=True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute delay in days, assign NaN for invalid ones, optionally drop negatives.\n",
    "        \"\"\"\n",
    "        # Calculate raw delta in days\n",
    "        self.df['delay_days'] = (\n",
    "            self.df[end_col] - self.df[start_col]\n",
    "        ).dt.days\n",
    "\n",
    "        # Invalidate rows where dates are wrong\n",
    "        invalid = self.flag_invalid_delays(start_col, end_col)\n",
    "        self.df.loc[invalid, 'delay_days'] = pd.NA\n",
    "\n",
    "        # Drop negative or missing delays\n",
    "        if drop_neg:\n",
    "            self.df = self.df[\n",
    "                self.df['delay_days'].notna() & (self.df['delay_days'] >= 0)\n",
    "            ].copy()\n",
    "        return self.df\n",
    "\n",
    "    def impute_delays(self, date_col: str='registrationdate') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fill missing delays with group-year mean, fallback to overall mean.\n",
    "        \"\"\"\n",
    "        # Determine delay year: reg year or receipt year\n",
    "        self.df['delay_year'] = (\n",
    "            self.df['registrationdate'].dt.year\n",
    "            .fillna(self.df['date_received_in_opg'].dt.year)\n",
    "            .astype(int)\n",
    "        )\n",
    "        # Group-wise fill\n",
    "        self.df['delay_days'] = self.df.groupby('delay_year')['delay_days']\n",
    "            .transform(lambda s: s.fillna(s.mean()))\n",
    "        # Fill any remaining with overall mean\n",
    "        overall = self.df['delay_days'].mean()\n",
    "        self.df['delay_days'] = self.df['delay_days'].fillna(overall)\n",
    "        # Clean up\n",
    "        self.df.drop(columns=['delay_year'], inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def derive_keys(self, id_cols: Tuple[str,str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build a hybrid derived_id: (case_no + date) or unique_id fallback.\n",
    "        :param id_cols: Tuple of (case_no_col, unique_id_col)\n",
    "        \"\"\"\n",
    "        c_no, u_id = id_cols\n",
    "        def make_id(row):\n",
    "            if pd.notna(row[c_no]) and str(row[c_no]).strip():\n",
    "                date_str = row['date_received_in_opg'].strftime('%Y%m%d')\n",
    "                return f\"{row[c_no]}_{date_str}\"\n",
    "            return str(row[u_id])\n",
    "        self.df['derived_id'] = self.df.apply(make_id, axis=1)\n",
    "        return self.df\n",
    "\n",
    "    def remove_duplicates(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Drop duplicate rows based on 'derived_id', keep first occurrence.\n",
    "        \"\"\"\n",
    "        self.df = self.df.drop_duplicates(subset='derived_id', keep='first')\n",
    "        return self.df\n",
    "\n",
    "    def run_all_checks(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes full pipeline of validation, delay compute, impute, dedup.\n",
    "        \"\"\"\n",
    "        # 1. Validate date formats\n",
    "        self.validate_dates(['registrationdate','date_received_in_opg'])\n",
    "        # 2. Compute and clean delays\n",
    "        self.compute_delay('registrationdate','date_received_in_opg')\n",
    "        # 3. Impute missing delays\n",
    "        self.impute_delays()\n",
    "        # 4. Derive keys & remove duplicates\n",
    "        self.derive_keys(('case_no','unique_id'))\n",
    "        self.remove_duplicates()\n",
    "        return self.df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.1 Validation: **Correct format** \n",
    "# Convert to correct format \n",
    "for col in ['registrationdate', 'date_received_in_opg']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True) # force an out-of-bounds date to NaT, \n",
    "    # in addition to forcing non-dates (or non-parseable dates) to NaT\n",
    "    # parses dates with the day first, e.g. \"10/11/12\" is parsed as 2012-11-10, yearfirst=True is not strict, \n",
    "    # but will prefer to parse with year first.\n",
    "\n",
    "# Count number of missing records based on missing values in 'registrationdate' 'date_received_in_opg'\n",
    "n_reg_missing = df['registrationdate'].isna().sum()\n",
    "n_opg_missing = df['date_received_in_opg'].isna().sum()\n",
    "print(f\"Missing registrationdate: {n_reg_missing}\")\n",
    "print(f\"Missing date_received_in_opg: {n_opg_missing}\")\n",
    "\n",
    "# Derive and Define year_month for monthly grouping\n",
    "df['year_month'] = df['date_received_in_opg'].dt.to_period('M').dt.to_timestamp()\n",
    "#df['year'] = df['date_received_in_opg'].dt.to_period('Y').dt.to_timestamp()\n",
    "df['year'] = df['date_received_in_opg'].dt.year\n",
    "df['month'] = df['date_received_in_opg'].dt.month\n",
    "df['day'] = df['date_received_in_opg'].dt.day\n",
    "\n",
    "# Compute delay_days with null assignment for invalid dates\n",
    "# If registrationdate is NaT or after receipt, delay_days = NaN\n",
    "df['delay_days'] = (df['date_received_in_opg'] - df['registrationdate']).dt.days\n",
    "invalid_mask = (df['registrationdate'].isna()) | \n",
    "                (df['date_received_in_opg'].isna()) | \n",
    "\n",
    "# print(df[df['registrationdate'].isna() | \n",
    "#    df['date_received_in_opg'].isna() | \n",
    "#    (df['registrationdate'] > df['date_received_in_opg'])][['case_no', 'registrationdate', 'date_received_in_opg']])\n",
    "\n",
    "df.loc[invalid_mask, 'delay_days'] = pd.NA\n",
    "\n",
    "# compute “delay in days” and then fill any missing delays with the mean delay for that calendar year \n",
    "# (falling back to the overall mean only if an entire year-group is empty):\n",
    "\n",
    "# Filter out invalid or negative delays\n",
    "# Keep rows where delay_days is non-negative, drop NaN\n",
    "df = df[df['delay_days'].notna() & (df['delay_days'] >= 0)].copy()\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "delays_missing_ids = df[df['delay_days'].isna()]['case_no']\n",
    "#print(\"delays_missing_ids: \", delays_missing_ids)\n",
    "\n",
    "df['delay_year'] = (\n",
    "    df['registrationdate'].dt.year\n",
    "    .fillna(df['date_received_in_opg'].dt.year)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Pick a “year” to group on. Use registration‐year if present, otherwise receipt‐year.\n",
    "\n",
    "# Impute missing delays with the mean for that year\n",
    "df['delay_days'] = (\n",
    "    df\n",
    "    .groupby('delay_year')['delay_days']\n",
    "    .transform(lambda s: s.fillna(s.mean()))\n",
    ")\n",
    "\n",
    "# If an entire year had only missing delays, fill those with the overall mean\n",
    "overall_mean = df['delay_days'].mean()\n",
    "df['delay_days'] = df['delay_days'].fillna(overall_mean)\n",
    "\n",
    "# Count number of missing records based on missing values in 'delay_days' \n",
    "n_delays_missing = df['delay_days'].isna().sum()\n",
    "print(f\"Missing delays: {n_delays_missing}\")\n",
    "\n",
    "imputed_delays_days = df[df['case_no'].isin(delays_missing_ids)]['delay_days']\n",
    "print(f\"imputed delays (per day): {imputed_delays_days}\")\n",
    "\n",
    "print(f\"imputed df: {df}\")\n",
    "\n",
    "# clean up (Optional) \n",
    "df.drop(columns=['delay_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.2 Completeness: **Decisions on missing data** \n",
    "# Missing Data Imputation: Drop rows missing key dates\n",
    "df = df[df['registrationdate'].notna() & df['date_received_in_opg'].notna()]\n",
    "\n",
    "#### 2.3.2 Uniqueness: **Decisions onduplicates:**  \n",
    "# Remove duplicates\n",
    "# Build hybrid unique ID and remove duplicate\n",
    "def make_derived_id(row):\n",
    "    if pd.notna(row['case_no']) and str(row['case_no']).strip():\n",
    "        return f\"{row['case_no']}_{row['date_received_in_opg'].strftime('%Y%m%d')}\"\n",
    "    return str(row['unique_id'])\n",
    "\n",
    "df['derived_id'] = df.apply(make_derived_id, axis=1)\n",
    "df = df.drop_duplicates(subset='derived_id')\n",
    "\n",
    "# Display processed dataframe\n",
    "print(\"The first few records:\", df.head(5))\n",
    "print(\"The last few records:\", df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### 2.3.4 Accuracy: **measures the correctness of the content of data** \n",
    "# Establish which attributes of the data are required and \n",
    "# design the logic used to test them based on the business requirement. \n",
    "# Consistency is part of Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Exploratory Data Analysis: Insert code cells for plots and summary statistics.\n",
    "\n",
    "\n",
    "# Define the target variables among the columns\n",
    "#df[\"target\"] = df['delay_days']\n",
    "# df[\"target\"] = df[\"concern_type\"]\n",
    "\n",
    "### 3.1 Univariate distributions\n",
    "fig, ax = plt.subplots()\n",
    "df[\"delay_days\"].value_counts().plot(kind=\"bar\", ax=ax)\n",
    "plt.title(\"Target distribution\")\n",
    "\n",
    "\n",
    "# ### 3.2 Bivariate relationships\n",
    "# plt.scatter(df[\"feature1\"], df[\"feature2\"])\n",
    "# plt.xlabel(\"feature1\")\n",
    "# plt.ylabel(\"feature2\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define periods and concern types\n",
    "PERIODS = {\n",
    "    'Pre-pandemic (2016–17)':   (2017, [2016, 2017]),\n",
    "    'Spike (2018–19)':          (2019, [2018, 2019]),\n",
    "    'Pandemic (2020–21)':       (2021, [2020, 2021]),\n",
    "    'Post-pandemic (2022–23)':  (2023, [2022, 2023]),\n",
    "}\n",
    "TYPES = ['Financial', 'Health and Welfare', 'Both']\n",
    "\n",
    "# Compute monthly max and min delay in months\n",
    "# For each month and concern type, the “worst-case” delay expressed in months:\n",
    "# So after this step, every row belonging to, say, “Financial” in May 2018 \n",
    "# will have the same number—the largest delay_days observed among all Financial cases received in May 2018.\n",
    "# Max monthly delay in months\n",
    "df['max_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('max') / 30.44 # takes each row in a group and \n",
    "                                                     # replaces its value with the maximum of that group\n",
    "                                                    # divide by 30.44 (the average length of a month in days) \n",
    "                                                    # to convert that maximum-day figure into “months of delay.”\n",
    "# Min monthly delay in months\n",
    "df['min_delay_months'] = df.groupby(['year_month', 'concern_type'])['delay_days']\\\n",
    "                           .transform('min') / 30.44\n",
    "\n",
    "# Build DataFrame for distributions\n",
    "records = []\n",
    "for period, (reg_end, rec_years) in PERIODS.items():\n",
    "    mask = (\n",
    "        #(df['registrationdate'].dt.year <= reg_end) &\n",
    "        df['date_received_in_opg'].dt.year.isin(rec_years) &\n",
    "        df['concern_type'].isin(TYPES)\n",
    "    )\n",
    "    subset = df.loc[mask, ['concern_type', 'delay_days', 'max_delay_months', 'min_delay_months', 'year_month']].copy()\n",
    "    subset['period'] = period\n",
    "    records.append(subset)\n",
    "dist_df = pd.concat(records, ignore_index=True)\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df.rename(columns={\n",
    "    \"delay_days\": \"target\"\n",
    "}, inplace=True)\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Feature Engineering & Modelling: Develop pipelines under the specified headings and record decisions in Markdown.\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "\n",
    "# PCA can’t handle missing values directly. We have two main options:\n",
    "# Impute the missing values before you scale → PCA\n",
    "# Drop any rows (or columns) containing NaNs\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Select numeric columns\n",
    "num_cols = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "\n",
    "# 2. Impute missing values (here we use the median)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_num_imputed = imputer.fit_transform(X[num_cols])\n",
    "\n",
    "# 3. Scale\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num_imputed)\n",
    "\n",
    "# 4. PCA to 90% explained variance\n",
    "pca = PCA(n_components=0.90, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_num_scaled)\n",
    "\n",
    "print(f\"PCA reduced {len(num_cols)} → {pca.n_components_} components\")\n",
    "\n",
    "# Or as a single Pipeline\n",
    "# This is handy if you plan to stick it into a larger Pipeline for CV/reproducibility:\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pca_pipeline = Pipeline([\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"pca\", PCA(n_components=0.90, random_state=RANDOM_STATE)),\n",
    "# ])\n",
    "\n",
    "# # fit + transform in one go\n",
    "# X_pca = pca_pipeline.fit_transform(X[num_cols])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca,         # or X_reduced if you use SelectKBest, etc.\n",
    "    y,             # or y_clean if you dropped rows\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Update your pipelines to force dense output\n",
    "num_feats = X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "cat_feats = X.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),  # <-- note sparse=False\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_feats),\n",
    "    (\"cat\", cat_pipeline, cat_feats),\n",
    "    # optional: sparse_threshold=0  to force dense even if some parts remain sparse\n",
    "], sparse_threshold=0)\n",
    "\n",
    "# Build your full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"pca\", PCA(n_components=0.90, random_state=RANDOM_STATE)),\n",
    "    (\"clf\", LogisticRegression(random_state=RANDOM_STATE)),\n",
    "])\n",
    "\n",
    "# Cross-validate on the original DataFrame X, Series y\n",
    "scores = cross_val_score(full_pipeline, X, y, cv=5, scoring=\"roc_auc\")\n",
    "print(\"CV AUC:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluation & Next Steps: Clearly report metrics, visualizations, and recommended follow‑up actions.\n",
    "\n",
    "### 5.1 Final test performance\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict_proba(X_test)[:,1]\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_pred))\n",
    "print(classification_report(y_test, rf.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 5.2 Insights & Recommendations\n",
    "- **Key finding 1:** …\n",
    "- **Key finding 2:** …\n",
    "- **Limitations:** data quality, potential biases\n",
    "- **Next steps:** hyper-parameter tuning, fairness audit, productionize pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
